{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13194188,"sourceType":"datasetVersion","datasetId":8361373}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/lehan1410/xPatch.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T06:25:50.285494Z","iopub.execute_input":"2025-10-22T06:25:50.285783Z","iopub.status.idle":"2025-10-22T06:25:51.660534Z","shell.execute_reply.started":"2025-10-22T06:25:50.285756Z","shell.execute_reply":"2025-10-22T06:25:51.659751Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'xPatch'...\nremote: Enumerating objects: 2688, done.\u001b[K\nremote: Counting objects: 100% (13/13), done.\u001b[K\nremote: Compressing objects: 100% (8/8), done.\u001b[K\nremote: Total 2688 (delta 5), reused 11 (delta 4), pack-reused 2675 (from 1)\u001b[K\nReceiving objects: 100% (2688/2688), 12.54 MiB | 38.00 MiB/s, done.\nResolving deltas: 100% (1778/1778), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"cd /kaggle/working/xPatch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T06:25:51.661569Z","iopub.execute_input":"2025-10-22T06:25:51.661865Z","iopub.status.idle":"2025-10-22T06:25:51.668171Z","shell.execute_reply.started":"2025-10-22T06:25:51.661827Z","shell.execute_reply":"2025-10-22T06:25:51.667311Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/xPatch\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"cp -r /kaggle/input/dataset /kaggle/working/xPatch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T06:25:51.669490Z","iopub.execute_input":"2025-10-22T06:25:51.669733Z","iopub.status.idle":"2025-10-22T06:25:54.793753Z","shell.execute_reply.started":"2025-10-22T06:25:51.669716Z","shell.execute_reply":"2025-10-22T06:25:54.792629Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!git pull","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T09:52:49.765543Z","iopub.execute_input":"2025-10-16T09:52:49.765852Z","iopub.status.idle":"2025-10-16T09:52:50.206387Z","shell.execute_reply.started":"2025-10-16T09:52:49.765825Z","shell.execute_reply":"2025-10-16T09:52:50.205439Z"}},"outputs":[{"name":"stdout","text":"Already up to date.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTh1 --learning_rate 0.00005 --data_path ETTh1.csv --seq_len 96 --pred_len 96 --enc_in 7 --batch_size 512 --dropout 0 --d_model 256 --lradj 'sigmoid' --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T09:57:15.908141Z","iopub.execute_input":"2025-10-16T09:57:15.908981Z","iopub.status.idle":"2025-10-16T09:59:38.769816Z","shell.execute_reply.started":"2025-10-16T09:57:15.908947Z","shell.execute_reply":"2025-10-16T09:59:38.769153Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=24, d_model=256, data='ETTh1', root_path='./dataset', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=96, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=512, patience=10, learning_rate=5e-05, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.0, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTh1_ftM_sl96_ll48_pl96_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 8449\nval 2785\ntest 2785\n[TRAIN MEMORY] Max memory allocated in epoch 1: 295.35 MB\nEpoch: 1 cost time: 1.5268681049346924\nEpoch: 1, Steps: 16 | Train Loss: 0.4765661 Vali Loss: 0.1889104 Test Loss: 0.6626001\nValidation loss decreased (inf --> 0.188910).  Saving model ...\nUpdating learning rate to 1.9766777377990105e-07\n[TRAIN MEMORY] Max memory allocated in epoch 2: 295.35 MB\nEpoch: 2 cost time: 0.7682361602783203\nEpoch: 2, Steps: 16 | Train Loss: 0.4611495 Vali Loss: 0.1886182 Test Loss: 0.6621796\nValidation loss decreased (0.188910 --> 0.188618).  Saving model ...\nUpdating learning rate to 5.297334308904794e-07\n[TRAIN MEMORY] Max memory allocated in epoch 3: 295.35 MB\nEpoch: 3 cost time: 0.7644352912902832\nEpoch: 3, Steps: 16 | Train Loss: 0.4609556 Vali Loss: 0.1887295 Test Loss: 0.6610510\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 1.0772330336670663e-06\n[TRAIN MEMORY] Max memory allocated in epoch 4: 295.35 MB\nEpoch: 4 cost time: 0.76580810546875\nEpoch: 4, Steps: 16 | Train Loss: 0.4599488 Vali Loss: 0.1882361 Test Loss: 0.6587669\nValidation loss decreased (0.188618 --> 0.188236).  Saving model ...\nUpdating learning rate to 1.9631651012203446e-06\n[TRAIN MEMORY] Max memory allocated in epoch 5: 295.35 MB\nEpoch: 5 cost time: 0.7353265285491943\nEpoch: 5, Steps: 16 | Train Loss: 0.4594830 Vali Loss: 0.1880296 Test Loss: 0.6546261\nValidation loss decreased (0.188236 --> 0.188030).  Saving model ...\nUpdating learning rate to 3.3640347303765784e-06\n[TRAIN MEMORY] Max memory allocated in epoch 6: 295.35 MB\nEpoch: 6 cost time: 0.8116805553436279\nEpoch: 6, Steps: 16 | Train Loss: 0.4581296 Vali Loss: 0.1868474 Test Loss: 0.6476117\nValidation loss decreased (0.188030 --> 0.186847).  Saving model ...\nUpdating learning rate to 5.509481168463486e-06\n[TRAIN MEMORY] Max memory allocated in epoch 7: 295.35 MB\nEpoch: 7 cost time: 0.770052433013916\nEpoch: 7, Steps: 16 | Train Loss: 0.4556471 Vali Loss: 0.1851782 Test Loss: 0.6363847\nValidation loss decreased (0.186847 --> 0.185178).  Saving model ...\nUpdating learning rate to 8.647724011220513e-06\n[TRAIN MEMORY] Max memory allocated in epoch 8: 295.35 MB\nEpoch: 8 cost time: 0.7522742748260498\nEpoch: 8, Steps: 16 | Train Loss: 0.4521756 Vali Loss: 0.1830492 Test Loss: 0.6195413\nValidation loss decreased (0.185178 --> 0.183049).  Saving model ...\nUpdating learning rate to 1.2949480975154542e-05\n[TRAIN MEMORY] Max memory allocated in epoch 9: 295.35 MB\nEpoch: 9 cost time: 0.7276477813720703\nEpoch: 9, Steps: 16 | Train Loss: 0.4468415 Vali Loss: 0.1799195 Test Loss: 0.5958958\nValidation loss decreased (0.183049 --> 0.179920).  Saving model ...\nUpdating learning rate to 1.835419812831137e-05\n[TRAIN MEMORY] Max memory allocated in epoch 10: 295.35 MB\nEpoch: 10 cost time: 0.7403764724731445\nEpoch: 10, Steps: 16 | Train Loss: 0.4392739 Vali Loss: 0.1759624 Test Loss: 0.5654588\nValidation loss decreased (0.179920 --> 0.175962).  Saving model ...\nUpdating learning rate to 2.4450652868470343e-05\n[TRAIN MEMORY] Max memory allocated in epoch 11: 295.35 MB\nEpoch: 11 cost time: 0.7456004619598389\nEpoch: 11, Steps: 16 | Train Loss: 0.4295831 Vali Loss: 0.1701240 Test Loss: 0.5301158\nValidation loss decreased (0.175962 --> 0.170124).  Saving model ...\nUpdating learning rate to 3.054577893589662e-05\n[TRAIN MEMORY] Max memory allocated in epoch 12: 295.35 MB\nEpoch: 12 cost time: 0.7713649272918701\nEpoch: 12, Steps: 16 | Train Loss: 0.4184297 Vali Loss: 0.1647561 Test Loss: 0.4936125\nValidation loss decreased (0.170124 --> 0.164756).  Saving model ...\nUpdating learning rate to 3.5946507182286536e-05\n[TRAIN MEMORY] Max memory allocated in epoch 13: 295.35 MB\nEpoch: 13 cost time: 0.7286438941955566\nEpoch: 13, Steps: 16 | Train Loss: 0.4070583 Vali Loss: 0.1607395 Test Loss: 0.4606777\nValidation loss decreased (0.164756 --> 0.160740).  Saving model ...\nUpdating learning rate to 4.02416063364766e-05\n[TRAIN MEMORY] Max memory allocated in epoch 14: 295.35 MB\nEpoch: 14 cost time: 0.8042726516723633\nEpoch: 14, Steps: 16 | Train Loss: 0.3965975 Vali Loss: 0.1555318 Test Loss: 0.4353346\nValidation loss decreased (0.160740 --> 0.155532).  Saving model ...\nUpdating learning rate to 4.337050800751088e-05\n[TRAIN MEMORY] Max memory allocated in epoch 15: 295.35 MB\nEpoch: 15 cost time: 0.7518527507781982\nEpoch: 15, Steps: 16 | Train Loss: 0.3879010 Vali Loss: 0.1514333 Test Loss: 0.4179148\nValidation loss decreased (0.155532 --> 0.151433).  Saving model ...\nUpdating learning rate to 4.5503909646775556e-05\n[TRAIN MEMORY] Max memory allocated in epoch 16: 295.35 MB\nEpoch: 16 cost time: 0.7672417163848877\nEpoch: 16, Steps: 16 | Train Loss: 0.3818719 Vali Loss: 0.1490695 Test Loss: 0.4069450\nValidation loss decreased (0.151433 --> 0.149070).  Saving model ...\nUpdating learning rate to 4.689000475645801e-05\n[TRAIN MEMORY] Max memory allocated in epoch 17: 295.35 MB\nEpoch: 17 cost time: 0.7373781204223633\nEpoch: 17, Steps: 16 | Train Loss: 0.3768076 Vali Loss: 0.1477355 Test Loss: 0.3998544\nValidation loss decreased (0.149070 --> 0.147735).  Saving model ...\nUpdating learning rate to 4.775840063351174e-05\n[TRAIN MEMORY] Max memory allocated in epoch 18: 295.35 MB\nEpoch: 18 cost time: 0.7377908229827881\nEpoch: 18, Steps: 16 | Train Loss: 0.3732199 Vali Loss: 0.1458664 Test Loss: 0.3952026\nValidation loss decreased (0.147735 --> 0.145866).  Saving model ...\nUpdating learning rate to 4.828556453332337e-05\n[TRAIN MEMORY] Max memory allocated in epoch 19: 295.35 MB\nEpoch: 19 cost time: 0.7198474407196045\nEpoch: 19, Steps: 16 | Train Loss: 0.3710095 Vali Loss: 0.1450679 Test Loss: 0.3921426\nValidation loss decreased (0.145866 --> 0.145068).  Saving model ...\nUpdating learning rate to 4.859445120268396e-05\n[TRAIN MEMORY] Max memory allocated in epoch 20: 295.35 MB\nEpoch: 20 cost time: 0.7243669033050537\nEpoch: 20, Steps: 16 | Train Loss: 0.3691797 Vali Loss: 0.1446520 Test Loss: 0.3902389\nValidation loss decreased (0.145068 --> 0.144652).  Saving model ...\nUpdating learning rate to 4.8766046955681185e-05\n[TRAIN MEMORY] Max memory allocated in epoch 21: 295.35 MB\nEpoch: 21 cost time: 0.7517998218536377\nEpoch: 21, Steps: 16 | Train Loss: 0.3673238 Vali Loss: 0.1436635 Test Loss: 0.3885695\nValidation loss decreased (0.144652 --> 0.143663).  Saving model ...\nUpdating learning rate to 4.885194501735325e-05\n[TRAIN MEMORY] Max memory allocated in epoch 22: 295.35 MB\nEpoch: 22 cost time: 0.7492413520812988\nEpoch: 22, Steps: 16 | Train Loss: 0.3659719 Vali Loss: 0.1434738 Test Loss: 0.3872962\nValidation loss decreased (0.143663 --> 0.143474).  Saving model ...\nUpdating learning rate to 4.8884353555464394e-05\n[TRAIN MEMORY] Max memory allocated in epoch 23: 295.35 MB\nEpoch: 23 cost time: 0.744950532913208\nEpoch: 23, Steps: 16 | Train Loss: 0.3654098 Vali Loss: 0.1422286 Test Loss: 0.3861165\nValidation loss decreased (0.143474 --> 0.142229).  Saving model ...\nUpdating learning rate to 4.888312366122914e-05\n[TRAIN MEMORY] Max memory allocated in epoch 24: 295.35 MB\nEpoch: 24 cost time: 0.7613122463226318\nEpoch: 24, Steps: 16 | Train Loss: 0.3638430 Vali Loss: 0.1417672 Test Loss: 0.3852066\nValidation loss decreased (0.142229 --> 0.141767).  Saving model ...\nUpdating learning rate to 4.886038389347345e-05\n[TRAIN MEMORY] Max memory allocated in epoch 25: 295.35 MB\nEpoch: 25 cost time: 0.7944631576538086\nEpoch: 25, Steps: 16 | Train Loss: 0.3631986 Vali Loss: 0.1418512 Test Loss: 0.3843375\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.8823492572652544e-05\n[TRAIN MEMORY] Max memory allocated in epoch 26: 295.35 MB\nEpoch: 26 cost time: 0.7687318325042725\nEpoch: 26, Steps: 16 | Train Loss: 0.3627603 Vali Loss: 0.1411748 Test Loss: 0.3837133\nValidation loss decreased (0.141767 --> 0.141175).  Saving model ...\nUpdating learning rate to 4.8776881422593227e-05\n[TRAIN MEMORY] Max memory allocated in epoch 27: 295.35 MB\nEpoch: 27 cost time: 0.7579660415649414\nEpoch: 27, Steps: 16 | Train Loss: 0.3614885 Vali Loss: 0.1409628 Test Loss: 0.3831393\nValidation loss decreased (0.141175 --> 0.140963).  Saving model ...\nUpdating learning rate to 4.872319348975365e-05\n[TRAIN MEMORY] Max memory allocated in epoch 28: 295.35 MB\nEpoch: 28 cost time: 0.7232785224914551\nEpoch: 28, Steps: 16 | Train Loss: 0.3610956 Vali Loss: 0.1405902 Test Loss: 0.3825470\nValidation loss decreased (0.140963 --> 0.140590).  Saving model ...\nUpdating learning rate to 4.86639805923574e-05\n[TRAIN MEMORY] Max memory allocated in epoch 29: 295.35 MB\nEpoch: 29 cost time: 0.7667369842529297\nEpoch: 29, Steps: 16 | Train Loss: 0.3601631 Vali Loss: 0.1397804 Test Loss: 0.3820950\nValidation loss decreased (0.140590 --> 0.139780).  Saving model ...\nUpdating learning rate to 4.860012899942082e-05\n[TRAIN MEMORY] Max memory allocated in epoch 30: 295.35 MB\nEpoch: 30 cost time: 0.7304701805114746\nEpoch: 30, Steps: 16 | Train Loss: 0.3602083 Vali Loss: 0.1395236 Test Loss: 0.3817206\nValidation loss decreased (0.139780 --> 0.139524).  Saving model ...\nUpdating learning rate to 4.8532118568997064e-05\n[TRAIN MEMORY] Max memory allocated in epoch 31: 295.35 MB\nEpoch: 31 cost time: 0.7437810897827148\nEpoch: 31, Steps: 16 | Train Loss: 0.3592900 Vali Loss: 0.1397974 Test Loss: 0.3810426\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.846018024758687e-05\n[TRAIN MEMORY] Max memory allocated in epoch 32: 295.35 MB\nEpoch: 32 cost time: 0.7633442878723145\nEpoch: 32, Steps: 16 | Train Loss: 0.3593948 Vali Loss: 0.1388282 Test Loss: 0.3808250\nValidation loss decreased (0.139524 --> 0.138828).  Saving model ...\nUpdating learning rate to 4.838439169398508e-05\n[TRAIN MEMORY] Max memory allocated in epoch 33: 295.35 MB\nEpoch: 33 cost time: 0.7804851531982422\nEpoch: 33, Steps: 16 | Train Loss: 0.3585232 Vali Loss: 0.1392767 Test Loss: 0.3804130\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.830473529249205e-05\n[TRAIN MEMORY] Max memory allocated in epoch 34: 295.35 MB\nEpoch: 34 cost time: 0.7717924118041992\nEpoch: 34, Steps: 16 | Train Loss: 0.3584448 Vali Loss: 0.1389733 Test Loss: 0.3800731\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 4.822113332763808e-05\n[TRAIN MEMORY] Max memory allocated in epoch 35: 295.35 MB\nEpoch: 35 cost time: 0.7422709465026855\nEpoch: 35, Steps: 16 | Train Loss: 0.3577998 Vali Loss: 0.1381868 Test Loss: 0.3794514\nValidation loss decreased (0.138828 --> 0.138187).  Saving model ...\nUpdating learning rate to 4.813346930082932e-05\n[TRAIN MEMORY] Max memory allocated in epoch 36: 295.35 MB\nEpoch: 36 cost time: 0.7584991455078125\nEpoch: 36, Steps: 16 | Train Loss: 0.3575391 Vali Loss: 0.1387247 Test Loss: 0.3793113\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.8041600843946885e-05\n[TRAIN MEMORY] Max memory allocated in epoch 37: 295.35 MB\nEpoch: 37 cost time: 0.7501099109649658\nEpoch: 37, Steps: 16 | Train Loss: 0.3570511 Vali Loss: 0.1382866 Test Loss: 0.3788487\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 4.794536754211641e-05\n[TRAIN MEMORY] Max memory allocated in epoch 38: 295.35 MB\nEpoch: 38 cost time: 0.7670488357543945\nEpoch: 38, Steps: 16 | Train Loss: 0.3564796 Vali Loss: 0.1383628 Test Loss: 0.3787321\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 4.784459567654431e-05\n[TRAIN MEMORY] Max memory allocated in epoch 39: 295.35 MB\nEpoch: 39 cost time: 0.7413439750671387\nEpoch: 39, Steps: 16 | Train Loss: 0.3566050 Vali Loss: 0.1379837 Test Loss: 0.3786479\nValidation loss decreased (0.138187 --> 0.137984).  Saving model ...\nUpdating learning rate to 4.773910110846522e-05\n[TRAIN MEMORY] Max memory allocated in epoch 40: 295.35 MB\nEpoch: 40 cost time: 0.7063088417053223\nEpoch: 40, Steps: 16 | Train Loss: 0.3562837 Vali Loss: 0.1376268 Test Loss: 0.3781177\nValidation loss decreased (0.137984 --> 0.137627).  Saving model ...\nUpdating learning rate to 4.762869104601032e-05\n[TRAIN MEMORY] Max memory allocated in epoch 41: 295.35 MB\nEpoch: 41 cost time: 0.7717554569244385\nEpoch: 41, Steps: 16 | Train Loss: 0.3558830 Vali Loss: 0.1374936 Test Loss: 0.3775769\nValidation loss decreased (0.137627 --> 0.137494).  Saving model ...\nUpdating learning rate to 4.7513165145117075e-05\n[TRAIN MEMORY] Max memory allocated in epoch 42: 295.35 MB\nEpoch: 42 cost time: 0.7512392997741699\nEpoch: 42, Steps: 16 | Train Loss: 0.3558858 Vali Loss: 0.1373118 Test Loss: 0.3779159\nValidation loss decreased (0.137494 --> 0.137312).  Saving model ...\nUpdating learning rate to 4.739231621932101e-05\n[TRAIN MEMORY] Max memory allocated in epoch 43: 295.35 MB\nEpoch: 43 cost time: 0.7501578330993652\nEpoch: 43, Steps: 16 | Train Loss: 0.3552285 Vali Loss: 0.1378075 Test Loss: 0.3773612\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.726593072640151e-05\n[TRAIN MEMORY] Max memory allocated in epoch 44: 295.35 MB\nEpoch: 44 cost time: 0.800471305847168\nEpoch: 44, Steps: 16 | Train Loss: 0.3551898 Vali Loss: 0.1373752 Test Loss: 0.3770676\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 4.713378913508779e-05\n[TRAIN MEMORY] Max memory allocated in epoch 45: 295.35 MB\nEpoch: 45 cost time: 0.7901015281677246\nEpoch: 45, Steps: 16 | Train Loss: 0.3546208 Vali Loss: 0.1367463 Test Loss: 0.3769977\nValidation loss decreased (0.137312 --> 0.136746).  Saving model ...\nUpdating learning rate to 4.699566623580007e-05\n[TRAIN MEMORY] Max memory allocated in epoch 46: 295.35 MB\nEpoch: 46 cost time: 0.774691104888916\nEpoch: 46, Steps: 16 | Train Loss: 0.3548208 Vali Loss: 0.1375678 Test Loss: 0.3766164\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.68513314356512e-05\n[TRAIN MEMORY] Max memory allocated in epoch 47: 295.35 MB\nEpoch: 47 cost time: 0.7670512199401855\nEpoch: 47, Steps: 16 | Train Loss: 0.3542402 Vali Loss: 0.1370621 Test Loss: 0.3766576\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 4.670054906356658e-05\n[TRAIN MEMORY] Max memory allocated in epoch 48: 295.35 MB\nEpoch: 48 cost time: 0.779029369354248\nEpoch: 48, Steps: 16 | Train Loss: 0.3545257 Vali Loss: 0.1371751 Test Loss: 0.3760992\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 4.654307870269284e-05\n[TRAIN MEMORY] Max memory allocated in epoch 49: 295.35 MB\nEpoch: 49 cost time: 0.7688989639282227\nEpoch: 49, Steps: 16 | Train Loss: 0.3539678 Vali Loss: 0.1372049 Test Loss: 0.3762124\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 4.6378675562010726e-05\n[TRAIN MEMORY] Max memory allocated in epoch 50: 295.35 MB\nEpoch: 50 cost time: 0.7923572063446045\nEpoch: 50, Steps: 16 | Train Loss: 0.3540946 Vali Loss: 0.1369313 Test Loss: 0.3759441\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 4.620709089588014e-05\n[TRAIN MEMORY] Max memory allocated in epoch 51: 295.35 MB\nEpoch: 51 cost time: 0.7471532821655273\nEpoch: 51, Steps: 16 | Train Loss: 0.3538384 Vali Loss: 0.1364807 Test Loss: 0.3754461\nValidation loss decreased (0.136746 --> 0.136481).  Saving model ...\nUpdating learning rate to 4.602807247829344e-05\n[TRAIN MEMORY] Max memory allocated in epoch 52: 295.35 MB\nEpoch: 52 cost time: 0.7732126712799072\nEpoch: 52, Steps: 16 | Train Loss: 0.3532691 Vali Loss: 0.1373967 Test Loss: 0.3755963\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.584136513739108e-05\n[TRAIN MEMORY] Max memory allocated in epoch 53: 295.35 MB\nEpoch: 53 cost time: 0.761970043182373\nEpoch: 53, Steps: 16 | Train Loss: 0.3533112 Vali Loss: 0.1372321 Test Loss: 0.3750123\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 4.564671135499117e-05\n[TRAIN MEMORY] Max memory allocated in epoch 54: 295.35 MB\nEpoch: 54 cost time: 0.7413344383239746\nEpoch: 54, Steps: 16 | Train Loss: 0.3532976 Vali Loss: 0.1367638 Test Loss: 0.3750911\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 4.544385193530985e-05\n[TRAIN MEMORY] Max memory allocated in epoch 55: 295.35 MB\nEpoch: 55 cost time: 0.7579481601715088\nEpoch: 55, Steps: 16 | Train Loss: 0.3533580 Vali Loss: 0.1373766 Test Loss: 0.3748970\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 4.523252674658504e-05\n[TRAIN MEMORY] Max memory allocated in epoch 56: 295.35 MB\nEpoch: 56 cost time: 0.7694704532623291\nEpoch: 56, Steps: 16 | Train Loss: 0.3528557 Vali Loss: 0.1369281 Test Loss: 0.3746887\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 4.501247553888481e-05\n[TRAIN MEMORY] Max memory allocated in epoch 57: 295.35 MB\nEpoch: 57 cost time: 0.7657060623168945\nEpoch: 57, Steps: 16 | Train Loss: 0.3526332 Vali Loss: 0.1369080 Test Loss: 0.3744757\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 4.4783438840937853e-05\n[TRAIN MEMORY] Max memory allocated in epoch 58: 295.35 MB\nEpoch: 58 cost time: 0.7724049091339111\nEpoch: 58, Steps: 16 | Train Loss: 0.3528072 Vali Loss: 0.1364621 Test Loss: 0.3744596\nValidation loss decreased (0.136481 --> 0.136462).  Saving model ...\nUpdating learning rate to 4.454515893833179e-05\n[TRAIN MEMORY] Max memory allocated in epoch 59: 295.35 MB\nEpoch: 59 cost time: 0.7336854934692383\nEpoch: 59, Steps: 16 | Train Loss: 0.3528179 Vali Loss: 0.1367691 Test Loss: 0.3740813\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.4297380934865594e-05\n[TRAIN MEMORY] Max memory allocated in epoch 60: 295.35 MB\nEpoch: 60 cost time: 0.7654433250427246\nEpoch: 60, Steps: 16 | Train Loss: 0.3524581 Vali Loss: 0.1363901 Test Loss: 0.3740600\nValidation loss decreased (0.136462 --> 0.136390).  Saving model ...\nUpdating learning rate to 4.4039853898199724e-05\n[TRAIN MEMORY] Max memory allocated in epoch 61: 295.35 MB\nEpoch: 61 cost time: 0.7403638362884521\nEpoch: 61, Steps: 16 | Train Loss: 0.3521345 Vali Loss: 0.1368687 Test Loss: 0.3740889\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.377233209020801e-05\n[TRAIN MEMORY] Max memory allocated in epoch 62: 295.35 MB\nEpoch: 62 cost time: 0.737206220626831\nEpoch: 62, Steps: 16 | Train Loss: 0.3521529 Vali Loss: 0.1369006 Test Loss: 0.3737205\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 4.3494576281594656e-05\n[TRAIN MEMORY] Max memory allocated in epoch 63: 295.35 MB\nEpoch: 63 cost time: 0.7609841823577881\nEpoch: 63, Steps: 16 | Train Loss: 0.3520557 Vali Loss: 0.1365398 Test Loss: 0.3736037\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 4.3206355149390344e-05\n[TRAIN MEMORY] Max memory allocated in epoch 64: 295.35 MB\nEpoch: 64 cost time: 0.7620477676391602\nEpoch: 64, Steps: 16 | Train Loss: 0.3513908 Vali Loss: 0.1364290 Test Loss: 0.3735560\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 4.290744675488163e-05\n[TRAIN MEMORY] Max memory allocated in epoch 65: 295.35 MB\nEpoch: 65 cost time: 0.7783899307250977\nEpoch: 65, Steps: 16 | Train Loss: 0.3517902 Vali Loss: 0.1359935 Test Loss: 0.3733224\nValidation loss decreased (0.136390 --> 0.135993).  Saving model ...\nUpdating learning rate to 4.2597640098358526e-05\n[TRAIN MEMORY] Max memory allocated in epoch 66: 295.35 MB\nEpoch: 66 cost time: 0.7672581672668457\nEpoch: 66, Steps: 16 | Train Loss: 0.3513179 Vali Loss: 0.1365853 Test Loss: 0.3733662\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.22767367457887e-05\n[TRAIN MEMORY] Max memory allocated in epoch 67: 295.35 MB\nEpoch: 67 cost time: 0.7529885768890381\nEpoch: 67, Steps: 16 | Train Loss: 0.3517223 Vali Loss: 0.1369695 Test Loss: 0.3731018\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 4.194455252114977e-05\n[TRAIN MEMORY] Max memory allocated in epoch 68: 295.35 MB\nEpoch: 68 cost time: 0.7654414176940918\nEpoch: 68, Steps: 16 | Train Loss: 0.3511657 Vali Loss: 0.1368512 Test Loss: 0.3731707\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 4.1600919256683504e-05\n[TRAIN MEMORY] Max memory allocated in epoch 69: 295.35 MB\nEpoch: 69 cost time: 0.8401477336883545\nEpoch: 69, Steps: 16 | Train Loss: 0.3512689 Vali Loss: 0.1369908 Test Loss: 0.3729081\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 4.1245686591790294e-05\n[TRAIN MEMORY] Max memory allocated in epoch 70: 295.35 MB\nEpoch: 70 cost time: 0.7592597007751465\nEpoch: 70, Steps: 16 | Train Loss: 0.3512930 Vali Loss: 0.1364892 Test Loss: 0.3727166\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 4.087872380967751e-05\n[TRAIN MEMORY] Max memory allocated in epoch 71: 295.35 MB\nEpoch: 71 cost time: 0.7669363021850586\nEpoch: 71, Steps: 16 | Train Loss: 0.3510975 Vali Loss: 0.1361694 Test Loss: 0.3727959\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 4.049992169923152e-05\n[TRAIN MEMORY] Max memory allocated in epoch 72: 295.35 MB\nEpoch: 72 cost time: 0.7411842346191406\nEpoch: 72, Steps: 16 | Train Loss: 0.3514527 Vali Loss: 0.1358921 Test Loss: 0.3723245\nValidation loss decreased (0.135993 --> 0.135892).  Saving model ...\nUpdating learning rate to 4.010919442792737e-05\n[TRAIN MEMORY] Max memory allocated in epoch 73: 295.35 MB\nEpoch: 73 cost time: 0.7386090755462646\nEpoch: 73, Steps: 16 | Train Loss: 0.3511024 Vali Loss: 0.1368730 Test Loss: 0.3725281\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 3.970648140995159e-05\n[TRAIN MEMORY] Max memory allocated in epoch 74: 295.35 MB\nEpoch: 74 cost time: 0.7268483638763428\nEpoch: 74, Steps: 16 | Train Loss: 0.3508957 Vali Loss: 0.1366120 Test Loss: 0.3725735\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 3.92917491521273e-05\n[TRAIN MEMORY] Max memory allocated in epoch 75: 295.35 MB\nEpoch: 75 cost time: 0.7467408180236816\nEpoch: 75, Steps: 16 | Train Loss: 0.3510871 Vali Loss: 0.1369966 Test Loss: 0.3722761\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 3.8864993058734175e-05\n[TRAIN MEMORY] Max memory allocated in epoch 76: 295.35 MB\nEpoch: 76 cost time: 0.7235302925109863\nEpoch: 76, Steps: 16 | Train Loss: 0.3506738 Vali Loss: 0.1357960 Test Loss: 0.3722202\nValidation loss decreased (0.135892 --> 0.135796).  Saving model ...\nUpdating learning rate to 3.842623917495065e-05\n[TRAIN MEMORY] Max memory allocated in epoch 77: 295.35 MB\nEpoch: 77 cost time: 0.7272460460662842\nEpoch: 77, Steps: 16 | Train Loss: 0.3507689 Vali Loss: 0.1373181 Test Loss: 0.3720230\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 3.797554584745541e-05\n[TRAIN MEMORY] Max memory allocated in epoch 78: 295.35 MB\nEpoch: 78 cost time: 0.7679462432861328\nEpoch: 78, Steps: 16 | Train Loss: 0.3506605 Vali Loss: 0.1366630 Test Loss: 0.3719750\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 3.75130052797558e-05\n[TRAIN MEMORY] Max memory allocated in epoch 79: 295.35 MB\nEpoch: 79 cost time: 0.7819128036499023\nEpoch: 79, Steps: 16 | Train Loss: 0.3504714 Vali Loss: 0.1365930 Test Loss: 0.3720072\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 3.7038744959107646e-05\n[TRAIN MEMORY] Max memory allocated in epoch 80: 295.35 MB\nEpoch: 80 cost time: 0.759347677230835\nEpoch: 80, Steps: 16 | Train Loss: 0.3508266 Vali Loss: 0.1367581 Test Loss: 0.3719363\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 3.6552928931500215e-05\n[TRAIN MEMORY] Max memory allocated in epoch 81: 295.35 MB\nEpoch: 81 cost time: 0.746382474899292\nEpoch: 81, Steps: 16 | Train Loss: 0.3503693 Vali Loss: 0.1370081 Test Loss: 0.3717116\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 3.605575890114313e-05\n[TRAIN MEMORY] Max memory allocated in epoch 82: 295.35 MB\nEpoch: 82 cost time: 0.8082950115203857\nEpoch: 82, Steps: 16 | Train Loss: 0.3497767 Vali Loss: 0.1366517 Test Loss: 0.3718833\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 3.5547475131250184e-05\n[TRAIN MEMORY] Max memory allocated in epoch 83: 295.35 MB\nEpoch: 83 cost time: 0.7368700504302979\nEpoch: 83, Steps: 16 | Train Loss: 0.3502659 Vali Loss: 0.1366424 Test Loss: 0.3715687\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 3.502835712369864e-05\n[TRAIN MEMORY] Max memory allocated in epoch 84: 295.35 MB\nEpoch: 84 cost time: 0.774059534072876\nEpoch: 84, Steps: 16 | Train Loss: 0.3504112 Vali Loss: 0.1358848 Test Loss: 0.3719579\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 3.449872405638063e-05\n[TRAIN MEMORY] Max memory allocated in epoch 85: 295.35 MB\nEpoch: 85 cost time: 0.7643191814422607\nEpoch: 85, Steps: 16 | Train Loss: 0.3498532 Vali Loss: 0.1361366 Test Loss: 0.3717165\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 3.3958934958769655e-05\n[TRAIN MEMORY] Max memory allocated in epoch 86: 295.35 MB\nEpoch: 86 cost time: 0.7670810222625732\nEpoch: 86, Steps: 16 | Train Loss: 0.3499338 Vali Loss: 0.1359581 Test Loss: 0.3716522\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 0.77 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTh1_ftM_sl96_ll48_pl96_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 2785\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.58 MB\n[PARAMS] Total parameters: 150,753\nmse:0.3722201883792877, mae:0.38307225704193115\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTh1 --learning_rate 0.0009 --data_path ETTh1.csv --seq_len 96 --pred_len 192 --enc_in 7 --batch_size 2048 --dropout 0.2 --d_model 128 --lradj 'sigmoid' --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T10:28:49.447540Z","iopub.execute_input":"2025-10-16T10:28:49.447882Z","iopub.status.idle":"2025-10-16T10:30:12.304052Z","shell.execute_reply.started":"2025-10-16T10:28:49.447854Z","shell.execute_reply":"2025-10-16T10:30:12.303247Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=24, d_model=128, data='ETTh1', root_path='./dataset', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=192, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.0009, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.2, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTh1_ftM_sl96_ll48_pl192_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 8353\nval 2689\ntest 2689\n[TRAIN MEMORY] Max memory allocated in epoch 1: 645.03 MB\nEpoch: 1 cost time: 1.5134687423706055\nEpoch: 1, Steps: 4 | Train Loss: 0.4706333 Vali Loss: 0.1756535 Test Loss: 0.5447786\nValidation loss decreased (inf --> 0.175653).  Saving model ...\nUpdating learning rate to 3.5580199280382172e-06\n[TRAIN MEMORY] Max memory allocated in epoch 2: 645.03 MB\nEpoch: 2 cost time: 0.7200653553009033\nEpoch: 2, Steps: 4 | Train Loss: 0.4264460 Vali Loss: 0.1761106 Test Loss: 0.5442031\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 9.535201756028628e-06\n[TRAIN MEMORY] Max memory allocated in epoch 3: 645.03 MB\nEpoch: 3 cost time: 0.7147016525268555\nEpoch: 3, Steps: 4 | Train Loss: 0.4260596 Vali Loss: 0.1766078 Test Loss: 0.5423679\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 1.939019460600719e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 645.03 MB\nEpoch: 4 cost time: 0.7340083122253418\nEpoch: 4, Steps: 4 | Train Loss: 0.4254244 Vali Loss: 0.1758043 Test Loss: 0.5384598\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 3.5336971821966205e-05\n[TRAIN MEMORY] Max memory allocated in epoch 5: 645.03 MB\nEpoch: 5 cost time: 0.7317085266113281\nEpoch: 5, Steps: 4 | Train Loss: 0.4237798 Vali Loss: 0.1743658 Test Loss: 0.5314747\nValidation loss decreased (0.175653 --> 0.174366).  Saving model ...\nUpdating learning rate to 6.0552625146778404e-05\n[TRAIN MEMORY] Max memory allocated in epoch 6: 645.03 MB\nEpoch: 6 cost time: 0.7309470176696777\nEpoch: 6, Steps: 4 | Train Loss: 0.4211998 Vali Loss: 0.1734182 Test Loss: 0.5204423\nValidation loss decreased (0.174366 --> 0.173418).  Saving model ...\nUpdating learning rate to 9.917066103234276e-05\n[TRAIN MEMORY] Max memory allocated in epoch 7: 645.03 MB\nEpoch: 7 cost time: 0.7400686740875244\nEpoch: 7, Steps: 4 | Train Loss: 0.4170017 Vali Loss: 0.1722651 Test Loss: 0.5053378\nValidation loss decreased (0.173418 --> 0.172265).  Saving model ...\nUpdating learning rate to 0.0001556590322019692\n[TRAIN MEMORY] Max memory allocated in epoch 8: 645.03 MB\nEpoch: 8 cost time: 0.7307190895080566\nEpoch: 8, Steps: 4 | Train Loss: 0.4115315 Vali Loss: 0.1683291 Test Loss: 0.4884300\nValidation loss decreased (0.172265 --> 0.168329).  Saving model ...\nUpdating learning rate to 0.00023309065755278173\n[TRAIN MEMORY] Max memory allocated in epoch 9: 645.03 MB\nEpoch: 9 cost time: 0.7590453624725342\nEpoch: 9, Steps: 4 | Train Loss: 0.4046477 Vali Loss: 0.1650683 Test Loss: 0.4748107\nValidation loss decreased (0.168329 --> 0.165068).  Saving model ...\nUpdating learning rate to 0.0003303755663096046\n[TRAIN MEMORY] Max memory allocated in epoch 10: 645.03 MB\nEpoch: 10 cost time: 0.7423098087310791\nEpoch: 10, Steps: 4 | Train Loss: 0.3977187 Vali Loss: 0.1616590 Test Loss: 0.4679129\nValidation loss decreased (0.165068 --> 0.161659).  Saving model ...\nUpdating learning rate to 0.00044011175163246615\n[TRAIN MEMORY] Max memory allocated in epoch 11: 645.03 MB\nEpoch: 11 cost time: 0.750817060470581\nEpoch: 11, Steps: 4 | Train Loss: 0.3922574 Vali Loss: 0.1609504 Test Loss: 0.4585970\nValidation loss decreased (0.161659 --> 0.160950).  Saving model ...\nUpdating learning rate to 0.000549824020846139\n[TRAIN MEMORY] Max memory allocated in epoch 12: 645.03 MB\nEpoch: 12 cost time: 0.7221493721008301\nEpoch: 12, Steps: 4 | Train Loss: 0.3872309 Vali Loss: 0.1578496 Test Loss: 0.4478351\nValidation loss decreased (0.160950 --> 0.157850).  Saving model ...\nUpdating learning rate to 0.0006470371292811575\n[TRAIN MEMORY] Max memory allocated in epoch 13: 645.03 MB\nEpoch: 13 cost time: 0.7286336421966553\nEpoch: 13, Steps: 4 | Train Loss: 0.3829586 Vali Loss: 0.1573051 Test Loss: 0.4455531\nValidation loss decreased (0.157850 --> 0.157305).  Saving model ...\nUpdating learning rate to 0.0007243489140565788\n[TRAIN MEMORY] Max memory allocated in epoch 14: 645.03 MB\nEpoch: 14 cost time: 0.741753339767456\nEpoch: 14, Steps: 4 | Train Loss: 0.3804220 Vali Loss: 0.1565080 Test Loss: 0.4424149\nValidation loss decreased (0.157305 --> 0.156508).  Saving model ...\nUpdating learning rate to 0.0007806691441351959\n[TRAIN MEMORY] Max memory allocated in epoch 15: 645.03 MB\nEpoch: 15 cost time: 0.7726507186889648\nEpoch: 15, Steps: 4 | Train Loss: 0.3777304 Vali Loss: 0.1554319 Test Loss: 0.4343126\nValidation loss decreased (0.156508 --> 0.155432).  Saving model ...\nUpdating learning rate to 0.0008190703736419599\n[TRAIN MEMORY] Max memory allocated in epoch 16: 645.03 MB\nEpoch: 16 cost time: 0.7656676769256592\nEpoch: 16, Steps: 4 | Train Loss: 0.3755116 Vali Loss: 0.1542257 Test Loss: 0.4259629\nValidation loss decreased (0.155432 --> 0.154226).  Saving model ...\nUpdating learning rate to 0.0008440200856162441\n[TRAIN MEMORY] Max memory allocated in epoch 17: 645.03 MB\nEpoch: 17 cost time: 0.7180588245391846\nEpoch: 17, Steps: 4 | Train Loss: 0.3747055 Vali Loss: 0.1537515 Test Loss: 0.4241419\nValidation loss decreased (0.154226 --> 0.153751).  Saving model ...\nUpdating learning rate to 0.0008596512114032111\n[TRAIN MEMORY] Max memory allocated in epoch 18: 645.03 MB\nEpoch: 18 cost time: 0.7201383113861084\nEpoch: 18, Steps: 4 | Train Loss: 0.3731563 Vali Loss: 0.1546842 Test Loss: 0.4231507\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008691401615998207\n[TRAIN MEMORY] Max memory allocated in epoch 19: 645.03 MB\nEpoch: 19 cost time: 0.7235324382781982\nEpoch: 19, Steps: 4 | Train Loss: 0.3725475 Vali Loss: 0.1533749 Test Loss: 0.4234143\nValidation loss decreased (0.153751 --> 0.153375).  Saving model ...\nUpdating learning rate to 0.0008747001216483112\n[TRAIN MEMORY] Max memory allocated in epoch 20: 645.03 MB\nEpoch: 20 cost time: 0.7160353660583496\nEpoch: 20, Steps: 4 | Train Loss: 0.3718108 Vali Loss: 0.1526651 Test Loss: 0.4237897\nValidation loss decreased (0.153375 --> 0.152665).  Saving model ...\nUpdating learning rate to 0.0008777888452022613\n[TRAIN MEMORY] Max memory allocated in epoch 21: 645.03 MB\nEpoch: 21 cost time: 0.7908451557159424\nEpoch: 21, Steps: 4 | Train Loss: 0.3705564 Vali Loss: 0.1524572 Test Loss: 0.4226946\nValidation loss decreased (0.152665 --> 0.152457).  Saving model ...\nUpdating learning rate to 0.0008793350103123584\n[TRAIN MEMORY] Max memory allocated in epoch 22: 645.03 MB\nEpoch: 22 cost time: 0.7179300785064697\nEpoch: 22, Steps: 4 | Train Loss: 0.3704461 Vali Loss: 0.1523637 Test Loss: 0.4213810\nValidation loss decreased (0.152457 --> 0.152364).  Saving model ...\nUpdating learning rate to 0.000879918363998359\n[TRAIN MEMORY] Max memory allocated in epoch 23: 645.03 MB\nEpoch: 23 cost time: 0.7142322063446045\nEpoch: 23, Steps: 4 | Train Loss: 0.3696222 Vali Loss: 0.1528537 Test Loss: 0.4196836\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008798962259021244\n[TRAIN MEMORY] Max memory allocated in epoch 24: 645.03 MB\nEpoch: 24 cost time: 0.710301399230957\nEpoch: 24, Steps: 4 | Train Loss: 0.3691433 Vali Loss: 0.1514316 Test Loss: 0.4190193\nValidation loss decreased (0.152364 --> 0.151432).  Saving model ...\nUpdating learning rate to 0.000879486910082522\n[TRAIN MEMORY] Max memory allocated in epoch 25: 645.03 MB\nEpoch: 25 cost time: 0.7181379795074463\nEpoch: 25, Steps: 4 | Train Loss: 0.3689735 Vali Loss: 0.1524471 Test Loss: 0.4192927\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008788228663077457\n[TRAIN MEMORY] Max memory allocated in epoch 26: 645.03 MB\nEpoch: 26 cost time: 0.7132043838500977\nEpoch: 26, Steps: 4 | Train Loss: 0.3684308 Vali Loss: 0.1529676 Test Loss: 0.4175451\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.000877983865606678\n[TRAIN MEMORY] Max memory allocated in epoch 27: 645.03 MB\nEpoch: 27 cost time: 0.7235894203186035\nEpoch: 27, Steps: 4 | Train Loss: 0.3681334 Vali Loss: 0.1518298 Test Loss: 0.4190121\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0008770174828155657\n[TRAIN MEMORY] Max memory allocated in epoch 28: 645.03 MB\nEpoch: 28 cost time: 0.7433013916015625\nEpoch: 28, Steps: 4 | Train Loss: 0.3676558 Vali Loss: 0.1516947 Test Loss: 0.4181552\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0008759516506624329\n[TRAIN MEMORY] Max memory allocated in epoch 29: 645.03 MB\nEpoch: 29 cost time: 0.7260868549346924\nEpoch: 29, Steps: 4 | Train Loss: 0.3678779 Vali Loss: 0.1515424 Test Loss: 0.4176638\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0008748023219895748\n[TRAIN MEMORY] Max memory allocated in epoch 30: 645.03 MB\nEpoch: 30 cost time: 0.7216122150421143\nEpoch: 30, Steps: 4 | Train Loss: 0.3674424 Vali Loss: 0.1512289 Test Loss: 0.4166633\nValidation loss decreased (0.151432 --> 0.151229).  Saving model ...\nUpdating learning rate to 0.0008735781342419471\n[TRAIN MEMORY] Max memory allocated in epoch 31: 645.03 MB\nEpoch: 31 cost time: 0.7319674491882324\nEpoch: 31, Steps: 4 | Train Loss: 0.3672189 Vali Loss: 0.1523417 Test Loss: 0.4189194\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008722832444565636\n[TRAIN MEMORY] Max memory allocated in epoch 32: 645.03 MB\nEpoch: 32 cost time: 0.7219126224517822\nEpoch: 32, Steps: 4 | Train Loss: 0.3670582 Vali Loss: 0.1524584 Test Loss: 0.4160599\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0008709190504917313\n[TRAIN MEMORY] Max memory allocated in epoch 33: 645.03 MB\nEpoch: 33 cost time: 0.7069711685180664\nEpoch: 33, Steps: 4 | Train Loss: 0.3668375 Vali Loss: 0.1528607 Test Loss: 0.4195738\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0008694852352648569\n[TRAIN MEMORY] Max memory allocated in epoch 34: 645.03 MB\nEpoch: 34 cost time: 0.7194685935974121\nEpoch: 34, Steps: 4 | Train Loss: 0.3665441 Vali Loss: 0.1503251 Test Loss: 0.4167378\nValidation loss decreased (0.151229 --> 0.150325).  Saving model ...\nUpdating learning rate to 0.0008679803998974854\n[TRAIN MEMORY] Max memory allocated in epoch 35: 645.03 MB\nEpoch: 35 cost time: 0.7412586212158203\nEpoch: 35, Steps: 4 | Train Loss: 0.3663310 Vali Loss: 0.1529381 Test Loss: 0.4175328\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008664024474149278\n[TRAIN MEMORY] Max memory allocated in epoch 36: 645.03 MB\nEpoch: 36 cost time: 0.7363371849060059\nEpoch: 36, Steps: 4 | Train Loss: 0.3662426 Vali Loss: 0.1522746 Test Loss: 0.4172730\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0008647488151910439\n[TRAIN MEMORY] Max memory allocated in epoch 37: 645.03 MB\nEpoch: 37 cost time: 0.7358372211456299\nEpoch: 37, Steps: 4 | Train Loss: 0.3660715 Vali Loss: 0.1518475 Test Loss: 0.4183408\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0008630166157580952\n[TRAIN MEMORY] Max memory allocated in epoch 38: 645.03 MB\nEpoch: 38 cost time: 0.7265458106994629\nEpoch: 38, Steps: 4 | Train Loss: 0.3661185 Vali Loss: 0.1513960 Test Loss: 0.4166106\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0008612027221777975\n[TRAIN MEMORY] Max memory allocated in epoch 39: 645.03 MB\nEpoch: 39 cost time: 0.7181086540222168\nEpoch: 39, Steps: 4 | Train Loss: 0.3657947 Vali Loss: 0.1524281 Test Loss: 0.4184495\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.000859303819952374\n[TRAIN MEMORY] Max memory allocated in epoch 40: 645.03 MB\nEpoch: 40 cost time: 0.7259433269500732\nEpoch: 40, Steps: 4 | Train Loss: 0.3655764 Vali Loss: 0.1511736 Test Loss: 0.4164685\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0008573164388281856\n[TRAIN MEMORY] Max memory allocated in epoch 41: 645.03 MB\nEpoch: 41 cost time: 0.758744478225708\nEpoch: 41, Steps: 4 | Train Loss: 0.3654163 Vali Loss: 0.1529967 Test Loss: 0.4178956\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0008552369726121073\n[TRAIN MEMORY] Max memory allocated in epoch 42: 645.03 MB\nEpoch: 42 cost time: 0.7302937507629395\nEpoch: 42, Steps: 4 | Train Loss: 0.3654606 Vali Loss: 0.1529880 Test Loss: 0.4160846\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0008530616919477782\n[TRAIN MEMORY] Max memory allocated in epoch 43: 645.03 MB\nEpoch: 43 cost time: 0.6996870040893555\nEpoch: 43, Steps: 4 | Train Loss: 0.3651602 Vali Loss: 0.1525214 Test Loss: 0.4186553\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.0008507867530752271\n[TRAIN MEMORY] Max memory allocated in epoch 44: 645.03 MB\nEpoch: 44 cost time: 0.7809183597564697\nEpoch: 44, Steps: 4 | Train Loss: 0.3651247 Vali Loss: 0.1528584 Test Loss: 0.4175379\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 0.75 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTh1_ftM_sl96_ll48_pl192_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 2689\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.78 MB\n[PARAMS] Total parameters: 202,213\nmse:0.416737824678421, mae:0.39892229437828064\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTh1 --learning_rate 0.0009 --data_path ETTh1.csv --seq_len 96 --pred_len 336 --enc_in 7 --batch_size 2048 --dropout 0.2 --d_model 128 --lradj 'sigmoid' --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T10:30:17.021536Z","iopub.execute_input":"2025-10-16T10:30:17.022199Z","iopub.status.idle":"2025-10-16T10:31:46.706447Z","shell.execute_reply.started":"2025-10-16T10:30:17.022166Z","shell.execute_reply":"2025-10-16T10:31:46.705763Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=24, d_model=128, data='ETTh1', root_path='./dataset', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=336, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.0009, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.2, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTh1_ftM_sl96_ll48_pl336_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 8209\nval 2545\ntest 2545\n[TRAIN MEMORY] Max memory allocated in epoch 1: 735.33 MB\nEpoch: 1 cost time: 1.6170949935913086\nEpoch: 1, Steps: 4 | Train Loss: 0.4664950 Vali Loss: 0.1849215 Test Loss: 0.5447285\nValidation loss decreased (inf --> 0.184921).  Saving model ...\nUpdating learning rate to 3.5580199280382172e-06\n[TRAIN MEMORY] Max memory allocated in epoch 2: 735.33 MB\nEpoch: 2 cost time: 0.8706672191619873\nEpoch: 2, Steps: 4 | Train Loss: 0.4298862 Vali Loss: 0.1854405 Test Loss: 0.5441784\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 9.535201756028628e-06\n[TRAIN MEMORY] Max memory allocated in epoch 3: 735.33 MB\nEpoch: 3 cost time: 0.9111266136169434\nEpoch: 3, Steps: 4 | Train Loss: 0.4296026 Vali Loss: 0.1854695 Test Loss: 0.5414090\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 1.939019460600719e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 735.33 MB\nEpoch: 4 cost time: 0.8573205471038818\nEpoch: 4, Steps: 4 | Train Loss: 0.4286368 Vali Loss: 0.1845237 Test Loss: 0.5351001\nValidation loss decreased (0.184921 --> 0.184524).  Saving model ...\nUpdating learning rate to 3.5336971821966205e-05\n[TRAIN MEMORY] Max memory allocated in epoch 5: 735.33 MB\nEpoch: 5 cost time: 0.8346457481384277\nEpoch: 5, Steps: 4 | Train Loss: 0.4268504 Vali Loss: 0.1831065 Test Loss: 0.5243412\nValidation loss decreased (0.184524 --> 0.183107).  Saving model ...\nUpdating learning rate to 6.0552625146778404e-05\n[TRAIN MEMORY] Max memory allocated in epoch 6: 735.33 MB\nEpoch: 6 cost time: 0.9172365665435791\nEpoch: 6, Steps: 4 | Train Loss: 0.4241522 Vali Loss: 0.1821582 Test Loss: 0.5099729\nValidation loss decreased (0.183107 --> 0.182158).  Saving model ...\nUpdating learning rate to 9.917066103234276e-05\n[TRAIN MEMORY] Max memory allocated in epoch 7: 735.33 MB\nEpoch: 7 cost time: 0.9192469120025635\nEpoch: 7, Steps: 4 | Train Loss: 0.4207469 Vali Loss: 0.1805157 Test Loss: 0.4956115\nValidation loss decreased (0.182158 --> 0.180516).  Saving model ...\nUpdating learning rate to 0.0001556590322019692\n[TRAIN MEMORY] Max memory allocated in epoch 8: 735.33 MB\nEpoch: 8 cost time: 0.9079256057739258\nEpoch: 8, Steps: 4 | Train Loss: 0.4172084 Vali Loss: 0.1799456 Test Loss: 0.4853093\nValidation loss decreased (0.180516 --> 0.179946).  Saving model ...\nUpdating learning rate to 0.00023309065755278173\n[TRAIN MEMORY] Max memory allocated in epoch 9: 735.33 MB\nEpoch: 9 cost time: 0.9260656833648682\nEpoch: 9, Steps: 4 | Train Loss: 0.4132932 Vali Loss: 0.1791988 Test Loss: 0.4810371\nValidation loss decreased (0.179946 --> 0.179199).  Saving model ...\nUpdating learning rate to 0.0003303755663096046\n[TRAIN MEMORY] Max memory allocated in epoch 10: 735.33 MB\nEpoch: 10 cost time: 0.8983643054962158\nEpoch: 10, Steps: 4 | Train Loss: 0.4087178 Vali Loss: 0.1764451 Test Loss: 0.4798640\nValidation loss decreased (0.179199 --> 0.176445).  Saving model ...\nUpdating learning rate to 0.00044011175163246615\n[TRAIN MEMORY] Max memory allocated in epoch 11: 735.33 MB\nEpoch: 11 cost time: 0.9271712303161621\nEpoch: 11, Steps: 4 | Train Loss: 0.4042099 Vali Loss: 0.1756419 Test Loss: 0.4773175\nValidation loss decreased (0.176445 --> 0.175642).  Saving model ...\nUpdating learning rate to 0.000549824020846139\n[TRAIN MEMORY] Max memory allocated in epoch 12: 735.33 MB\nEpoch: 12 cost time: 0.8508872985839844\nEpoch: 12, Steps: 4 | Train Loss: 0.4008669 Vali Loss: 0.1738951 Test Loss: 0.4768104\nValidation loss decreased (0.175642 --> 0.173895).  Saving model ...\nUpdating learning rate to 0.0006470371292811575\n[TRAIN MEMORY] Max memory allocated in epoch 13: 735.33 MB\nEpoch: 13 cost time: 0.951117992401123\nEpoch: 13, Steps: 4 | Train Loss: 0.3984941 Vali Loss: 0.1734146 Test Loss: 0.4702888\nValidation loss decreased (0.173895 --> 0.173415).  Saving model ...\nUpdating learning rate to 0.0007243489140565788\n[TRAIN MEMORY] Max memory allocated in epoch 14: 735.33 MB\nEpoch: 14 cost time: 0.9214131832122803\nEpoch: 14, Steps: 4 | Train Loss: 0.3959853 Vali Loss: 0.1728144 Test Loss: 0.4570114\nValidation loss decreased (0.173415 --> 0.172814).  Saving model ...\nUpdating learning rate to 0.0007806691441351959\n[TRAIN MEMORY] Max memory allocated in epoch 15: 735.33 MB\nEpoch: 15 cost time: 0.8927140235900879\nEpoch: 15, Steps: 4 | Train Loss: 0.3943230 Vali Loss: 0.1721596 Test Loss: 0.4493922\nValidation loss decreased (0.172814 --> 0.172160).  Saving model ...\nUpdating learning rate to 0.0008190703736419599\n[TRAIN MEMORY] Max memory allocated in epoch 16: 735.33 MB\nEpoch: 16 cost time: 0.8860311508178711\nEpoch: 16, Steps: 4 | Train Loss: 0.3931769 Vali Loss: 0.1717078 Test Loss: 0.4489440\nValidation loss decreased (0.172160 --> 0.171708).  Saving model ...\nUpdating learning rate to 0.0008440200856162441\n[TRAIN MEMORY] Max memory allocated in epoch 17: 735.33 MB\nEpoch: 17 cost time: 0.8442130088806152\nEpoch: 17, Steps: 4 | Train Loss: 0.3921116 Vali Loss: 0.1715551 Test Loss: 0.4493112\nValidation loss decreased (0.171708 --> 0.171555).  Saving model ...\nUpdating learning rate to 0.0008596512114032111\n[TRAIN MEMORY] Max memory allocated in epoch 18: 735.33 MB\nEpoch: 18 cost time: 0.8709969520568848\nEpoch: 18, Steps: 4 | Train Loss: 0.3912152 Vali Loss: 0.1721104 Test Loss: 0.4519184\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008691401615998207\n[TRAIN MEMORY] Max memory allocated in epoch 19: 735.33 MB\nEpoch: 19 cost time: 0.9030117988586426\nEpoch: 19, Steps: 4 | Train Loss: 0.3904327 Vali Loss: 0.1715631 Test Loss: 0.4517654\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0008747001216483112\n[TRAIN MEMORY] Max memory allocated in epoch 20: 735.33 MB\nEpoch: 20 cost time: 0.9032676219940186\nEpoch: 20, Steps: 4 | Train Loss: 0.3896973 Vali Loss: 0.1713004 Test Loss: 0.4476000\nValidation loss decreased (0.171555 --> 0.171300).  Saving model ...\nUpdating learning rate to 0.0008777888452022613\n[TRAIN MEMORY] Max memory allocated in epoch 21: 735.33 MB\nEpoch: 21 cost time: 0.8907604217529297\nEpoch: 21, Steps: 4 | Train Loss: 0.3892671 Vali Loss: 0.1711603 Test Loss: 0.4494822\nValidation loss decreased (0.171300 --> 0.171160).  Saving model ...\nUpdating learning rate to 0.0008793350103123584\n[TRAIN MEMORY] Max memory allocated in epoch 22: 735.33 MB\nEpoch: 22 cost time: 0.8885786533355713\nEpoch: 22, Steps: 4 | Train Loss: 0.3887581 Vali Loss: 0.1704602 Test Loss: 0.4445245\nValidation loss decreased (0.171160 --> 0.170460).  Saving model ...\nUpdating learning rate to 0.000879918363998359\n[TRAIN MEMORY] Max memory allocated in epoch 23: 735.33 MB\nEpoch: 23 cost time: 0.8960187435150146\nEpoch: 23, Steps: 4 | Train Loss: 0.3882394 Vali Loss: 0.1709165 Test Loss: 0.4476391\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008798962259021244\n[TRAIN MEMORY] Max memory allocated in epoch 24: 735.33 MB\nEpoch: 24 cost time: 0.8863677978515625\nEpoch: 24, Steps: 4 | Train Loss: 0.3878381 Vali Loss: 0.1710357 Test Loss: 0.4450110\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.000879486910082522\n[TRAIN MEMORY] Max memory allocated in epoch 25: 735.33 MB\nEpoch: 25 cost time: 0.9046247005462646\nEpoch: 25, Steps: 4 | Train Loss: 0.3875859 Vali Loss: 0.1714048 Test Loss: 0.4475833\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0008788228663077457\n[TRAIN MEMORY] Max memory allocated in epoch 26: 735.33 MB\nEpoch: 26 cost time: 0.8778285980224609\nEpoch: 26, Steps: 4 | Train Loss: 0.3871345 Vali Loss: 0.1715132 Test Loss: 0.4449778\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.000877983865606678\n[TRAIN MEMORY] Max memory allocated in epoch 27: 735.33 MB\nEpoch: 27 cost time: 0.9013333320617676\nEpoch: 27, Steps: 4 | Train Loss: 0.3869219 Vali Loss: 0.1720751 Test Loss: 0.4458401\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0008770174828155657\n[TRAIN MEMORY] Max memory allocated in epoch 28: 735.33 MB\nEpoch: 28 cost time: 0.9118285179138184\nEpoch: 28, Steps: 4 | Train Loss: 0.3867043 Vali Loss: 0.1712272 Test Loss: 0.4470279\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0008759516506624329\n[TRAIN MEMORY] Max memory allocated in epoch 29: 735.33 MB\nEpoch: 29 cost time: 0.868300199508667\nEpoch: 29, Steps: 4 | Train Loss: 0.3863257 Vali Loss: 0.1706100 Test Loss: 0.4465486\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0008748023219895748\n[TRAIN MEMORY] Max memory allocated in epoch 30: 735.33 MB\nEpoch: 30 cost time: 0.8679242134094238\nEpoch: 30, Steps: 4 | Train Loss: 0.3862504 Vali Loss: 0.1701053 Test Loss: 0.4429157\nValidation loss decreased (0.170460 --> 0.170105).  Saving model ...\nUpdating learning rate to 0.0008735781342419471\n[TRAIN MEMORY] Max memory allocated in epoch 31: 735.33 MB\nEpoch: 31 cost time: 0.9492802619934082\nEpoch: 31, Steps: 4 | Train Loss: 0.3859794 Vali Loss: 0.1714917 Test Loss: 0.4491598\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008722832444565636\n[TRAIN MEMORY] Max memory allocated in epoch 32: 735.33 MB\nEpoch: 32 cost time: 0.916353702545166\nEpoch: 32, Steps: 4 | Train Loss: 0.3856222 Vali Loss: 0.1711537 Test Loss: 0.4450178\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0008709190504917313\n[TRAIN MEMORY] Max memory allocated in epoch 33: 735.33 MB\nEpoch: 33 cost time: 0.8868467807769775\nEpoch: 33, Steps: 4 | Train Loss: 0.3855723 Vali Loss: 0.1713139 Test Loss: 0.4462957\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0008694852352648569\n[TRAIN MEMORY] Max memory allocated in epoch 34: 735.33 MB\nEpoch: 34 cost time: 0.9057159423828125\nEpoch: 34, Steps: 4 | Train Loss: 0.3852550 Vali Loss: 0.1712376 Test Loss: 0.4450875\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0008679803998974854\n[TRAIN MEMORY] Max memory allocated in epoch 35: 735.33 MB\nEpoch: 35 cost time: 0.9096469879150391\nEpoch: 35, Steps: 4 | Train Loss: 0.3852689 Vali Loss: 0.1717343 Test Loss: 0.4473970\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0008664024474149278\n[TRAIN MEMORY] Max memory allocated in epoch 36: 735.33 MB\nEpoch: 36 cost time: 0.8568346500396729\nEpoch: 36, Steps: 4 | Train Loss: 0.3852230 Vali Loss: 0.1705986 Test Loss: 0.4463937\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0008647488151910439\n[TRAIN MEMORY] Max memory allocated in epoch 37: 735.33 MB\nEpoch: 37 cost time: 0.8909111022949219\nEpoch: 37, Steps: 4 | Train Loss: 0.3850781 Vali Loss: 0.1719389 Test Loss: 0.4457613\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0008630166157580952\n[TRAIN MEMORY] Max memory allocated in epoch 38: 735.33 MB\nEpoch: 38 cost time: 0.9340214729309082\nEpoch: 38, Steps: 4 | Train Loss: 0.3848292 Vali Loss: 0.1713988 Test Loss: 0.4497271\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0008612027221777975\n[TRAIN MEMORY] Max memory allocated in epoch 39: 735.33 MB\nEpoch: 39 cost time: 0.8715090751647949\nEpoch: 39, Steps: 4 | Train Loss: 0.3845460 Vali Loss: 0.1710868 Test Loss: 0.4429215\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.000859303819952374\n[TRAIN MEMORY] Max memory allocated in epoch 40: 735.33 MB\nEpoch: 40 cost time: 0.866246223449707\nEpoch: 40, Steps: 4 | Train Loss: 0.3845289 Vali Loss: 0.1718763 Test Loss: 0.4504754\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 0.91 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTh1_ftM_sl96_ll48_pl336_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 2545\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 1.76 MB\n[PARAMS] Total parameters: 460,651\nmse:0.44291573762893677, mae:0.4159151315689087\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTh1 --learning_rate 0.005 --data_path ETTh1.csv --seq_len 96 --pred_len 720 --enc_in 7 --batch_size 128 --dropout 0.5 --d_model 256 --lradj 'sigmoid' --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T06:50:38.339386Z","iopub.execute_input":"2025-10-17T06:50:38.339677Z","iopub.status.idle":"2025-10-17T06:51:30.734560Z","shell.execute_reply.started":"2025-10-17T06:50:38.339646Z","shell.execute_reply":"2025-10-17T06:51:30.733843Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=24, d_model=256, data='ETTh1', root_path='./dataset', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=720, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.005, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.5, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTh1_ftM_sl96_ll48_pl720_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 7825\nval 2161\ntest 2161\n[TRAIN MEMORY] Max memory allocated in epoch 1: 131.90 MB\nEpoch: 1 cost time: 2.652740240097046\nEpoch: 1, Steps: 61 | Train Loss: 0.4773330 Vali Loss: 0.1882824 Test Loss: 0.4882809\nValidation loss decreased (inf --> 0.188282).  Saving model ...\nUpdating learning rate to 1.9766777377990098e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 131.90 MB\nEpoch: 2 cost time: 1.900238037109375\nEpoch: 2, Steps: 61 | Train Loss: 0.4364221 Vali Loss: 0.1868672 Test Loss: 0.4730836\nValidation loss decreased (0.188282 --> 0.186867).  Saving model ...\nUpdating learning rate to 5.297334308904794e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 131.90 MB\nEpoch: 3 cost time: 1.8706212043762207\nEpoch: 3, Steps: 61 | Train Loss: 0.4341783 Vali Loss: 0.1867281 Test Loss: 0.4712069\nValidation loss decreased (0.186867 --> 0.186728).  Saving model ...\nUpdating learning rate to 0.00010772330336670662\n[TRAIN MEMORY] Max memory allocated in epoch 4: 131.90 MB\nEpoch: 4 cost time: 1.8185832500457764\nEpoch: 4, Steps: 61 | Train Loss: 0.4333463 Vali Loss: 0.1864499 Test Loss: 0.4666393\nValidation loss decreased (0.186728 --> 0.186450).  Saving model ...\nUpdating learning rate to 0.00019631651012203447\n[TRAIN MEMORY] Max memory allocated in epoch 5: 131.90 MB\nEpoch: 5 cost time: 1.8408544063568115\nEpoch: 5, Steps: 61 | Train Loss: 0.4330847 Vali Loss: 0.1865231 Test Loss: 0.4714490\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00033640347303765786\n[TRAIN MEMORY] Max memory allocated in epoch 6: 131.90 MB\nEpoch: 6 cost time: 1.8228180408477783\nEpoch: 6, Steps: 61 | Train Loss: 0.4327405 Vali Loss: 0.1871236 Test Loss: 0.4793979\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0005509481168463487\n[TRAIN MEMORY] Max memory allocated in epoch 7: 131.90 MB\nEpoch: 7 cost time: 1.8967456817626953\nEpoch: 7, Steps: 61 | Train Loss: 0.4325701 Vali Loss: 0.1873337 Test Loss: 0.4709202\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0008647724011220512\n[TRAIN MEMORY] Max memory allocated in epoch 8: 131.90 MB\nEpoch: 8 cost time: 1.8145861625671387\nEpoch: 8, Steps: 61 | Train Loss: 0.4323491 Vali Loss: 0.1877658 Test Loss: 0.4644720\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.001294948097515454\n[TRAIN MEMORY] Max memory allocated in epoch 9: 131.90 MB\nEpoch: 9 cost time: 1.8368513584136963\nEpoch: 9, Steps: 61 | Train Loss: 0.4322617 Vali Loss: 0.1877183 Test Loss: 0.4723772\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0018354198128311366\n[TRAIN MEMORY] Max memory allocated in epoch 10: 131.90 MB\nEpoch: 10 cost time: 1.817866563796997\nEpoch: 10, Steps: 61 | Train Loss: 0.4325301 Vali Loss: 0.1877657 Test Loss: 0.4789357\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.002445065286847034\n[TRAIN MEMORY] Max memory allocated in epoch 11: 131.90 MB\nEpoch: 11 cost time: 1.8567235469818115\nEpoch: 11, Steps: 61 | Train Loss: 0.4325255 Vali Loss: 0.1895354 Test Loss: 0.4788193\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0030545778935896616\n[TRAIN MEMORY] Max memory allocated in epoch 12: 131.90 MB\nEpoch: 12 cost time: 1.8511309623718262\nEpoch: 12, Steps: 61 | Train Loss: 0.4318893 Vali Loss: 0.1881562 Test Loss: 0.4899735\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.003594650718228653\n[TRAIN MEMORY] Max memory allocated in epoch 13: 131.90 MB\nEpoch: 13 cost time: 1.8414812088012695\nEpoch: 13, Steps: 61 | Train Loss: 0.4310140 Vali Loss: 0.1873873 Test Loss: 0.5165220\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.00402416063364766\n[TRAIN MEMORY] Max memory allocated in epoch 14: 131.90 MB\nEpoch: 14 cost time: 1.9045078754425049\nEpoch: 14, Steps: 61 | Train Loss: 0.4294916 Vali Loss: 0.1878817 Test Loss: 0.5009474\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 1.91 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTh1_ftM_sl96_ll48_pl720_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 2161\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 6.94 MB\n[PARAMS] Total parameters: 1,816,443\nmse:0.46663931012153625, mae:0.4452425539493561\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTh2 --learning_rate 0.005 --data_path ETTh2.csv --seq_len 96 --pred_len 96 --enc_in 7 --batch_size 2048 --dropout 0 --d_model 128 --lradj 'sigmoid' --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T06:51:30.735915Z","iopub.execute_input":"2025-10-17T06:51:30.736225Z","iopub.status.idle":"2025-10-17T06:52:22.369810Z","shell.execute_reply.started":"2025-10-17T06:51:30.736196Z","shell.execute_reply":"2025-10-17T06:52:22.369116Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=24, d_model=128, data='ETTh2', root_path='./dataset', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=96, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.005, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.0, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTh2_ftM_sl96_ll48_pl96_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 8449\nval 2785\ntest 2785\n[TRAIN MEMORY] Max memory allocated in epoch 1: 606.39 MB\nEpoch: 1 cost time: 1.432260274887085\nEpoch: 1, Steps: 4 | Train Loss: 0.3502312 Vali Loss: 0.0836295 Test Loss: 0.2410943\nValidation loss decreased (inf --> 0.083629).  Saving model ...\nUpdating learning rate to 1.9766777377990098e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 606.39 MB\nEpoch: 2 cost time: 0.6560595035552979\nEpoch: 2, Steps: 4 | Train Loss: 0.3238537 Vali Loss: 0.0839145 Test Loss: 0.2409367\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 5.297334308904794e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 606.39 MB\nEpoch: 3 cost time: 0.6641395092010498\nEpoch: 3, Steps: 4 | Train Loss: 0.3228504 Vali Loss: 0.0835405 Test Loss: 0.2402381\nValidation loss decreased (0.083629 --> 0.083540).  Saving model ...\nUpdating learning rate to 0.00010772330336670662\n[TRAIN MEMORY] Max memory allocated in epoch 4: 606.39 MB\nEpoch: 4 cost time: 0.6616766452789307\nEpoch: 4, Steps: 4 | Train Loss: 0.3225112 Vali Loss: 0.0820807 Test Loss: 0.2390505\nValidation loss decreased (0.083540 --> 0.082081).  Saving model ...\nUpdating learning rate to 0.00019631651012203447\n[TRAIN MEMORY] Max memory allocated in epoch 5: 606.39 MB\nEpoch: 5 cost time: 0.6603550910949707\nEpoch: 5, Steps: 4 | Train Loss: 0.3198899 Vali Loss: 0.0821537 Test Loss: 0.2376187\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00033640347303765786\n[TRAIN MEMORY] Max memory allocated in epoch 6: 606.39 MB\nEpoch: 6 cost time: 0.653508186340332\nEpoch: 6, Steps: 4 | Train Loss: 0.3171958 Vali Loss: 0.0819683 Test Loss: 0.2358529\nValidation loss decreased (0.082081 --> 0.081968).  Saving model ...\nUpdating learning rate to 0.0005509481168463487\n[TRAIN MEMORY] Max memory allocated in epoch 7: 606.39 MB\nEpoch: 7 cost time: 0.6528687477111816\nEpoch: 7, Steps: 4 | Train Loss: 0.3141964 Vali Loss: 0.0810798 Test Loss: 0.2326573\nValidation loss decreased (0.081968 --> 0.081080).  Saving model ...\nUpdating learning rate to 0.0008647724011220512\n[TRAIN MEMORY] Max memory allocated in epoch 8: 606.39 MB\nEpoch: 8 cost time: 0.6459922790527344\nEpoch: 8, Steps: 4 | Train Loss: 0.3133079 Vali Loss: 0.0800598 Test Loss: 0.2299993\nValidation loss decreased (0.081080 --> 0.080060).  Saving model ...\nUpdating learning rate to 0.001294948097515454\n[TRAIN MEMORY] Max memory allocated in epoch 9: 606.39 MB\nEpoch: 9 cost time: 0.6539936065673828\nEpoch: 9, Steps: 4 | Train Loss: 0.3101915 Vali Loss: 0.0798670 Test Loss: 0.2316829\nValidation loss decreased (0.080060 --> 0.079867).  Saving model ...\nUpdating learning rate to 0.0018354198128311366\n[TRAIN MEMORY] Max memory allocated in epoch 10: 606.39 MB\nEpoch: 10 cost time: 0.6745102405548096\nEpoch: 10, Steps: 4 | Train Loss: 0.3076498 Vali Loss: 0.0792094 Test Loss: 0.2321249\nValidation loss decreased (0.079867 --> 0.079209).  Saving model ...\nUpdating learning rate to 0.002445065286847034\n[TRAIN MEMORY] Max memory allocated in epoch 11: 606.39 MB\nEpoch: 11 cost time: 0.6642708778381348\nEpoch: 11, Steps: 4 | Train Loss: 0.3049820 Vali Loss: 0.0793917 Test Loss: 0.2305738\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0030545778935896616\n[TRAIN MEMORY] Max memory allocated in epoch 12: 606.39 MB\nEpoch: 12 cost time: 0.6657776832580566\nEpoch: 12, Steps: 4 | Train Loss: 0.3010043 Vali Loss: 0.0786959 Test Loss: 0.2290932\nValidation loss decreased (0.079209 --> 0.078696).  Saving model ...\nUpdating learning rate to 0.003594650718228653\n[TRAIN MEMORY] Max memory allocated in epoch 13: 606.39 MB\nEpoch: 13 cost time: 0.6657876968383789\nEpoch: 13, Steps: 4 | Train Loss: 0.3009308 Vali Loss: 0.0783518 Test Loss: 0.2288683\nValidation loss decreased (0.078696 --> 0.078352).  Saving model ...\nUpdating learning rate to 0.00402416063364766\n[TRAIN MEMORY] Max memory allocated in epoch 14: 606.39 MB\nEpoch: 14 cost time: 0.6598460674285889\nEpoch: 14, Steps: 4 | Train Loss: 0.3002252 Vali Loss: 0.0792778 Test Loss: 0.2314166\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.004337050800751087\n[TRAIN MEMORY] Max memory allocated in epoch 15: 606.39 MB\nEpoch: 15 cost time: 0.6810154914855957\nEpoch: 15, Steps: 4 | Train Loss: 0.2991099 Vali Loss: 0.0799039 Test Loss: 0.2349986\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0045503909646775545\n[TRAIN MEMORY] Max memory allocated in epoch 16: 606.39 MB\nEpoch: 16 cost time: 0.6748092174530029\nEpoch: 16, Steps: 4 | Train Loss: 0.2976648 Vali Loss: 0.0787585 Test Loss: 0.2282235\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.004689000475645802\n[TRAIN MEMORY] Max memory allocated in epoch 17: 606.39 MB\nEpoch: 17 cost time: 0.6581332683563232\nEpoch: 17, Steps: 4 | Train Loss: 0.2977246 Vali Loss: 0.0786026 Test Loss: 0.2316109\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.004775840063351173\n[TRAIN MEMORY] Max memory allocated in epoch 18: 606.39 MB\nEpoch: 18 cost time: 0.6686220169067383\nEpoch: 18, Steps: 4 | Train Loss: 0.2960221 Vali Loss: 0.0782422 Test Loss: 0.2277143\nValidation loss decreased (0.078352 --> 0.078242).  Saving model ...\nUpdating learning rate to 0.004828556453332337\n[TRAIN MEMORY] Max memory allocated in epoch 19: 606.39 MB\nEpoch: 19 cost time: 0.6873838901519775\nEpoch: 19, Steps: 4 | Train Loss: 0.2949826 Vali Loss: 0.0790294 Test Loss: 0.2314307\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0048594451202683955\n[TRAIN MEMORY] Max memory allocated in epoch 20: 606.39 MB\nEpoch: 20 cost time: 0.6595745086669922\nEpoch: 20, Steps: 4 | Train Loss: 0.2939234 Vali Loss: 0.0792156 Test Loss: 0.2382127\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.004876604695568118\n[TRAIN MEMORY] Max memory allocated in epoch 21: 606.39 MB\nEpoch: 21 cost time: 0.6756100654602051\nEpoch: 21, Steps: 4 | Train Loss: 0.2945726 Vali Loss: 0.0788889 Test Loss: 0.2312541\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.004885194501735324\n[TRAIN MEMORY] Max memory allocated in epoch 22: 606.39 MB\nEpoch: 22 cost time: 0.6766881942749023\nEpoch: 22, Steps: 4 | Train Loss: 0.2931290 Vali Loss: 0.0793593 Test Loss: 0.2306312\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0048884353555464395\n[TRAIN MEMORY] Max memory allocated in epoch 23: 606.39 MB\nEpoch: 23 cost time: 0.6635777950286865\nEpoch: 23, Steps: 4 | Train Loss: 0.2922715 Vali Loss: 0.0790619 Test Loss: 0.2326953\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.004888312366122913\n[TRAIN MEMORY] Max memory allocated in epoch 24: 606.39 MB\nEpoch: 24 cost time: 0.6823992729187012\nEpoch: 24, Steps: 4 | Train Loss: 0.2914972 Vali Loss: 0.0787806 Test Loss: 0.2306576\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.004886038389347345\n[TRAIN MEMORY] Max memory allocated in epoch 25: 606.39 MB\nEpoch: 25 cost time: 0.6483476161956787\nEpoch: 25, Steps: 4 | Train Loss: 0.2902584 Vali Loss: 0.0800428 Test Loss: 0.2321310\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0048823492572652545\n[TRAIN MEMORY] Max memory allocated in epoch 26: 606.39 MB\nEpoch: 26 cost time: 0.6492552757263184\nEpoch: 26, Steps: 4 | Train Loss: 0.2916916 Vali Loss: 0.0797570 Test Loss: 0.2338566\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.004877688142259322\n[TRAIN MEMORY] Max memory allocated in epoch 27: 606.39 MB\nEpoch: 27 cost time: 0.7042636871337891\nEpoch: 27, Steps: 4 | Train Loss: 0.2898077 Vali Loss: 0.0796564 Test Loss: 0.2356138\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.004872319348975366\n[TRAIN MEMORY] Max memory allocated in epoch 28: 606.39 MB\nEpoch: 28 cost time: 0.6728408336639404\nEpoch: 28, Steps: 4 | Train Loss: 0.2892688 Vali Loss: 0.0791880 Test Loss: 0.2338953\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 0.69 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTh2_ftM_sl96_ll48_pl96_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 2785\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.39 MB\n[PARAMS] Total parameters: 99,041\nmse:0.22771431505680084, mae:0.2971794605255127\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTh2 --learning_rate 0.004 --data_path ETTh2.csv --seq_len 96 --pred_len 192 --enc_in 7 --batch_size 2048 --dropout 0.3 --d_model 128 --lradj 'sigmoid' --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T06:59:48.648750Z","iopub.execute_input":"2025-10-17T06:59:48.649202Z","iopub.status.idle":"2025-10-17T07:00:49.538942Z","shell.execute_reply.started":"2025-10-17T06:59:48.649165Z","shell.execute_reply":"2025-10-17T07:00:49.538083Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=24, d_model=128, data='ETTh2', root_path='./dataset', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=192, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.004, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.3, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTh2_ftM_sl96_ll48_pl192_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 8353\nval 2689\ntest 2689\n[TRAIN MEMORY] Max memory allocated in epoch 1: 645.03 MB\nEpoch: 1 cost time: 1.6730282306671143\nEpoch: 1, Steps: 4 | Train Loss: 0.3740949 Vali Loss: 0.0856703 Test Loss: 0.2994859\nValidation loss decreased (inf --> 0.085670).  Saving model ...\nUpdating learning rate to 1.581342190239208e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 645.03 MB\nEpoch: 2 cost time: 0.7924392223358154\nEpoch: 2, Steps: 4 | Train Loss: 0.3495132 Vali Loss: 0.0858966 Test Loss: 0.2992488\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 4.237867447123836e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 645.03 MB\nEpoch: 3 cost time: 0.8534603118896484\nEpoch: 3, Steps: 4 | Train Loss: 0.3493758 Vali Loss: 0.0857305 Test Loss: 0.2982230\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 8.61786426933653e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 645.03 MB\nEpoch: 4 cost time: 0.8697052001953125\nEpoch: 4, Steps: 4 | Train Loss: 0.3482055 Vali Loss: 0.0852309 Test Loss: 0.2962106\nValidation loss decreased (0.085670 --> 0.085231).  Saving model ...\nUpdating learning rate to 0.00015705320809762755\n[TRAIN MEMORY] Max memory allocated in epoch 5: 645.03 MB\nEpoch: 5 cost time: 0.8511300086975098\nEpoch: 5, Steps: 4 | Train Loss: 0.3461521 Vali Loss: 0.0846915 Test Loss: 0.2935863\nValidation loss decreased (0.085231 --> 0.084691).  Saving model ...\nUpdating learning rate to 0.00026912277843012624\n[TRAIN MEMORY] Max memory allocated in epoch 6: 645.03 MB\nEpoch: 6 cost time: 0.8601779937744141\nEpoch: 6, Steps: 4 | Train Loss: 0.3445820 Vali Loss: 0.0841436 Test Loss: 0.2917722\nValidation loss decreased (0.084691 --> 0.084144).  Saving model ...\nUpdating learning rate to 0.00044075849347707894\n[TRAIN MEMORY] Max memory allocated in epoch 7: 645.03 MB\nEpoch: 7 cost time: 0.8435254096984863\nEpoch: 7, Steps: 4 | Train Loss: 0.3432595 Vali Loss: 0.0841298 Test Loss: 0.2905597\nValidation loss decreased (0.084144 --> 0.084130).  Saving model ...\nUpdating learning rate to 0.0006918179208976411\n[TRAIN MEMORY] Max memory allocated in epoch 8: 645.03 MB\nEpoch: 8 cost time: 0.8845360279083252\nEpoch: 8, Steps: 4 | Train Loss: 0.3418075 Vali Loss: 0.0843345 Test Loss: 0.2900430\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0010359584780123632\n[TRAIN MEMORY] Max memory allocated in epoch 9: 645.03 MB\nEpoch: 9 cost time: 0.8792815208435059\nEpoch: 9, Steps: 4 | Train Loss: 0.3403032 Vali Loss: 0.0832618 Test Loss: 0.2898848\nValidation loss decreased (0.084130 --> 0.083262).  Saving model ...\nUpdating learning rate to 0.0014683358502649096\n[TRAIN MEMORY] Max memory allocated in epoch 10: 645.03 MB\nEpoch: 10 cost time: 0.8331730365753174\nEpoch: 10, Steps: 4 | Train Loss: 0.3372814 Vali Loss: 0.0829697 Test Loss: 0.2881109\nValidation loss decreased (0.083262 --> 0.082970).  Saving model ...\nUpdating learning rate to 0.0019560522294776272\n[TRAIN MEMORY] Max memory allocated in epoch 11: 645.03 MB\nEpoch: 11 cost time: 0.8374068737030029\nEpoch: 11, Steps: 4 | Train Loss: 0.3354266 Vali Loss: 0.0832545 Test Loss: 0.2888174\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.002443662314871729\n[TRAIN MEMORY] Max memory allocated in epoch 12: 645.03 MB\nEpoch: 12 cost time: 0.844735860824585\nEpoch: 12, Steps: 4 | Train Loss: 0.3337300 Vali Loss: 0.0832333 Test Loss: 0.2890701\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0028757205745829225\n[TRAIN MEMORY] Max memory allocated in epoch 13: 645.03 MB\nEpoch: 13 cost time: 0.8829329013824463\nEpoch: 13, Steps: 4 | Train Loss: 0.3317651 Vali Loss: 0.0825278 Test Loss: 0.2896757\nValidation loss decreased (0.082970 --> 0.082528).  Saving model ...\nUpdating learning rate to 0.003219328506918128\n[TRAIN MEMORY] Max memory allocated in epoch 14: 645.03 MB\nEpoch: 14 cost time: 0.8777782917022705\nEpoch: 14, Steps: 4 | Train Loss: 0.3308808 Vali Loss: 0.0829483 Test Loss: 0.2951124\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0034696406406008705\n[TRAIN MEMORY] Max memory allocated in epoch 15: 645.03 MB\nEpoch: 15 cost time: 0.8305859565734863\nEpoch: 15, Steps: 4 | Train Loss: 0.3302700 Vali Loss: 0.0830347 Test Loss: 0.2937513\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.003640312771742044\n[TRAIN MEMORY] Max memory allocated in epoch 16: 645.03 MB\nEpoch: 16 cost time: 0.8495461940765381\nEpoch: 16, Steps: 4 | Train Loss: 0.3288171 Vali Loss: 0.0820311 Test Loss: 0.2897774\nValidation loss decreased (0.082528 --> 0.082031).  Saving model ...\nUpdating learning rate to 0.003751200380516641\n[TRAIN MEMORY] Max memory allocated in epoch 17: 645.03 MB\nEpoch: 17 cost time: 0.8438100814819336\nEpoch: 17, Steps: 4 | Train Loss: 0.3276828 Vali Loss: 0.0836208 Test Loss: 0.2910489\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0038206720506809393\n[TRAIN MEMORY] Max memory allocated in epoch 18: 645.03 MB\nEpoch: 18 cost time: 0.8535075187683105\nEpoch: 18, Steps: 4 | Train Loss: 0.3277613 Vali Loss: 0.0831511 Test Loss: 0.2936606\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.00386284516266587\n[TRAIN MEMORY] Max memory allocated in epoch 19: 645.03 MB\nEpoch: 19 cost time: 0.8272011280059814\nEpoch: 19, Steps: 4 | Train Loss: 0.3273170 Vali Loss: 0.0823459 Test Loss: 0.2923983\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0038875560962147167\n[TRAIN MEMORY] Max memory allocated in epoch 20: 645.03 MB\nEpoch: 20 cost time: 0.8322243690490723\nEpoch: 20, Steps: 4 | Train Loss: 0.3276207 Vali Loss: 0.0828806 Test Loss: 0.2917831\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0039012837564544947\n[TRAIN MEMORY] Max memory allocated in epoch 21: 645.03 MB\nEpoch: 21 cost time: 0.903397798538208\nEpoch: 21, Steps: 4 | Train Loss: 0.3253066 Vali Loss: 0.0827362 Test Loss: 0.2944144\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0039081556013882595\n[TRAIN MEMORY] Max memory allocated in epoch 22: 645.03 MB\nEpoch: 22 cost time: 0.871272087097168\nEpoch: 22, Steps: 4 | Train Loss: 0.3251353 Vali Loss: 0.0828966 Test Loss: 0.2951609\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.003910748284437151\n[TRAIN MEMORY] Max memory allocated in epoch 23: 645.03 MB\nEpoch: 23 cost time: 0.8653578758239746\nEpoch: 23, Steps: 4 | Train Loss: 0.3252769 Vali Loss: 0.0830230 Test Loss: 0.2989003\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.00391064989289833\n[TRAIN MEMORY] Max memory allocated in epoch 24: 645.03 MB\nEpoch: 24 cost time: 0.8686478137969971\nEpoch: 24, Steps: 4 | Train Loss: 0.3245552 Vali Loss: 0.0827624 Test Loss: 0.2939001\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.003908830711477876\n[TRAIN MEMORY] Max memory allocated in epoch 25: 645.03 MB\nEpoch: 25 cost time: 0.8732068538665771\nEpoch: 25, Steps: 4 | Train Loss: 0.3238755 Vali Loss: 0.0823315 Test Loss: 0.2936497\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.003905879405812203\n[TRAIN MEMORY] Max memory allocated in epoch 26: 645.03 MB\nEpoch: 26 cost time: 0.8876469135284424\nEpoch: 26, Steps: 4 | Train Loss: 0.3234781 Vali Loss: 0.0832010 Test Loss: 0.2951273\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 0.89 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTh2_ftM_sl96_ll48_pl192_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 2689\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.78 MB\n[PARAMS] Total parameters: 202,213\nmse:0.28977739810943604, mae:0.3376955986022949\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTh2 --learning_rate 0.004 --data_path ETTh2.csv --seq_len 96 --pred_len 336 --enc_in 7 --batch_size 2048 --dropout 0 --d_model 128 --lradj 'sigmoid' --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T07:03:23.268739Z","iopub.execute_input":"2025-10-17T07:03:23.269036Z","iopub.status.idle":"2025-10-17T07:04:21.941075Z","shell.execute_reply.started":"2025-10-17T07:03:23.269012Z","shell.execute_reply":"2025-10-17T07:04:21.939963Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=24, d_model=128, data='ETTh2', root_path='./dataset', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=336, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.004, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.0, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTh2_ftM_sl96_ll48_pl336_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 8209\nval 2545\ntest 2545\n[TRAIN MEMORY] Max memory allocated in epoch 1: 734.17 MB\nEpoch: 1 cost time: 1.8355274200439453\nEpoch: 1, Steps: 4 | Train Loss: 0.4064443 Vali Loss: 0.0945927 Test Loss: 0.3510004\nValidation loss decreased (inf --> 0.094593).  Saving model ...\nUpdating learning rate to 1.581342190239208e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 732.92 MB\nEpoch: 2 cost time: 0.9968883991241455\nEpoch: 2, Steps: 4 | Train Loss: 0.3821594 Vali Loss: 0.0945287 Test Loss: 0.3505352\nValidation loss decreased (0.094593 --> 0.094529).  Saving model ...\nUpdating learning rate to 4.237867447123836e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 732.92 MB\nEpoch: 3 cost time: 1.0670573711395264\nEpoch: 3, Steps: 4 | Train Loss: 0.3809113 Vali Loss: 0.0935988 Test Loss: 0.3486432\nValidation loss decreased (0.094529 --> 0.093599).  Saving model ...\nUpdating learning rate to 8.61786426933653e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 732.92 MB\nEpoch: 4 cost time: 0.9954538345336914\nEpoch: 4, Steps: 4 | Train Loss: 0.3786057 Vali Loss: 0.0931072 Test Loss: 0.3460436\nValidation loss decreased (0.093599 --> 0.093107).  Saving model ...\nUpdating learning rate to 0.00015705320809762755\n[TRAIN MEMORY] Max memory allocated in epoch 5: 732.92 MB\nEpoch: 5 cost time: 1.016312837600708\nEpoch: 5, Steps: 4 | Train Loss: 0.3755170 Vali Loss: 0.0925651 Test Loss: 0.3455057\nValidation loss decreased (0.093107 --> 0.092565).  Saving model ...\nUpdating learning rate to 0.00026912277843012624\n[TRAIN MEMORY] Max memory allocated in epoch 6: 732.92 MB\nEpoch: 6 cost time: 1.0521767139434814\nEpoch: 6, Steps: 4 | Train Loss: 0.3738597 Vali Loss: 0.0924772 Test Loss: 0.3459117\nValidation loss decreased (0.092565 --> 0.092477).  Saving model ...\nUpdating learning rate to 0.00044075849347707894\n[TRAIN MEMORY] Max memory allocated in epoch 7: 732.92 MB\nEpoch: 7 cost time: 1.0404582023620605\nEpoch: 7, Steps: 4 | Train Loss: 0.3724926 Vali Loss: 0.0925419 Test Loss: 0.3409330\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0006918179208976411\n[TRAIN MEMORY] Max memory allocated in epoch 8: 732.92 MB\nEpoch: 8 cost time: 1.0397191047668457\nEpoch: 8, Steps: 4 | Train Loss: 0.3715937 Vali Loss: 0.0919751 Test Loss: 0.3392665\nValidation loss decreased (0.092477 --> 0.091975).  Saving model ...\nUpdating learning rate to 0.0010359584780123632\n[TRAIN MEMORY] Max memory allocated in epoch 9: 732.92 MB\nEpoch: 9 cost time: 1.0724079608917236\nEpoch: 9, Steps: 4 | Train Loss: 0.3699149 Vali Loss: 0.0910778 Test Loss: 0.3412257\nValidation loss decreased (0.091975 --> 0.091078).  Saving model ...\nUpdating learning rate to 0.0014683358502649096\n[TRAIN MEMORY] Max memory allocated in epoch 10: 732.92 MB\nEpoch: 10 cost time: 1.017235517501831\nEpoch: 10, Steps: 4 | Train Loss: 0.3674120 Vali Loss: 0.0911872 Test Loss: 0.3408669\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0019560522294776272\n[TRAIN MEMORY] Max memory allocated in epoch 11: 732.92 MB\nEpoch: 11 cost time: 1.0374119281768799\nEpoch: 11, Steps: 4 | Train Loss: 0.3655507 Vali Loss: 0.0906439 Test Loss: 0.3387407\nValidation loss decreased (0.091078 --> 0.090644).  Saving model ...\nUpdating learning rate to 0.002443662314871729\n[TRAIN MEMORY] Max memory allocated in epoch 12: 732.92 MB\nEpoch: 12 cost time: 1.026956558227539\nEpoch: 12, Steps: 4 | Train Loss: 0.3644074 Vali Loss: 0.0915026 Test Loss: 0.3430499\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0028757205745829225\n[TRAIN MEMORY] Max memory allocated in epoch 13: 732.92 MB\nEpoch: 13 cost time: 0.9964172840118408\nEpoch: 13, Steps: 4 | Train Loss: 0.3626327 Vali Loss: 0.0915757 Test Loss: 0.3436180\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.003219328506918128\n[TRAIN MEMORY] Max memory allocated in epoch 14: 732.92 MB\nEpoch: 14 cost time: 0.9867329597473145\nEpoch: 14, Steps: 4 | Train Loss: 0.3611857 Vali Loss: 0.0920854 Test Loss: 0.3373548\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0034696406406008705\n[TRAIN MEMORY] Max memory allocated in epoch 15: 732.92 MB\nEpoch: 15 cost time: 1.0459709167480469\nEpoch: 15, Steps: 4 | Train Loss: 0.3606731 Vali Loss: 0.0918832 Test Loss: 0.3418058\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.003640312771742044\n[TRAIN MEMORY] Max memory allocated in epoch 16: 732.92 MB\nEpoch: 16 cost time: 1.0168092250823975\nEpoch: 16, Steps: 4 | Train Loss: 0.3596570 Vali Loss: 0.0919972 Test Loss: 0.3440947\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.003751200380516641\n[TRAIN MEMORY] Max memory allocated in epoch 17: 732.92 MB\nEpoch: 17 cost time: 0.9903976917266846\nEpoch: 17, Steps: 4 | Train Loss: 0.3591042 Vali Loss: 0.0921483 Test Loss: 0.3395549\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0038206720506809393\n[TRAIN MEMORY] Max memory allocated in epoch 18: 732.92 MB\nEpoch: 18 cost time: 0.9895391464233398\nEpoch: 18, Steps: 4 | Train Loss: 0.3581125 Vali Loss: 0.0917669 Test Loss: 0.3478715\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.00386284516266587\n[TRAIN MEMORY] Max memory allocated in epoch 19: 732.92 MB\nEpoch: 19 cost time: 1.0261611938476562\nEpoch: 19, Steps: 4 | Train Loss: 0.3573010 Vali Loss: 0.0912989 Test Loss: 0.3389541\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0038875560962147167\n[TRAIN MEMORY] Max memory allocated in epoch 20: 732.92 MB\nEpoch: 20 cost time: 1.0010628700256348\nEpoch: 20, Steps: 4 | Train Loss: 0.3561887 Vali Loss: 0.0912571 Test Loss: 0.3414478\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.0039012837564544947\n[TRAIN MEMORY] Max memory allocated in epoch 21: 732.92 MB\nEpoch: 21 cost time: 1.0143098831176758\nEpoch: 21, Steps: 4 | Train Loss: 0.3549384 Vali Loss: 0.0919455 Test Loss: 0.3453142\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 1.06 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTh2_ftM_sl96_ll48_pl336_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 2545\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 1.76 MB\n[PARAMS] Total parameters: 460,651\nmse:0.33874067664146423, mae:0.3763096034526825\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTh2 --learning_rate 0.002 --data_path ETTh2.csv --seq_len 96 --pred_len 720 --enc_in 7 --batch_size 2048 --dropout 0.1 --d_model 128 --lradj 'sigmoid' --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T07:11:48.172158Z","iopub.execute_input":"2025-10-17T07:11:48.172479Z","iopub.status.idle":"2025-10-17T07:12:54.848891Z","shell.execute_reply.started":"2025-10-17T07:11:48.172450Z","shell.execute_reply":"2025-10-17T07:12:54.848091Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=24, d_model=128, data='ETTh2', root_path='./dataset', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=720, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.002, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.1, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTh2_ftM_sl96_ll48_pl720_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 7825\nval 2161\ntest 2161\n[TRAIN MEMORY] Max memory allocated in epoch 1: 1121.67 MB\nEpoch: 1 cost time: 1.9539644718170166\nEpoch: 1, Steps: 3 | Train Loss: 0.4748982 Vali Loss: 0.1224312 Test Loss: 0.4305660\nValidation loss decreased (inf --> 0.122431).  Saving model ...\nUpdating learning rate to 7.90671095119604e-06\n[TRAIN MEMORY] Max memory allocated in epoch 2: 1121.67 MB\nEpoch: 2 cost time: 1.186884880065918\nEpoch: 2, Steps: 3 | Train Loss: 0.4552912 Vali Loss: 0.1222392 Test Loss: 0.4298624\nValidation loss decreased (0.122431 --> 0.122239).  Saving model ...\nUpdating learning rate to 2.118933723561918e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 1121.67 MB\nEpoch: 3 cost time: 1.1687242984771729\nEpoch: 3, Steps: 3 | Train Loss: 0.4540299 Vali Loss: 0.1218649 Test Loss: 0.4269019\nValidation loss decreased (0.122239 --> 0.121865).  Saving model ...\nUpdating learning rate to 4.308932134668265e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 1121.67 MB\nEpoch: 4 cost time: 1.0946054458618164\nEpoch: 4, Steps: 3 | Train Loss: 0.4499893 Vali Loss: 0.1215320 Test Loss: 0.4209442\nValidation loss decreased (0.121865 --> 0.121532).  Saving model ...\nUpdating learning rate to 7.852660404881378e-05\n[TRAIN MEMORY] Max memory allocated in epoch 5: 1121.67 MB\nEpoch: 5 cost time: 1.2083303928375244\nEpoch: 5, Steps: 3 | Train Loss: 0.4463934 Vali Loss: 0.1205967 Test Loss: 0.4135251\nValidation loss decreased (0.121532 --> 0.120597).  Saving model ...\nUpdating learning rate to 0.00013456138921506312\n[TRAIN MEMORY] Max memory allocated in epoch 6: 1121.67 MB\nEpoch: 6 cost time: 1.2003693580627441\nEpoch: 6, Steps: 3 | Train Loss: 0.4433533 Vali Loss: 0.1203681 Test Loss: 0.4087882\nValidation loss decreased (0.120597 --> 0.120368).  Saving model ...\nUpdating learning rate to 0.00022037924673853947\n[TRAIN MEMORY] Max memory allocated in epoch 7: 1121.67 MB\nEpoch: 7 cost time: 1.1235792636871338\nEpoch: 7, Steps: 3 | Train Loss: 0.4434814 Vali Loss: 0.1202285 Test Loss: 0.4093129\nValidation loss decreased (0.120368 --> 0.120229).  Saving model ...\nUpdating learning rate to 0.00034590896044882054\n[TRAIN MEMORY] Max memory allocated in epoch 8: 1121.67 MB\nEpoch: 8 cost time: 1.1409590244293213\nEpoch: 8, Steps: 3 | Train Loss: 0.4432886 Vali Loss: 0.1204354 Test Loss: 0.4113773\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0005179792390061816\n[TRAIN MEMORY] Max memory allocated in epoch 9: 1121.67 MB\nEpoch: 9 cost time: 1.1325671672821045\nEpoch: 9, Steps: 3 | Train Loss: 0.4430561 Vali Loss: 0.1197703 Test Loss: 0.4071428\nValidation loss decreased (0.120229 --> 0.119770).  Saving model ...\nUpdating learning rate to 0.0007341679251324548\n[TRAIN MEMORY] Max memory allocated in epoch 10: 1121.67 MB\nEpoch: 10 cost time: 1.1329846382141113\nEpoch: 10, Steps: 3 | Train Loss: 0.4384050 Vali Loss: 0.1194627 Test Loss: 0.4071368\nValidation loss decreased (0.119770 --> 0.119463).  Saving model ...\nUpdating learning rate to 0.0009780261147388136\n[TRAIN MEMORY] Max memory allocated in epoch 11: 1121.67 MB\nEpoch: 11 cost time: 1.0761640071868896\nEpoch: 11, Steps: 3 | Train Loss: 0.4420296 Vali Loss: 0.1189055 Test Loss: 0.4041640\nValidation loss decreased (0.119463 --> 0.118905).  Saving model ...\nUpdating learning rate to 0.0012218311574358645\n[TRAIN MEMORY] Max memory allocated in epoch 12: 1121.67 MB\nEpoch: 12 cost time: 1.131777286529541\nEpoch: 12, Steps: 3 | Train Loss: 0.4392574 Vali Loss: 0.1189560 Test Loss: 0.4082702\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0014378602872914612\n[TRAIN MEMORY] Max memory allocated in epoch 13: 1121.67 MB\nEpoch: 13 cost time: 1.180570363998413\nEpoch: 13, Steps: 3 | Train Loss: 0.4350632 Vali Loss: 0.1198399 Test Loss: 0.4103815\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.001609664253459064\n[TRAIN MEMORY] Max memory allocated in epoch 14: 1121.67 MB\nEpoch: 14 cost time: 1.141540288925171\nEpoch: 14, Steps: 3 | Train Loss: 0.4353197 Vali Loss: 0.1193133 Test Loss: 0.4072522\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0017348203203004352\n[TRAIN MEMORY] Max memory allocated in epoch 15: 1121.67 MB\nEpoch: 15 cost time: 1.1626238822937012\nEpoch: 15, Steps: 3 | Train Loss: 0.4342999 Vali Loss: 0.1193631 Test Loss: 0.4082549\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.001820156385871022\n[TRAIN MEMORY] Max memory allocated in epoch 16: 1121.67 MB\nEpoch: 16 cost time: 1.0772087574005127\nEpoch: 16, Steps: 3 | Train Loss: 0.4339292 Vali Loss: 0.1191568 Test Loss: 0.4081645\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0018756001902583204\n[TRAIN MEMORY] Max memory allocated in epoch 17: 1121.67 MB\nEpoch: 17 cost time: 1.1306769847869873\nEpoch: 17, Steps: 3 | Train Loss: 0.4314977 Vali Loss: 0.1192988 Test Loss: 0.4089610\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0019103360253404696\n[TRAIN MEMORY] Max memory allocated in epoch 18: 1121.67 MB\nEpoch: 18 cost time: 1.102283000946045\nEpoch: 18, Steps: 3 | Train Loss: 0.4284538 Vali Loss: 0.1195890 Test Loss: 0.4042664\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.001931422581332935\n[TRAIN MEMORY] Max memory allocated in epoch 19: 1121.67 MB\nEpoch: 19 cost time: 1.152573585510254\nEpoch: 19, Steps: 3 | Train Loss: 0.4295251 Vali Loss: 0.1192233 Test Loss: 0.4080628\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0019437780481073583\n[TRAIN MEMORY] Max memory allocated in epoch 20: 1121.67 MB\nEpoch: 20 cost time: 1.0869693756103516\nEpoch: 20, Steps: 3 | Train Loss: 0.4286824 Vali Loss: 0.1191650 Test Loss: 0.4044408\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.0019506418782272474\n[TRAIN MEMORY] Max memory allocated in epoch 21: 1121.67 MB\nEpoch: 21 cost time: 1.1809897422790527\nEpoch: 21, Steps: 3 | Train Loss: 0.4287967 Vali Loss: 0.1189226 Test Loss: 0.4048898\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 1.18 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTh2_ftM_sl96_ll48_pl720_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 2161\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 6.71 MB\n[PARAMS] Total parameters: 1,758,075\nmse:0.40416398644447327, mae:0.42695942521095276\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTm1 --learning_rate 0.008 --data_path ETTm1.csv --seq_len 96 --pred_len 96 --enc_in 7 --batch_size 2048 --dropout 0.1 --lradj 'sigmoid' --period_len 4 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T03:44:09.271529Z","iopub.execute_input":"2025-10-19T03:44:09.272110Z","iopub.status.idle":"2025-10-19T03:45:40.793886Z","shell.execute_reply.started":"2025-10-19T03:44:09.272071Z","shell.execute_reply":"2025-10-19T03:45:40.793211Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=4, d_model=128, data='ETTm1', root_path='./dataset', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=96, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.008, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.1, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTm1_ftM_sl96_ll48_pl96_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 34369\nval 11425\ntest 11425\n[TRAIN MEMORY] Max memory allocated in epoch 1: 219.76 MB\nEpoch: 1 cost time: 2.27337384223938\nEpoch: 1, Steps: 16 | Train Loss: 0.3584042 Vali Loss: 0.1045779 Test Loss: 0.3362018\nValidation loss decreased (inf --> 0.104578).  Saving model ...\nUpdating learning rate to 3.162684380478416e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 219.76 MB\nEpoch: 2 cost time: 1.4453074932098389\nEpoch: 2, Steps: 16 | Train Loss: 0.3307539 Vali Loss: 0.1037235 Test Loss: 0.3342183\nValidation loss decreased (0.104578 --> 0.103724).  Saving model ...\nUpdating learning rate to 8.475734894247672e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 219.76 MB\nEpoch: 3 cost time: 1.4686803817749023\nEpoch: 3, Steps: 16 | Train Loss: 0.3289231 Vali Loss: 0.1029157 Test Loss: 0.3322877\nValidation loss decreased (0.103724 --> 0.102916).  Saving model ...\nUpdating learning rate to 0.0001723572853867306\n[TRAIN MEMORY] Max memory allocated in epoch 4: 219.76 MB\nEpoch: 4 cost time: 1.4505186080932617\nEpoch: 4, Steps: 16 | Train Loss: 0.3274494 Vali Loss: 0.1021770 Test Loss: 0.3300648\nValidation loss decreased (0.102916 --> 0.102177).  Saving model ...\nUpdating learning rate to 0.0003141064161952551\n[TRAIN MEMORY] Max memory allocated in epoch 5: 219.76 MB\nEpoch: 5 cost time: 1.4974403381347656\nEpoch: 5, Steps: 16 | Train Loss: 0.3261822 Vali Loss: 0.1020127 Test Loss: 0.3288740\nValidation loss decreased (0.102177 --> 0.102013).  Saving model ...\nUpdating learning rate to 0.0005382455568602525\n[TRAIN MEMORY] Max memory allocated in epoch 6: 219.76 MB\nEpoch: 6 cost time: 1.4555325508117676\nEpoch: 6, Steps: 16 | Train Loss: 0.3250547 Vali Loss: 0.1019662 Test Loss: 0.3275191\nValidation loss decreased (0.102013 --> 0.101966).  Saving model ...\nUpdating learning rate to 0.0008815169869541579\n[TRAIN MEMORY] Max memory allocated in epoch 7: 219.76 MB\nEpoch: 7 cost time: 1.4157037734985352\nEpoch: 7, Steps: 16 | Train Loss: 0.3234498 Vali Loss: 0.1014620 Test Loss: 0.3261881\nValidation loss decreased (0.101966 --> 0.101462).  Saving model ...\nUpdating learning rate to 0.0013836358417952822\n[TRAIN MEMORY] Max memory allocated in epoch 8: 219.76 MB\nEpoch: 8 cost time: 1.480337142944336\nEpoch: 8, Steps: 16 | Train Loss: 0.3214695 Vali Loss: 0.1016354 Test Loss: 0.3233894\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0020719169560247264\n[TRAIN MEMORY] Max memory allocated in epoch 9: 219.76 MB\nEpoch: 9 cost time: 1.4465324878692627\nEpoch: 9, Steps: 16 | Train Loss: 0.3192137 Vali Loss: 0.1015465 Test Loss: 0.3205512\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.002936671700529819\n[TRAIN MEMORY] Max memory allocated in epoch 10: 219.76 MB\nEpoch: 10 cost time: 1.4382824897766113\nEpoch: 10, Steps: 16 | Train Loss: 0.3164236 Vali Loss: 0.1020661 Test Loss: 0.3151826\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0039121044589552545\n[TRAIN MEMORY] Max memory allocated in epoch 11: 219.76 MB\nEpoch: 11 cost time: 1.4367306232452393\nEpoch: 11, Steps: 16 | Train Loss: 0.3133829 Vali Loss: 0.1026678 Test Loss: 0.3151902\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.004887324629743458\n[TRAIN MEMORY] Max memory allocated in epoch 12: 219.76 MB\nEpoch: 12 cost time: 1.4459648132324219\nEpoch: 12, Steps: 16 | Train Loss: 0.3097498 Vali Loss: 0.1017298 Test Loss: 0.3101710\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.005751441149165845\n[TRAIN MEMORY] Max memory allocated in epoch 13: 219.76 MB\nEpoch: 13 cost time: 1.4572856426239014\nEpoch: 13, Steps: 16 | Train Loss: 0.3071045 Vali Loss: 0.1013397 Test Loss: 0.3127157\nValidation loss decreased (0.101462 --> 0.101340).  Saving model ...\nUpdating learning rate to 0.006438657013836256\n[TRAIN MEMORY] Max memory allocated in epoch 14: 219.76 MB\nEpoch: 14 cost time: 1.4691298007965088\nEpoch: 14, Steps: 16 | Train Loss: 0.3048810 Vali Loss: 0.1016214 Test Loss: 0.3122553\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006939281281201741\n[TRAIN MEMORY] Max memory allocated in epoch 15: 219.76 MB\nEpoch: 15 cost time: 1.485748052597046\nEpoch: 15, Steps: 16 | Train Loss: 0.3024245 Vali Loss: 0.1017412 Test Loss: 0.3086348\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.007280625543484088\n[TRAIN MEMORY] Max memory allocated in epoch 16: 219.76 MB\nEpoch: 16 cost time: 1.4316933155059814\nEpoch: 16, Steps: 16 | Train Loss: 0.3006500 Vali Loss: 0.1011102 Test Loss: 0.3055785\nValidation loss decreased (0.101340 --> 0.101110).  Saving model ...\nUpdating learning rate to 0.007502400761033282\n[TRAIN MEMORY] Max memory allocated in epoch 17: 219.76 MB\nEpoch: 17 cost time: 1.4269235134124756\nEpoch: 17, Steps: 16 | Train Loss: 0.2985435 Vali Loss: 0.1012925 Test Loss: 0.3046415\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0076413441013618785\n[TRAIN MEMORY] Max memory allocated in epoch 18: 219.76 MB\nEpoch: 18 cost time: 1.4523022174835205\nEpoch: 18, Steps: 16 | Train Loss: 0.2975208 Vali Loss: 0.1009624 Test Loss: 0.3062562\nValidation loss decreased (0.101110 --> 0.100962).  Saving model ...\nUpdating learning rate to 0.00772569032533174\n[TRAIN MEMORY] Max memory allocated in epoch 19: 219.76 MB\nEpoch: 19 cost time: 1.4800806045532227\nEpoch: 19, Steps: 16 | Train Loss: 0.2963070 Vali Loss: 0.1014066 Test Loss: 0.3048399\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.007775112192429433\n[TRAIN MEMORY] Max memory allocated in epoch 20: 219.76 MB\nEpoch: 20 cost time: 1.495743989944458\nEpoch: 20, Steps: 16 | Train Loss: 0.2955382 Vali Loss: 0.1008949 Test Loss: 0.3064830\nValidation loss decreased (0.100962 --> 0.100895).  Saving model ...\nUpdating learning rate to 0.007802567512908989\n[TRAIN MEMORY] Max memory allocated in epoch 21: 219.76 MB\nEpoch: 21 cost time: 1.4189095497131348\nEpoch: 21, Steps: 16 | Train Loss: 0.2945630 Vali Loss: 0.1018672 Test Loss: 0.3077612\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.007816311202776519\n[TRAIN MEMORY] Max memory allocated in epoch 22: 219.76 MB\nEpoch: 22 cost time: 1.433955430984497\nEpoch: 22, Steps: 16 | Train Loss: 0.2934655 Vali Loss: 0.1010521 Test Loss: 0.3051397\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.007821496568874303\n[TRAIN MEMORY] Max memory allocated in epoch 23: 219.76 MB\nEpoch: 23 cost time: 1.4523420333862305\nEpoch: 23, Steps: 16 | Train Loss: 0.2923810 Vali Loss: 0.1017600 Test Loss: 0.3081234\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.00782129978579666\n[TRAIN MEMORY] Max memory allocated in epoch 24: 219.76 MB\nEpoch: 24 cost time: 1.4127743244171143\nEpoch: 24, Steps: 16 | Train Loss: 0.2909371 Vali Loss: 0.1011338 Test Loss: 0.3087092\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.007817661422955752\n[TRAIN MEMORY] Max memory allocated in epoch 25: 219.76 MB\nEpoch: 25 cost time: 1.4465878009796143\nEpoch: 25, Steps: 16 | Train Loss: 0.2913336 Vali Loss: 0.1022333 Test Loss: 0.3126817\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.007811758811624406\n[TRAIN MEMORY] Max memory allocated in epoch 26: 219.76 MB\nEpoch: 26 cost time: 1.5128488540649414\nEpoch: 26, Steps: 16 | Train Loss: 0.2903714 Vali Loss: 0.1014783 Test Loss: 0.3091277\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.007804301027614915\n[TRAIN MEMORY] Max memory allocated in epoch 27: 219.76 MB\nEpoch: 27 cost time: 1.414109706878662\nEpoch: 27, Steps: 16 | Train Loss: 0.2892491 Vali Loss: 0.1015519 Test Loss: 0.3113097\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.007795710958360585\n[TRAIN MEMORY] Max memory allocated in epoch 28: 219.76 MB\nEpoch: 28 cost time: 1.4380557537078857\nEpoch: 28, Steps: 16 | Train Loss: 0.2882121 Vali Loss: 0.1015638 Test Loss: 0.3090309\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.007786236894777182\n[TRAIN MEMORY] Max memory allocated in epoch 29: 219.76 MB\nEpoch: 29 cost time: 1.4533500671386719\nEpoch: 29, Steps: 16 | Train Loss: 0.2873455 Vali Loss: 0.1014822 Test Loss: 0.3098608\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.007776020639907333\n[TRAIN MEMORY] Max memory allocated in epoch 30: 219.76 MB\nEpoch: 30 cost time: 1.4800665378570557\nEpoch: 30, Steps: 16 | Train Loss: 0.2870152 Vali Loss: 0.1013601 Test Loss: 0.3120005\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 1.48 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTm1_ftM_sl96_ll48_pl96_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 11425\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.42 MB\n[PARAMS] Total parameters: 109,161\nmse:0.30648303031921387, mae:0.3451153337955475\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTm1 --learning_rate 0.002 --data_path ETTm1.csv --seq_len 96 --pred_len 192 --enc_in 7 --batch_size 2048 --dropout 0.2 --lradj 'sigmoid' --period_len 4 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T04:03:20.842657Z","iopub.execute_input":"2025-10-19T04:03:20.843273Z","iopub.status.idle":"2025-10-19T04:05:40.101680Z","shell.execute_reply.started":"2025-10-19T04:03:20.843240Z","shell.execute_reply":"2025-10-19T04:05:40.100968Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=4, d_model=128, data='ETTm1', root_path='./dataset', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=192, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.002, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.2, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTm1_ftM_sl96_ll48_pl192_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 34273\nval 11329\ntest 11329\n[TRAIN MEMORY] Max memory allocated in epoch 1: 320.62 MB\nEpoch: 1 cost time: 2.5625193119049072\nEpoch: 1, Steps: 16 | Train Loss: 0.3611279 Vali Loss: 0.1118102 Test Loss: 0.3763897\nValidation loss decreased (inf --> 0.111810).  Saving model ...\nUpdating learning rate to 7.90671095119604e-06\n[TRAIN MEMORY] Max memory allocated in epoch 2: 320.50 MB\nEpoch: 2 cost time: 1.8506979942321777\nEpoch: 2, Steps: 16 | Train Loss: 0.3385667 Vali Loss: 0.1114177 Test Loss: 0.3753406\nValidation loss decreased (0.111810 --> 0.111418).  Saving model ...\nUpdating learning rate to 2.118933723561918e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 320.50 MB\nEpoch: 3 cost time: 1.7524096965789795\nEpoch: 3, Steps: 16 | Train Loss: 0.3376456 Vali Loss: 0.1112701 Test Loss: 0.3723647\nValidation loss decreased (0.111418 --> 0.111270).  Saving model ...\nUpdating learning rate to 4.308932134668265e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 320.50 MB\nEpoch: 4 cost time: 1.7882468700408936\nEpoch: 4, Steps: 16 | Train Loss: 0.3364385 Vali Loss: 0.1103793 Test Loss: 0.3698618\nValidation loss decreased (0.111270 --> 0.110379).  Saving model ...\nUpdating learning rate to 7.852660404881378e-05\n[TRAIN MEMORY] Max memory allocated in epoch 5: 320.50 MB\nEpoch: 5 cost time: 1.7796430587768555\nEpoch: 5, Steps: 16 | Train Loss: 0.3350969 Vali Loss: 0.1103247 Test Loss: 0.3680346\nValidation loss decreased (0.110379 --> 0.110325).  Saving model ...\nUpdating learning rate to 0.00013456138921506312\n[TRAIN MEMORY] Max memory allocated in epoch 6: 320.50 MB\nEpoch: 6 cost time: 1.746994972229004\nEpoch: 6, Steps: 16 | Train Loss: 0.3340519 Vali Loss: 0.1098401 Test Loss: 0.3663567\nValidation loss decreased (0.110325 --> 0.109840).  Saving model ...\nUpdating learning rate to 0.00022037924673853947\n[TRAIN MEMORY] Max memory allocated in epoch 7: 320.50 MB\nEpoch: 7 cost time: 1.8084323406219482\nEpoch: 7, Steps: 16 | Train Loss: 0.3332702 Vali Loss: 0.1098588 Test Loss: 0.3648489\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00034590896044882054\n[TRAIN MEMORY] Max memory allocated in epoch 8: 320.50 MB\nEpoch: 8 cost time: 1.8184869289398193\nEpoch: 8, Steps: 16 | Train Loss: 0.3316445 Vali Loss: 0.1098897 Test Loss: 0.3637936\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0005179792390061816\n[TRAIN MEMORY] Max memory allocated in epoch 9: 320.50 MB\nEpoch: 9 cost time: 1.817251443862915\nEpoch: 9, Steps: 16 | Train Loss: 0.3302384 Vali Loss: 0.1096627 Test Loss: 0.3618419\nValidation loss decreased (0.109840 --> 0.109663).  Saving model ...\nUpdating learning rate to 0.0007341679251324548\n[TRAIN MEMORY] Max memory allocated in epoch 10: 320.50 MB\nEpoch: 10 cost time: 1.7539386749267578\nEpoch: 10, Steps: 16 | Train Loss: 0.3295176 Vali Loss: 0.1097110 Test Loss: 0.3612505\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009780261147388136\n[TRAIN MEMORY] Max memory allocated in epoch 11: 320.50 MB\nEpoch: 11 cost time: 1.8119268417358398\nEpoch: 11, Steps: 16 | Train Loss: 0.3279243 Vali Loss: 0.1097486 Test Loss: 0.3576974\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0012218311574358645\n[TRAIN MEMORY] Max memory allocated in epoch 12: 320.50 MB\nEpoch: 12 cost time: 1.80576753616333\nEpoch: 12, Steps: 16 | Train Loss: 0.3267898 Vali Loss: 0.1093968 Test Loss: 0.3570760\nValidation loss decreased (0.109663 --> 0.109397).  Saving model ...\nUpdating learning rate to 0.0014378602872914612\n[TRAIN MEMORY] Max memory allocated in epoch 13: 320.50 MB\nEpoch: 13 cost time: 1.7930381298065186\nEpoch: 13, Steps: 16 | Train Loss: 0.3257241 Vali Loss: 0.1098412 Test Loss: 0.3577126\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.001609664253459064\n[TRAIN MEMORY] Max memory allocated in epoch 14: 320.50 MB\nEpoch: 14 cost time: 1.7494778633117676\nEpoch: 14, Steps: 16 | Train Loss: 0.3246547 Vali Loss: 0.1106071 Test Loss: 0.3566926\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0017348203203004352\n[TRAIN MEMORY] Max memory allocated in epoch 15: 320.50 MB\nEpoch: 15 cost time: 1.8060963153839111\nEpoch: 15, Steps: 16 | Train Loss: 0.3230979 Vali Loss: 0.1102025 Test Loss: 0.3518039\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.001820156385871022\n[TRAIN MEMORY] Max memory allocated in epoch 16: 320.50 MB\nEpoch: 16 cost time: 1.7904250621795654\nEpoch: 16, Steps: 16 | Train Loss: 0.3221735 Vali Loss: 0.1098738 Test Loss: 0.3524399\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0018756001902583204\n[TRAIN MEMORY] Max memory allocated in epoch 17: 320.50 MB\nEpoch: 17 cost time: 1.8080849647521973\nEpoch: 17, Steps: 16 | Train Loss: 0.3207162 Vali Loss: 0.1100976 Test Loss: 0.3524375\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0019103360253404696\n[TRAIN MEMORY] Max memory allocated in epoch 18: 320.50 MB\nEpoch: 18 cost time: 1.775635004043579\nEpoch: 18, Steps: 16 | Train Loss: 0.3196447 Vali Loss: 0.1095278 Test Loss: 0.3535043\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.001931422581332935\n[TRAIN MEMORY] Max memory allocated in epoch 19: 320.50 MB\nEpoch: 19 cost time: 1.842280626296997\nEpoch: 19, Steps: 16 | Train Loss: 0.3186097 Vali Loss: 0.1092890 Test Loss: 0.3523936\nValidation loss decreased (0.109397 --> 0.109289).  Saving model ...\nUpdating learning rate to 0.0019437780481073583\n[TRAIN MEMORY] Max memory allocated in epoch 20: 320.50 MB\nEpoch: 20 cost time: 1.7540318965911865\nEpoch: 20, Steps: 16 | Train Loss: 0.3181016 Vali Loss: 0.1096462 Test Loss: 0.3506192\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0019506418782272474\n[TRAIN MEMORY] Max memory allocated in epoch 21: 320.50 MB\nEpoch: 21 cost time: 1.8474795818328857\nEpoch: 21, Steps: 16 | Train Loss: 0.3168810 Vali Loss: 0.1094338 Test Loss: 0.3539129\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0019540778006941297\n[TRAIN MEMORY] Max memory allocated in epoch 22: 320.50 MB\nEpoch: 22 cost time: 1.751413106918335\nEpoch: 22, Steps: 16 | Train Loss: 0.3163075 Vali Loss: 0.1092198 Test Loss: 0.3516329\nValidation loss decreased (0.109289 --> 0.109220).  Saving model ...\nUpdating learning rate to 0.0019553741422185756\n[TRAIN MEMORY] Max memory allocated in epoch 23: 320.50 MB\nEpoch: 23 cost time: 1.7665269374847412\nEpoch: 23, Steps: 16 | Train Loss: 0.3158336 Vali Loss: 0.1091284 Test Loss: 0.3546775\nValidation loss decreased (0.109220 --> 0.109128).  Saving model ...\nUpdating learning rate to 0.001955324946449165\n[TRAIN MEMORY] Max memory allocated in epoch 24: 320.50 MB\nEpoch: 24 cost time: 1.7772605419158936\nEpoch: 24, Steps: 16 | Train Loss: 0.3146258 Vali Loss: 0.1095115 Test Loss: 0.3511110\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.001954415355738938\n[TRAIN MEMORY] Max memory allocated in epoch 25: 320.50 MB\nEpoch: 25 cost time: 1.7996103763580322\nEpoch: 25, Steps: 16 | Train Loss: 0.3143323 Vali Loss: 0.1090128 Test Loss: 0.3505362\nValidation loss decreased (0.109128 --> 0.109013).  Saving model ...\nUpdating learning rate to 0.0019529397029061015\n[TRAIN MEMORY] Max memory allocated in epoch 26: 320.50 MB\nEpoch: 26 cost time: 1.7451813220977783\nEpoch: 26, Steps: 16 | Train Loss: 0.3138559 Vali Loss: 0.1096889 Test Loss: 0.3499014\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0019510752569037288\n[TRAIN MEMORY] Max memory allocated in epoch 27: 320.50 MB\nEpoch: 27 cost time: 1.7862985134124756\nEpoch: 27, Steps: 16 | Train Loss: 0.3133711 Vali Loss: 0.1093402 Test Loss: 0.3520386\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0019489277395901463\n[TRAIN MEMORY] Max memory allocated in epoch 28: 320.50 MB\nEpoch: 28 cost time: 1.7349133491516113\nEpoch: 28, Steps: 16 | Train Loss: 0.3132028 Vali Loss: 0.1088917 Test Loss: 0.3505357\nValidation loss decreased (0.109013 --> 0.108892).  Saving model ...\nUpdating learning rate to 0.0019465592236942956\n[TRAIN MEMORY] Max memory allocated in epoch 29: 320.50 MB\nEpoch: 29 cost time: 1.7398910522460938\nEpoch: 29, Steps: 16 | Train Loss: 0.3126784 Vali Loss: 0.1092220 Test Loss: 0.3530245\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0019440051599768332\n[TRAIN MEMORY] Max memory allocated in epoch 30: 320.50 MB\nEpoch: 30 cost time: 1.7670581340789795\nEpoch: 30, Steps: 16 | Train Loss: 0.3122941 Vali Loss: 0.1094379 Test Loss: 0.3544618\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0019412847427598826\n[TRAIN MEMORY] Max memory allocated in epoch 31: 320.50 MB\nEpoch: 31 cost time: 1.7811903953552246\nEpoch: 31, Steps: 16 | Train Loss: 0.3116933 Vali Loss: 0.1091721 Test Loss: 0.3508657\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0019384072099034749\n[TRAIN MEMORY] Max memory allocated in epoch 32: 320.50 MB\nEpoch: 32 cost time: 1.803980827331543\nEpoch: 32, Steps: 16 | Train Loss: 0.3111496 Vali Loss: 0.1096158 Test Loss: 0.3555870\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0019353756677594031\n[TRAIN MEMORY] Max memory allocated in epoch 33: 320.50 MB\nEpoch: 33 cost time: 1.7366273403167725\nEpoch: 33, Steps: 16 | Train Loss: 0.3111915 Vali Loss: 0.1093622 Test Loss: 0.3489628\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0019321894116996821\n[TRAIN MEMORY] Max memory allocated in epoch 34: 320.50 MB\nEpoch: 34 cost time: 1.7950108051300049\nEpoch: 34, Steps: 16 | Train Loss: 0.3105220 Vali Loss: 0.1094391 Test Loss: 0.3523318\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0019288453331055232\n[TRAIN MEMORY] Max memory allocated in epoch 35: 320.50 MB\nEpoch: 35 cost time: 1.7747747898101807\nEpoch: 35, Steps: 16 | Train Loss: 0.3099137 Vali Loss: 0.1092678 Test Loss: 0.3514507\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.001925338772033173\n[TRAIN MEMORY] Max memory allocated in epoch 36: 320.50 MB\nEpoch: 36 cost time: 1.7908422946929932\nEpoch: 36, Steps: 16 | Train Loss: 0.3097044 Vali Loss: 0.1091833 Test Loss: 0.3516842\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0019216640337578754\n[TRAIN MEMORY] Max memory allocated in epoch 37: 320.50 MB\nEpoch: 37 cost time: 1.7782528400421143\nEpoch: 37, Steps: 16 | Train Loss: 0.3095859 Vali Loss: 0.1090088 Test Loss: 0.3527933\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.0019178147016846562\n[TRAIN MEMORY] Max memory allocated in epoch 38: 320.50 MB\nEpoch: 38 cost time: 1.7972664833068848\nEpoch: 38, Steps: 16 | Train Loss: 0.3093406 Vali Loss: 0.1094419 Test Loss: 0.3511615\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 1.80 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTm1_ftM_sl96_ll48_pl192_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 11329\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.84 MB\n[PARAMS] Total parameters: 217,473\nmse:0.35053572058677673, mae:0.3689365088939667\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTm1 --learning_rate 0.003 --data_path ETTm1.csv --seq_len 96 --pred_len 336 --enc_in 7 --batch_size 2048 --dropout 0.4 --lradj 'sigmoid' --period_len 4 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T04:23:14.627692Z","iopub.execute_input":"2025-10-19T04:23:14.628443Z","iopub.status.idle":"2025-10-19T04:25:53.675028Z","shell.execute_reply.started":"2025-10-19T04:23:14.628411Z","shell.execute_reply":"2025-10-19T04:25:53.674142Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=4, d_model=128, data='ETTm1', root_path='./dataset', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=336, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.003, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.4, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTm1_ftM_sl96_ll48_pl336_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 34129\nval 11185\ntest 11185\n[TRAIN MEMORY] Max memory allocated in epoch 1: 454.30 MB\nEpoch: 1 cost time: 3.0540733337402344\nEpoch: 1, Steps: 16 | Train Loss: 0.3756389 Vali Loss: 0.1234386 Test Loss: 0.4088890\nValidation loss decreased (inf --> 0.123439).  Saving model ...\nUpdating learning rate to 1.186006642679406e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 454.30 MB\nEpoch: 2 cost time: 2.3203136920928955\nEpoch: 2, Steps: 16 | Train Loss: 0.3544115 Vali Loss: 0.1224777 Test Loss: 0.4079183\nValidation loss decreased (0.123439 --> 0.122478).  Saving model ...\nUpdating learning rate to 3.1784005853428764e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 454.30 MB\nEpoch: 3 cost time: 2.30804443359375\nEpoch: 3, Steps: 16 | Train Loss: 0.3526434 Vali Loss: 0.1217203 Test Loss: 0.4070897\nValidation loss decreased (0.122478 --> 0.121720).  Saving model ...\nUpdating learning rate to 6.463398202002396e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 454.30 MB\nEpoch: 4 cost time: 2.3401427268981934\nEpoch: 4, Steps: 16 | Train Loss: 0.3516700 Vali Loss: 0.1216152 Test Loss: 0.4041912\nValidation loss decreased (0.121720 --> 0.121615).  Saving model ...\nUpdating learning rate to 0.00011778990607322068\n[TRAIN MEMORY] Max memory allocated in epoch 5: 454.30 MB\nEpoch: 5 cost time: 2.2605998516082764\nEpoch: 5, Steps: 16 | Train Loss: 0.3509541 Vali Loss: 0.1210805 Test Loss: 0.4028430\nValidation loss decreased (0.121615 --> 0.121081).  Saving model ...\nUpdating learning rate to 0.0002018420838225947\n[TRAIN MEMORY] Max memory allocated in epoch 6: 454.30 MB\nEpoch: 6 cost time: 2.2340478897094727\nEpoch: 6, Steps: 16 | Train Loss: 0.3500175 Vali Loss: 0.1213806 Test Loss: 0.4026630\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00033056887010780915\n[TRAIN MEMORY] Max memory allocated in epoch 7: 454.30 MB\nEpoch: 7 cost time: 2.2842519283294678\nEpoch: 7, Steps: 16 | Train Loss: 0.3493483 Vali Loss: 0.1208638 Test Loss: 0.4020825\nValidation loss decreased (0.121081 --> 0.120864).  Saving model ...\nUpdating learning rate to 0.0005188634406732307\n[TRAIN MEMORY] Max memory allocated in epoch 8: 454.30 MB\nEpoch: 8 cost time: 2.416153907775879\nEpoch: 8, Steps: 16 | Train Loss: 0.3482408 Vali Loss: 0.1212486 Test Loss: 0.4009092\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0007769688585092725\n[TRAIN MEMORY] Max memory allocated in epoch 9: 454.30 MB\nEpoch: 9 cost time: 2.3201847076416016\nEpoch: 9, Steps: 16 | Train Loss: 0.3473553 Vali Loss: 0.1212103 Test Loss: 0.3998391\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.001101251887698682\n[TRAIN MEMORY] Max memory allocated in epoch 10: 454.30 MB\nEpoch: 10 cost time: 2.258056879043579\nEpoch: 10, Steps: 16 | Train Loss: 0.3462792 Vali Loss: 0.1212180 Test Loss: 0.3982635\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0014670391721082205\n[TRAIN MEMORY] Max memory allocated in epoch 11: 454.30 MB\nEpoch: 11 cost time: 2.3475120067596436\nEpoch: 11, Steps: 16 | Train Loss: 0.3459421 Vali Loss: 0.1217042 Test Loss: 0.3917689\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0018327467361537967\n[TRAIN MEMORY] Max memory allocated in epoch 12: 454.30 MB\nEpoch: 12 cost time: 2.2875332832336426\nEpoch: 12, Steps: 16 | Train Loss: 0.3451200 Vali Loss: 0.1208747 Test Loss: 0.3920771\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0021567904309371918\n[TRAIN MEMORY] Max memory allocated in epoch 13: 454.30 MB\nEpoch: 13 cost time: 2.2655136585235596\nEpoch: 13, Steps: 16 | Train Loss: 0.3438376 Vali Loss: 0.1207454 Test Loss: 0.3933116\nValidation loss decreased (0.120864 --> 0.120745).  Saving model ...\nUpdating learning rate to 0.002414496380188596\n[TRAIN MEMORY] Max memory allocated in epoch 14: 454.30 MB\nEpoch: 14 cost time: 2.3826823234558105\nEpoch: 14, Steps: 16 | Train Loss: 0.3431603 Vali Loss: 0.1210002 Test Loss: 0.3905677\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.002602230480450653\n[TRAIN MEMORY] Max memory allocated in epoch 15: 454.30 MB\nEpoch: 15 cost time: 2.354584217071533\nEpoch: 15, Steps: 16 | Train Loss: 0.3416484 Vali Loss: 0.1206690 Test Loss: 0.3894020\nValidation loss decreased (0.120745 --> 0.120669).  Saving model ...\nUpdating learning rate to 0.002730234578806533\n[TRAIN MEMORY] Max memory allocated in epoch 16: 454.30 MB\nEpoch: 16 cost time: 2.366551160812378\nEpoch: 16, Steps: 16 | Train Loss: 0.3410014 Vali Loss: 0.1212888 Test Loss: 0.3931953\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0028134002853874808\n[TRAIN MEMORY] Max memory allocated in epoch 17: 454.30 MB\nEpoch: 17 cost time: 2.3233118057250977\nEpoch: 17, Steps: 16 | Train Loss: 0.3405950 Vali Loss: 0.1212414 Test Loss: 0.3893503\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0028655040380107044\n[TRAIN MEMORY] Max memory allocated in epoch 18: 454.30 MB\nEpoch: 18 cost time: 2.277804374694824\nEpoch: 18, Steps: 16 | Train Loss: 0.3392633 Vali Loss: 0.1203763 Test Loss: 0.3923898\nValidation loss decreased (0.120669 --> 0.120376).  Saving model ...\nUpdating learning rate to 0.0028971338719994025\n[TRAIN MEMORY] Max memory allocated in epoch 19: 454.30 MB\nEpoch: 19 cost time: 2.376537799835205\nEpoch: 19, Steps: 16 | Train Loss: 0.3392041 Vali Loss: 0.1205832 Test Loss: 0.3879048\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0029156670721610374\n[TRAIN MEMORY] Max memory allocated in epoch 20: 454.30 MB\nEpoch: 20 cost time: 2.3904879093170166\nEpoch: 20, Steps: 16 | Train Loss: 0.3377476 Vali Loss: 0.1205274 Test Loss: 0.3885715\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.002925962817340871\n[TRAIN MEMORY] Max memory allocated in epoch 21: 454.30 MB\nEpoch: 21 cost time: 2.3294572830200195\nEpoch: 21, Steps: 16 | Train Loss: 0.3364927 Vali Loss: 0.1203176 Test Loss: 0.3888464\nValidation loss decreased (0.120376 --> 0.120318).  Saving model ...\nUpdating learning rate to 0.002931116701041195\n[TRAIN MEMORY] Max memory allocated in epoch 22: 454.30 MB\nEpoch: 22 cost time: 2.292764902114868\nEpoch: 22, Steps: 16 | Train Loss: 0.3361931 Vali Loss: 0.1208338 Test Loss: 0.3880574\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0029330612133278634\n[TRAIN MEMORY] Max memory allocated in epoch 23: 454.30 MB\nEpoch: 23 cost time: 2.3550353050231934\nEpoch: 23, Steps: 16 | Train Loss: 0.3354496 Vali Loss: 0.1200846 Test Loss: 0.3873096\nValidation loss decreased (0.120318 --> 0.120085).  Saving model ...\nUpdating learning rate to 0.0029329874196737478\n[TRAIN MEMORY] Max memory allocated in epoch 24: 454.30 MB\nEpoch: 24 cost time: 2.2964725494384766\nEpoch: 24, Steps: 16 | Train Loss: 0.3350442 Vali Loss: 0.1208762 Test Loss: 0.3910051\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.002931623033608407\n[TRAIN MEMORY] Max memory allocated in epoch 25: 454.30 MB\nEpoch: 25 cost time: 2.327692985534668\nEpoch: 25, Steps: 16 | Train Loss: 0.3353435 Vali Loss: 0.1206653 Test Loss: 0.3845093\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0029294095543591523\n[TRAIN MEMORY] Max memory allocated in epoch 26: 454.30 MB\nEpoch: 26 cost time: 2.328340768814087\nEpoch: 26, Steps: 16 | Train Loss: 0.3343503 Vali Loss: 0.1204393 Test Loss: 0.3874225\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0029266128853555934\n[TRAIN MEMORY] Max memory allocated in epoch 27: 454.30 MB\nEpoch: 27 cost time: 2.320495843887329\nEpoch: 27, Steps: 16 | Train Loss: 0.3340598 Vali Loss: 0.1201167 Test Loss: 0.3898463\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0029233916093852194\n[TRAIN MEMORY] Max memory allocated in epoch 28: 454.30 MB\nEpoch: 28 cost time: 2.3020570278167725\nEpoch: 28, Steps: 16 | Train Loss: 0.3333335 Vali Loss: 0.1207926 Test Loss: 0.3857969\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0029198388355414434\n[TRAIN MEMORY] Max memory allocated in epoch 29: 454.30 MB\nEpoch: 29 cost time: 2.2191591262817383\nEpoch: 29, Steps: 16 | Train Loss: 0.3332423 Vali Loss: 0.1211267 Test Loss: 0.3881922\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.002916007739965249\n[TRAIN MEMORY] Max memory allocated in epoch 30: 454.30 MB\nEpoch: 30 cost time: 2.2951884269714355\nEpoch: 30, Steps: 16 | Train Loss: 0.3332327 Vali Loss: 0.1201725 Test Loss: 0.3871124\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.002911927114139824\n[TRAIN MEMORY] Max memory allocated in epoch 31: 454.30 MB\nEpoch: 31 cost time: 2.320751190185547\nEpoch: 31, Steps: 16 | Train Loss: 0.3321607 Vali Loss: 0.1209766 Test Loss: 0.3869540\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.002907610814855212\n[TRAIN MEMORY] Max memory allocated in epoch 32: 454.30 MB\nEpoch: 32 cost time: 2.2386255264282227\nEpoch: 32, Steps: 16 | Train Loss: 0.3321543 Vali Loss: 0.1210682 Test Loss: 0.3871973\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.002903063501639105\n[TRAIN MEMORY] Max memory allocated in epoch 33: 454.30 MB\nEpoch: 33 cost time: 2.289746046066284\nEpoch: 33, Steps: 16 | Train Loss: 0.3321688 Vali Loss: 0.1207575 Test Loss: 0.3891051\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 2.34 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTm1_ftM_sl96_ll48_pl336_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 11185\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 1.85 MB\n[PARAMS] Total parameters: 483,621\nmse:0.38730964064598083, mae:0.39184167981147766\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTm1 --learning_rate 0.003 --data_path ETTm1.csv --seq_len 96 --pred_len 720 --enc_in 7 --batch_size 2048 --dropout 0.4 --lradj 'sigmoid' --period_len 4 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T05:17:29.886107Z","iopub.execute_input":"2025-10-19T05:17:29.886712Z","iopub.status.idle":"2025-10-19T05:23:22.132500Z","shell.execute_reply.started":"2025-10-19T05:17:29.886685Z","shell.execute_reply":"2025-10-19T05:23:22.131630Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=4, d_model=128, data='ETTm1', root_path='./dataset', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=720, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.003, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.4, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTm1_ftM_sl96_ll48_pl720_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 33745\nval 10801\ntest 10801\n[TRAIN MEMORY] Max memory allocated in epoch 1: 840.89 MB\nEpoch: 1 cost time: 4.460429906845093\nEpoch: 1, Steps: 16 | Train Loss: 0.4353579 Vali Loss: 0.1469685 Test Loss: 0.4967199\nValidation loss decreased (inf --> 0.146968).  Saving model ...\nUpdating learning rate to 1.186006642679406e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 840.89 MB\nEpoch: 2 cost time: 3.6230993270874023\nEpoch: 2, Steps: 16 | Train Loss: 0.3873467 Vali Loss: 0.1460813 Test Loss: 0.4907857\nValidation loss decreased (0.146968 --> 0.146081).  Saving model ...\nUpdating learning rate to 3.1784005853428764e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 840.89 MB\nEpoch: 3 cost time: 3.6412649154663086\nEpoch: 3, Steps: 16 | Train Loss: 0.3837870 Vali Loss: 0.1446223 Test Loss: 0.4831527\nValidation loss decreased (0.146081 --> 0.144622).  Saving model ...\nUpdating learning rate to 6.463398202002396e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 840.89 MB\nEpoch: 4 cost time: 3.6081793308258057\nEpoch: 4, Steps: 16 | Train Loss: 0.3815887 Vali Loss: 0.1442925 Test Loss: 0.4801507\nValidation loss decreased (0.144622 --> 0.144292).  Saving model ...\nUpdating learning rate to 0.00011778990607322068\n[TRAIN MEMORY] Max memory allocated in epoch 5: 840.89 MB\nEpoch: 5 cost time: 3.6315622329711914\nEpoch: 5, Steps: 16 | Train Loss: 0.3805215 Vali Loss: 0.1439440 Test Loss: 0.4783292\nValidation loss decreased (0.144292 --> 0.143944).  Saving model ...\nUpdating learning rate to 0.0002018420838225947\n[TRAIN MEMORY] Max memory allocated in epoch 6: 840.89 MB\nEpoch: 6 cost time: 3.3268795013427734\nEpoch: 6, Steps: 16 | Train Loss: 0.3796906 Vali Loss: 0.1438640 Test Loss: 0.4763967\nValidation loss decreased (0.143944 --> 0.143864).  Saving model ...\nUpdating learning rate to 0.00033056887010780915\n[TRAIN MEMORY] Max memory allocated in epoch 7: 840.89 MB\nEpoch: 7 cost time: 3.6626205444335938\nEpoch: 7, Steps: 16 | Train Loss: 0.3785940 Vali Loss: 0.1435508 Test Loss: 0.4766154\nValidation loss decreased (0.143864 --> 0.143551).  Saving model ...\nUpdating learning rate to 0.0005188634406732307\n[TRAIN MEMORY] Max memory allocated in epoch 8: 840.89 MB\nEpoch: 8 cost time: 3.629225492477417\nEpoch: 8, Steps: 16 | Train Loss: 0.3774743 Vali Loss: 0.1437290 Test Loss: 0.4746658\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0007769688585092725\n[TRAIN MEMORY] Max memory allocated in epoch 9: 840.89 MB\nEpoch: 9 cost time: 3.6053507328033447\nEpoch: 9, Steps: 16 | Train Loss: 0.3763155 Vali Loss: 0.1438824 Test Loss: 0.4739004\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.001101251887698682\n[TRAIN MEMORY] Max memory allocated in epoch 10: 840.89 MB\nEpoch: 10 cost time: 3.297513008117676\nEpoch: 10, Steps: 16 | Train Loss: 0.3753712 Vali Loss: 0.1437508 Test Loss: 0.4720957\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0014670391721082205\n[TRAIN MEMORY] Max memory allocated in epoch 11: 840.89 MB\nEpoch: 11 cost time: 3.6610360145568848\nEpoch: 11, Steps: 16 | Train Loss: 0.3750216 Vali Loss: 0.1443068 Test Loss: 0.4703492\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0018327467361537967\n[TRAIN MEMORY] Max memory allocated in epoch 12: 840.89 MB\nEpoch: 12 cost time: 3.4471583366394043\nEpoch: 12, Steps: 16 | Train Loss: 0.3745980 Vali Loss: 0.1435955 Test Loss: 0.4694413\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0021567904309371918\n[TRAIN MEMORY] Max memory allocated in epoch 13: 840.89 MB\nEpoch: 13 cost time: 3.6002964973449707\nEpoch: 13, Steps: 16 | Train Loss: 0.3735088 Vali Loss: 0.1434066 Test Loss: 0.4713116\nValidation loss decreased (0.143551 --> 0.143407).  Saving model ...\nUpdating learning rate to 0.002414496380188596\n[TRAIN MEMORY] Max memory allocated in epoch 14: 840.89 MB\nEpoch: 14 cost time: 3.65329647064209\nEpoch: 14, Steps: 16 | Train Loss: 0.3723356 Vali Loss: 0.1430417 Test Loss: 0.4712130\nValidation loss decreased (0.143407 --> 0.143042).  Saving model ...\nUpdating learning rate to 0.002602230480450653\n[TRAIN MEMORY] Max memory allocated in epoch 15: 840.89 MB\nEpoch: 15 cost time: 3.604370355606079\nEpoch: 15, Steps: 16 | Train Loss: 0.3713239 Vali Loss: 0.1441499 Test Loss: 0.4687680\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.002730234578806533\n[TRAIN MEMORY] Max memory allocated in epoch 16: 840.89 MB\nEpoch: 16 cost time: 3.5695366859436035\nEpoch: 16, Steps: 16 | Train Loss: 0.3711591 Vali Loss: 0.1435519 Test Loss: 0.4699026\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0028134002853874808\n[TRAIN MEMORY] Max memory allocated in epoch 17: 840.89 MB\nEpoch: 17 cost time: 3.6101176738739014\nEpoch: 17, Steps: 16 | Train Loss: 0.3702050 Vali Loss: 0.1431117 Test Loss: 0.4630682\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0028655040380107044\n[TRAIN MEMORY] Max memory allocated in epoch 18: 840.89 MB\nEpoch: 18 cost time: 3.5362634658813477\nEpoch: 18, Steps: 16 | Train Loss: 0.3692098 Vali Loss: 0.1429784 Test Loss: 0.4667134\nValidation loss decreased (0.143042 --> 0.142978).  Saving model ...\nUpdating learning rate to 0.0028971338719994025\n[TRAIN MEMORY] Max memory allocated in epoch 19: 840.89 MB\nEpoch: 19 cost time: 3.438912868499756\nEpoch: 19, Steps: 16 | Train Loss: 0.3688473 Vali Loss: 0.1432975 Test Loss: 0.4664088\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0029156670721610374\n[TRAIN MEMORY] Max memory allocated in epoch 20: 840.89 MB\nEpoch: 20 cost time: 3.657841444015503\nEpoch: 20, Steps: 16 | Train Loss: 0.3684562 Vali Loss: 0.1425503 Test Loss: 0.4675872\nValidation loss decreased (0.142978 --> 0.142550).  Saving model ...\nUpdating learning rate to 0.002925962817340871\n[TRAIN MEMORY] Max memory allocated in epoch 21: 840.89 MB\nEpoch: 21 cost time: 3.6647133827209473\nEpoch: 21, Steps: 16 | Train Loss: 0.3675048 Vali Loss: 0.1429559 Test Loss: 0.4653543\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.002931116701041195\n[TRAIN MEMORY] Max memory allocated in epoch 22: 840.89 MB\nEpoch: 22 cost time: 3.4018330574035645\nEpoch: 22, Steps: 16 | Train Loss: 0.3669886 Vali Loss: 0.1431766 Test Loss: 0.4606017\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0029330612133278634\n[TRAIN MEMORY] Max memory allocated in epoch 23: 840.89 MB\nEpoch: 23 cost time: 3.5951054096221924\nEpoch: 23, Steps: 16 | Train Loss: 0.3668055 Vali Loss: 0.1429080 Test Loss: 0.4628282\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0029329874196737478\n[TRAIN MEMORY] Max memory allocated in epoch 24: 840.89 MB\nEpoch: 24 cost time: 3.6217081546783447\nEpoch: 24, Steps: 16 | Train Loss: 0.3657061 Vali Loss: 0.1429934 Test Loss: 0.4608250\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.002931623033608407\n[TRAIN MEMORY] Max memory allocated in epoch 25: 840.89 MB\nEpoch: 25 cost time: 3.648550510406494\nEpoch: 25, Steps: 16 | Train Loss: 0.3652159 Vali Loss: 0.1430521 Test Loss: 0.4614794\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0029294095543591523\n[TRAIN MEMORY] Max memory allocated in epoch 26: 840.89 MB\nEpoch: 26 cost time: 3.5638232231140137\nEpoch: 26, Steps: 16 | Train Loss: 0.3644585 Vali Loss: 0.1428849 Test Loss: 0.4621986\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0029266128853555934\n[TRAIN MEMORY] Max memory allocated in epoch 27: 840.89 MB\nEpoch: 27 cost time: 3.6267614364624023\nEpoch: 27, Steps: 16 | Train Loss: 0.3645252 Vali Loss: 0.1431685 Test Loss: 0.4559545\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0029233916093852194\n[TRAIN MEMORY] Max memory allocated in epoch 28: 840.89 MB\nEpoch: 28 cost time: 3.601550340652466\nEpoch: 28, Steps: 16 | Train Loss: 0.3645568 Vali Loss: 0.1424470 Test Loss: 0.4615145\nValidation loss decreased (0.142550 --> 0.142447).  Saving model ...\nUpdating learning rate to 0.0029198388355414434\n[TRAIN MEMORY] Max memory allocated in epoch 29: 840.89 MB\nEpoch: 29 cost time: 3.648861885070801\nEpoch: 29, Steps: 16 | Train Loss: 0.3638104 Vali Loss: 0.1425080 Test Loss: 0.4637810\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.002916007739965249\n[TRAIN MEMORY] Max memory allocated in epoch 30: 840.89 MB\nEpoch: 30 cost time: 3.5850746631622314\nEpoch: 30, Steps: 16 | Train Loss: 0.3632503 Vali Loss: 0.1428698 Test Loss: 0.4608485\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.002911927114139824\n[TRAIN MEMORY] Max memory allocated in epoch 31: 840.89 MB\nEpoch: 31 cost time: 3.5826261043548584\nEpoch: 31, Steps: 16 | Train Loss: 0.3633566 Vali Loss: 0.1427141 Test Loss: 0.4609692\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.002907610814855212\n[TRAIN MEMORY] Max memory allocated in epoch 32: 840.89 MB\nEpoch: 32 cost time: 3.6017467975616455\nEpoch: 32, Steps: 16 | Train Loss: 0.3630381 Vali Loss: 0.1427707 Test Loss: 0.4617820\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.002903063501639105\n[TRAIN MEMORY] Max memory allocated in epoch 33: 840.89 MB\nEpoch: 33 cost time: 3.7149040699005127\nEpoch: 33, Steps: 16 | Train Loss: 0.3627266 Vali Loss: 0.1427034 Test Loss: 0.4630339\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.002898284117549523\n[TRAIN MEMORY] Max memory allocated in epoch 34: 840.89 MB\nEpoch: 34 cost time: 3.5869956016540527\nEpoch: 34, Steps: 16 | Train Loss: 0.3621529 Vali Loss: 0.1423770 Test Loss: 0.4608698\nValidation loss decreased (0.142447 --> 0.142377).  Saving model ...\nUpdating learning rate to 0.0028932679996582847\n[TRAIN MEMORY] Max memory allocated in epoch 35: 840.89 MB\nEpoch: 35 cost time: 3.6281681060791016\nEpoch: 35, Steps: 16 | Train Loss: 0.3621000 Vali Loss: 0.1428356 Test Loss: 0.4592815\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.002888008158049759\n[TRAIN MEMORY] Max memory allocated in epoch 36: 840.89 MB\nEpoch: 36 cost time: 3.6974663734436035\nEpoch: 36, Steps: 16 | Train Loss: 0.3615210 Vali Loss: 0.1428292 Test Loss: 0.4604589\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0028824960506368133\n[TRAIN MEMORY] Max memory allocated in epoch 37: 840.89 MB\nEpoch: 37 cost time: 3.716815233230591\nEpoch: 37, Steps: 16 | Train Loss: 0.3613999 Vali Loss: 0.1428059 Test Loss: 0.4594242\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.002876722052526984\n[TRAIN MEMORY] Max memory allocated in epoch 38: 840.89 MB\nEpoch: 38 cost time: 3.2973616123199463\nEpoch: 38, Steps: 16 | Train Loss: 0.3610412 Vali Loss: 0.1425599 Test Loss: 0.4591660\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0028706757405926584\n[TRAIN MEMORY] Max memory allocated in epoch 39: 840.89 MB\nEpoch: 39 cost time: 3.624333381652832\nEpoch: 39, Steps: 16 | Train Loss: 0.3606315 Vali Loss: 0.1424751 Test Loss: 0.4628519\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.002864346066507913\n[TRAIN MEMORY] Max memory allocated in epoch 40: 840.89 MB\nEpoch: 40 cost time: 3.532874345779419\nEpoch: 40, Steps: 16 | Train Loss: 0.3603362 Vali Loss: 0.1421174 Test Loss: 0.4603576\nValidation loss decreased (0.142377 --> 0.142117).  Saving model ...\nUpdating learning rate to 0.0028577214627606186\n[TRAIN MEMORY] Max memory allocated in epoch 41: 840.89 MB\nEpoch: 41 cost time: 3.7177350521087646\nEpoch: 41, Steps: 16 | Train Loss: 0.3601038 Vali Loss: 0.1427305 Test Loss: 0.4617908\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0028507899087070244\n[TRAIN MEMORY] Max memory allocated in epoch 42: 840.89 MB\nEpoch: 42 cost time: 3.6570451259613037\nEpoch: 42, Steps: 16 | Train Loss: 0.3600154 Vali Loss: 0.1424683 Test Loss: 0.4605964\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0028435389731592607\n[TRAIN MEMORY] Max memory allocated in epoch 43: 840.89 MB\nEpoch: 43 cost time: 3.5372602939605713\nEpoch: 43, Steps: 16 | Train Loss: 0.3596365 Vali Loss: 0.1423213 Test Loss: 0.4558494\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.002835955843584091\n[TRAIN MEMORY] Max memory allocated in epoch 44: 840.89 MB\nEpoch: 44 cost time: 3.5916695594787598\nEpoch: 44, Steps: 16 | Train Loss: 0.3592938 Vali Loss: 0.1428772 Test Loss: 0.4615831\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.002828027348105267\n[TRAIN MEMORY] Max memory allocated in epoch 45: 840.89 MB\nEpoch: 45 cost time: 3.5692265033721924\nEpoch: 45, Steps: 16 | Train Loss: 0.3589378 Vali Loss: 0.1432115 Test Loss: 0.4576577\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.002819739974148004\n[TRAIN MEMORY] Max memory allocated in epoch 46: 840.89 MB\nEpoch: 46 cost time: 3.636612892150879\nEpoch: 46, Steps: 16 | Train Loss: 0.3585804 Vali Loss: 0.1427346 Test Loss: 0.4578773\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.002811079886139072\n[TRAIN MEMORY] Max memory allocated in epoch 47: 840.89 MB\nEpoch: 47 cost time: 3.6255059242248535\nEpoch: 47, Steps: 16 | Train Loss: 0.3587719 Vali Loss: 0.1429823 Test Loss: 0.4631997\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0028020329438139947\n[TRAIN MEMORY] Max memory allocated in epoch 48: 840.89 MB\nEpoch: 48 cost time: 3.6125476360321045\nEpoch: 48, Steps: 16 | Train Loss: 0.3584796 Vali Loss: 0.1423275 Test Loss: 0.4611968\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0027925847221615704\n[TRAIN MEMORY] Max memory allocated in epoch 49: 840.89 MB\nEpoch: 49 cost time: 3.695892095565796\nEpoch: 49, Steps: 16 | Train Loss: 0.3577231 Vali Loss: 0.1429708 Test Loss: 0.4651970\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.002782720533720643\n[TRAIN MEMORY] Max memory allocated in epoch 50: 840.89 MB\nEpoch: 50 cost time: 3.6000280380249023\nEpoch: 50, Steps: 16 | Train Loss: 0.3578266 Vali Loss: 0.1426358 Test Loss: 0.4544386\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 3.61 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTm1_ftM_sl96_ll48_pl720_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 10801\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 6.88 MB\n[PARAMS] Total parameters: 1,801,605\nmse:0.4603577256202698, mae:0.43026626110076904\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTm2 --learning_rate 0.002 --data_path ETTm2.csv --seq_len 96 --pred_len 96 --enc_in 7 --batch_size 2048 --lradj 'sigmoid' --dropout 0.1 --period_len 4 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T05:33:47.932006Z","iopub.execute_input":"2025-10-19T05:33:47.932539Z","iopub.status.idle":"2025-10-19T05:35:33.278620Z","shell.execute_reply.started":"2025-10-19T05:33:47.932515Z","shell.execute_reply":"2025-10-19T05:35:33.277914Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=4, d_model=128, data='ETTm2', root_path='./dataset', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=96, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.002, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.1, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTm2_ftM_sl96_ll48_pl96_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 34369\nval 11425\ntest 11425\n[TRAIN MEMORY] Max memory allocated in epoch 1: 219.76 MB\nEpoch: 1 cost time: 2.144735336303711\nEpoch: 1, Steps: 16 | Train Loss: 0.2532673 Vali Loss: 0.0616841 Test Loss: 0.1715270\nValidation loss decreased (inf --> 0.061684).  Saving model ...\nUpdating learning rate to 7.90671095119604e-06\n[TRAIN MEMORY] Max memory allocated in epoch 2: 219.76 MB\nEpoch: 2 cost time: 1.4753355979919434\nEpoch: 2, Steps: 16 | Train Loss: 0.2370846 Vali Loss: 0.0617619 Test Loss: 0.1713720\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 2.118933723561918e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 219.76 MB\nEpoch: 3 cost time: 1.4794533252716064\nEpoch: 3, Steps: 16 | Train Loss: 0.2368067 Vali Loss: 0.0616991 Test Loss: 0.1709047\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 4.308932134668265e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 219.76 MB\nEpoch: 4 cost time: 1.474245309829712\nEpoch: 4, Steps: 16 | Train Loss: 0.2363789 Vali Loss: 0.0615041 Test Loss: 0.1703290\nValidation loss decreased (0.061684 --> 0.061504).  Saving model ...\nUpdating learning rate to 7.852660404881378e-05\n[TRAIN MEMORY] Max memory allocated in epoch 5: 219.76 MB\nEpoch: 5 cost time: 1.4953887462615967\nEpoch: 5, Steps: 16 | Train Loss: 0.2351744 Vali Loss: 0.0611513 Test Loss: 0.1698065\nValidation loss decreased (0.061504 --> 0.061151).  Saving model ...\nUpdating learning rate to 0.00013456138921506312\n[TRAIN MEMORY] Max memory allocated in epoch 6: 219.76 MB\nEpoch: 6 cost time: 1.4652488231658936\nEpoch: 6, Steps: 16 | Train Loss: 0.2342005 Vali Loss: 0.0610661 Test Loss: 0.1691792\nValidation loss decreased (0.061151 --> 0.061066).  Saving model ...\nUpdating learning rate to 0.00022037924673853947\n[TRAIN MEMORY] Max memory allocated in epoch 7: 219.76 MB\nEpoch: 7 cost time: 1.4663035869598389\nEpoch: 7, Steps: 16 | Train Loss: 0.2337553 Vali Loss: 0.0608041 Test Loss: 0.1683560\nValidation loss decreased (0.061066 --> 0.060804).  Saving model ...\nUpdating learning rate to 0.00034590896044882054\n[TRAIN MEMORY] Max memory allocated in epoch 8: 219.76 MB\nEpoch: 8 cost time: 1.4419395923614502\nEpoch: 8, Steps: 16 | Train Loss: 0.2323831 Vali Loss: 0.0605223 Test Loss: 0.1675076\nValidation loss decreased (0.060804 --> 0.060522).  Saving model ...\nUpdating learning rate to 0.0005179792390061816\n[TRAIN MEMORY] Max memory allocated in epoch 9: 219.76 MB\nEpoch: 9 cost time: 1.4695968627929688\nEpoch: 9, Steps: 16 | Train Loss: 0.2314650 Vali Loss: 0.0602781 Test Loss: 0.1667740\nValidation loss decreased (0.060522 --> 0.060278).  Saving model ...\nUpdating learning rate to 0.0007341679251324548\n[TRAIN MEMORY] Max memory allocated in epoch 10: 219.76 MB\nEpoch: 10 cost time: 1.4986252784729004\nEpoch: 10, Steps: 16 | Train Loss: 0.2300613 Vali Loss: 0.0602285 Test Loss: 0.1661091\nValidation loss decreased (0.060278 --> 0.060228).  Saving model ...\nUpdating learning rate to 0.0009780261147388136\n[TRAIN MEMORY] Max memory allocated in epoch 11: 219.76 MB\nEpoch: 11 cost time: 1.4727506637573242\nEpoch: 11, Steps: 16 | Train Loss: 0.2291846 Vali Loss: 0.0601331 Test Loss: 0.1659424\nValidation loss decreased (0.060228 --> 0.060133).  Saving model ...\nUpdating learning rate to 0.0012218311574358645\n[TRAIN MEMORY] Max memory allocated in epoch 12: 219.76 MB\nEpoch: 12 cost time: 1.5273230075836182\nEpoch: 12, Steps: 16 | Train Loss: 0.2280864 Vali Loss: 0.0599809 Test Loss: 0.1653896\nValidation loss decreased (0.060133 --> 0.059981).  Saving model ...\nUpdating learning rate to 0.0014378602872914612\n[TRAIN MEMORY] Max memory allocated in epoch 13: 219.76 MB\nEpoch: 13 cost time: 1.4783048629760742\nEpoch: 13, Steps: 16 | Train Loss: 0.2274253 Vali Loss: 0.0599368 Test Loss: 0.1652055\nValidation loss decreased (0.059981 --> 0.059937).  Saving model ...\nUpdating learning rate to 0.001609664253459064\n[TRAIN MEMORY] Max memory allocated in epoch 14: 219.76 MB\nEpoch: 14 cost time: 1.4542229175567627\nEpoch: 14, Steps: 16 | Train Loss: 0.2262658 Vali Loss: 0.0602011 Test Loss: 0.1656099\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0017348203203004352\n[TRAIN MEMORY] Max memory allocated in epoch 15: 219.76 MB\nEpoch: 15 cost time: 1.462470531463623\nEpoch: 15, Steps: 16 | Train Loss: 0.2247619 Vali Loss: 0.0598908 Test Loss: 0.1650353\nValidation loss decreased (0.059937 --> 0.059891).  Saving model ...\nUpdating learning rate to 0.001820156385871022\n[TRAIN MEMORY] Max memory allocated in epoch 16: 219.76 MB\nEpoch: 16 cost time: 1.452826976776123\nEpoch: 16, Steps: 16 | Train Loss: 0.2237490 Vali Loss: 0.0600148 Test Loss: 0.1646774\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0018756001902583204\n[TRAIN MEMORY] Max memory allocated in epoch 17: 219.76 MB\nEpoch: 17 cost time: 1.4548594951629639\nEpoch: 17, Steps: 16 | Train Loss: 0.2228683 Vali Loss: 0.0597059 Test Loss: 0.1638890\nValidation loss decreased (0.059891 --> 0.059706).  Saving model ...\nUpdating learning rate to 0.0019103360253404696\n[TRAIN MEMORY] Max memory allocated in epoch 18: 219.76 MB\nEpoch: 18 cost time: 1.409287452697754\nEpoch: 18, Steps: 16 | Train Loss: 0.2216922 Vali Loss: 0.0595385 Test Loss: 0.1636321\nValidation loss decreased (0.059706 --> 0.059539).  Saving model ...\nUpdating learning rate to 0.001931422581332935\n[TRAIN MEMORY] Max memory allocated in epoch 19: 219.76 MB\nEpoch: 19 cost time: 1.475632905960083\nEpoch: 19, Steps: 16 | Train Loss: 0.2212071 Vali Loss: 0.0595627 Test Loss: 0.1635870\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0019437780481073583\n[TRAIN MEMORY] Max memory allocated in epoch 20: 219.76 MB\nEpoch: 20 cost time: 1.4859836101531982\nEpoch: 20, Steps: 16 | Train Loss: 0.2200346 Vali Loss: 0.0594357 Test Loss: 0.1634742\nValidation loss decreased (0.059539 --> 0.059436).  Saving model ...\nUpdating learning rate to 0.0019506418782272474\n[TRAIN MEMORY] Max memory allocated in epoch 21: 219.76 MB\nEpoch: 21 cost time: 1.5018434524536133\nEpoch: 21, Steps: 16 | Train Loss: 0.2194846 Vali Loss: 0.0595431 Test Loss: 0.1632354\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0019540778006941297\n[TRAIN MEMORY] Max memory allocated in epoch 22: 219.76 MB\nEpoch: 22 cost time: 1.453761339187622\nEpoch: 22, Steps: 16 | Train Loss: 0.2184886 Vali Loss: 0.0596608 Test Loss: 0.1625328\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0019553741422185756\n[TRAIN MEMORY] Max memory allocated in epoch 23: 219.76 MB\nEpoch: 23 cost time: 1.4609975814819336\nEpoch: 23, Steps: 16 | Train Loss: 0.2180380 Vali Loss: 0.0596540 Test Loss: 0.1633262\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.001955324946449165\n[TRAIN MEMORY] Max memory allocated in epoch 24: 219.76 MB\nEpoch: 24 cost time: 1.486583948135376\nEpoch: 24, Steps: 16 | Train Loss: 0.2173634 Vali Loss: 0.0594233 Test Loss: 0.1622675\nValidation loss decreased (0.059436 --> 0.059423).  Saving model ...\nUpdating learning rate to 0.001954415355738938\n[TRAIN MEMORY] Max memory allocated in epoch 25: 219.76 MB\nEpoch: 25 cost time: 1.4893615245819092\nEpoch: 25, Steps: 16 | Train Loss: 0.2168311 Vali Loss: 0.0595955 Test Loss: 0.1624452\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0019529397029061015\n[TRAIN MEMORY] Max memory allocated in epoch 26: 219.76 MB\nEpoch: 26 cost time: 1.4402108192443848\nEpoch: 26, Steps: 16 | Train Loss: 0.2162099 Vali Loss: 0.0595603 Test Loss: 0.1624077\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0019510752569037288\n[TRAIN MEMORY] Max memory allocated in epoch 27: 219.76 MB\nEpoch: 27 cost time: 1.5221428871154785\nEpoch: 27, Steps: 16 | Train Loss: 0.2156506 Vali Loss: 0.0596016 Test Loss: 0.1633693\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0019489277395901463\n[TRAIN MEMORY] Max memory allocated in epoch 28: 219.76 MB\nEpoch: 28 cost time: 1.4887275695800781\nEpoch: 28, Steps: 16 | Train Loss: 0.2150206 Vali Loss: 0.0596056 Test Loss: 0.1628055\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0019465592236942956\n[TRAIN MEMORY] Max memory allocated in epoch 29: 219.76 MB\nEpoch: 29 cost time: 1.430968999862671\nEpoch: 29, Steps: 16 | Train Loss: 0.2147226 Vali Loss: 0.0597607 Test Loss: 0.1629828\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0019440051599768332\n[TRAIN MEMORY] Max memory allocated in epoch 30: 219.76 MB\nEpoch: 30 cost time: 1.4779107570648193\nEpoch: 30, Steps: 16 | Train Loss: 0.2141310 Vali Loss: 0.0598031 Test Loss: 0.1630612\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0019412847427598826\n[TRAIN MEMORY] Max memory allocated in epoch 31: 219.76 MB\nEpoch: 31 cost time: 1.5140337944030762\nEpoch: 31, Steps: 16 | Train Loss: 0.2136564 Vali Loss: 0.0595492 Test Loss: 0.1626681\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0019384072099034749\n[TRAIN MEMORY] Max memory allocated in epoch 32: 219.76 MB\nEpoch: 32 cost time: 1.4476284980773926\nEpoch: 32, Steps: 16 | Train Loss: 0.2136399 Vali Loss: 0.0595727 Test Loss: 0.1631138\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0019353756677594031\n[TRAIN MEMORY] Max memory allocated in epoch 33: 219.76 MB\nEpoch: 33 cost time: 1.5372776985168457\nEpoch: 33, Steps: 16 | Train Loss: 0.2130515 Vali Loss: 0.0597671 Test Loss: 0.1634074\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.0019321894116996821\n[TRAIN MEMORY] Max memory allocated in epoch 34: 219.76 MB\nEpoch: 34 cost time: 1.4538280963897705\nEpoch: 34, Steps: 16 | Train Loss: 0.2125906 Vali Loss: 0.0596521 Test Loss: 0.1632777\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 1.49 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTm2_ftM_sl96_ll48_pl96_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 11425\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.42 MB\n[PARAMS] Total parameters: 109,161\nmse:0.1622675210237503, mae:0.24627184867858887\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTm2 --learning_rate 0.005 --data_path ETTm2.csv --seq_len 96 --pred_len 192 --enc_in 7 --batch_size 2048 --lradj 'sigmoid' --dropout 0.3 --patience 5 --period_len 4 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T05:48:30.684744Z","iopub.execute_input":"2025-10-19T05:48:30.685362Z","iopub.status.idle":"2025-10-19T05:49:52.503389Z","shell.execute_reply.started":"2025-10-19T05:48:30.685333Z","shell.execute_reply":"2025-10-19T05:49:52.502727Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=4, d_model=128, data='ETTm2', root_path='./dataset', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=192, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=5, learning_rate=0.005, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.3, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTm2_ftM_sl96_ll48_pl192_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 34273\nval 11329\ntest 11329\n[TRAIN MEMORY] Max memory allocated in epoch 1: 320.62 MB\nEpoch: 1 cost time: 2.526890277862549\nEpoch: 1, Steps: 16 | Train Loss: 0.2706378 Vali Loss: 0.0662053 Test Loss: 0.2347275\nValidation loss decreased (inf --> 0.066205).  Saving model ...\nUpdating learning rate to 1.9766777377990098e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 320.50 MB\nEpoch: 2 cost time: 1.7977757453918457\nEpoch: 2, Steps: 16 | Train Loss: 0.2593536 Vali Loss: 0.0661123 Test Loss: 0.2343651\nValidation loss decreased (0.066205 --> 0.066112).  Saving model ...\nUpdating learning rate to 5.297334308904794e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 320.50 MB\nEpoch: 3 cost time: 1.7560198307037354\nEpoch: 3, Steps: 16 | Train Loss: 0.2584788 Vali Loss: 0.0658677 Test Loss: 0.2334849\nValidation loss decreased (0.066112 --> 0.065868).  Saving model ...\nUpdating learning rate to 0.00010772330336670662\n[TRAIN MEMORY] Max memory allocated in epoch 4: 320.50 MB\nEpoch: 4 cost time: 1.7978107929229736\nEpoch: 4, Steps: 16 | Train Loss: 0.2574556 Vali Loss: 0.0656975 Test Loss: 0.2332700\nValidation loss decreased (0.065868 --> 0.065698).  Saving model ...\nUpdating learning rate to 0.00019631651012203447\n[TRAIN MEMORY] Max memory allocated in epoch 5: 320.50 MB\nEpoch: 5 cost time: 1.8049845695495605\nEpoch: 5, Steps: 16 | Train Loss: 0.2571948 Vali Loss: 0.0656592 Test Loss: 0.2326601\nValidation loss decreased (0.065698 --> 0.065659).  Saving model ...\nUpdating learning rate to 0.00033640347303765786\n[TRAIN MEMORY] Max memory allocated in epoch 6: 320.50 MB\nEpoch: 6 cost time: 1.8707928657531738\nEpoch: 6, Steps: 16 | Train Loss: 0.2568943 Vali Loss: 0.0655876 Test Loss: 0.2324485\nValidation loss decreased (0.065659 --> 0.065588).  Saving model ...\nUpdating learning rate to 0.0005509481168463487\n[TRAIN MEMORY] Max memory allocated in epoch 7: 320.50 MB\nEpoch: 7 cost time: 1.761709213256836\nEpoch: 7, Steps: 16 | Train Loss: 0.2558146 Vali Loss: 0.0652401 Test Loss: 0.2316325\nValidation loss decreased (0.065588 --> 0.065240).  Saving model ...\nUpdating learning rate to 0.0008647724011220512\n[TRAIN MEMORY] Max memory allocated in epoch 8: 320.50 MB\nEpoch: 8 cost time: 1.7537086009979248\nEpoch: 8, Steps: 16 | Train Loss: 0.2554649 Vali Loss: 0.0653886 Test Loss: 0.2316986\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 0.001294948097515454\n[TRAIN MEMORY] Max memory allocated in epoch 9: 320.50 MB\nEpoch: 9 cost time: 1.7735576629638672\nEpoch: 9, Steps: 16 | Train Loss: 0.2546176 Vali Loss: 0.0652728 Test Loss: 0.2314005\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 0.0018354198128311366\n[TRAIN MEMORY] Max memory allocated in epoch 10: 320.50 MB\nEpoch: 10 cost time: 1.7681009769439697\nEpoch: 10, Steps: 16 | Train Loss: 0.2535180 Vali Loss: 0.0650137 Test Loss: 0.2303912\nValidation loss decreased (0.065240 --> 0.065014).  Saving model ...\nUpdating learning rate to 0.002445065286847034\n[TRAIN MEMORY] Max memory allocated in epoch 11: 320.50 MB\nEpoch: 11 cost time: 1.7702162265777588\nEpoch: 11, Steps: 16 | Train Loss: 0.2525874 Vali Loss: 0.0650567 Test Loss: 0.2286286\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 0.0030545778935896616\n[TRAIN MEMORY] Max memory allocated in epoch 12: 320.50 MB\nEpoch: 12 cost time: 1.797752857208252\nEpoch: 12, Steps: 16 | Train Loss: 0.2514359 Vali Loss: 0.0649816 Test Loss: 0.2297879\nValidation loss decreased (0.065014 --> 0.064982).  Saving model ...\nUpdating learning rate to 0.003594650718228653\n[TRAIN MEMORY] Max memory allocated in epoch 13: 320.50 MB\nEpoch: 13 cost time: 1.7642252445220947\nEpoch: 13, Steps: 16 | Train Loss: 0.2506960 Vali Loss: 0.0652322 Test Loss: 0.2296658\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 0.00402416063364766\n[TRAIN MEMORY] Max memory allocated in epoch 14: 320.50 MB\nEpoch: 14 cost time: 1.7911779880523682\nEpoch: 14, Steps: 16 | Train Loss: 0.2487911 Vali Loss: 0.0651709 Test Loss: 0.2281674\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 0.004337050800751087\n[TRAIN MEMORY] Max memory allocated in epoch 15: 320.50 MB\nEpoch: 15 cost time: 1.7841801643371582\nEpoch: 15, Steps: 16 | Train Loss: 0.2485623 Vali Loss: 0.0650868 Test Loss: 0.2281064\nEarlyStopping counter: 3 out of 5\nUpdating learning rate to 0.0045503909646775545\n[TRAIN MEMORY] Max memory allocated in epoch 16: 320.50 MB\nEpoch: 16 cost time: 1.774073600769043\nEpoch: 16, Steps: 16 | Train Loss: 0.2475527 Vali Loss: 0.0649090 Test Loss: 0.2270677\nValidation loss decreased (0.064982 --> 0.064909).  Saving model ...\nUpdating learning rate to 0.004689000475645802\n[TRAIN MEMORY] Max memory allocated in epoch 17: 320.50 MB\nEpoch: 17 cost time: 1.7995853424072266\nEpoch: 17, Steps: 16 | Train Loss: 0.2464634 Vali Loss: 0.0649520 Test Loss: 0.2288814\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 0.004775840063351173\n[TRAIN MEMORY] Max memory allocated in epoch 18: 320.50 MB\nEpoch: 18 cost time: 1.786498785018921\nEpoch: 18, Steps: 16 | Train Loss: 0.2448116 Vali Loss: 0.0649792 Test Loss: 0.2264612\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 0.004828556453332337\n[TRAIN MEMORY] Max memory allocated in epoch 19: 320.50 MB\nEpoch: 19 cost time: 1.834991216659546\nEpoch: 19, Steps: 16 | Train Loss: 0.2441520 Vali Loss: 0.0650640 Test Loss: 0.2293919\nEarlyStopping counter: 3 out of 5\nUpdating learning rate to 0.0048594451202683955\n[TRAIN MEMORY] Max memory allocated in epoch 20: 320.50 MB\nEpoch: 20 cost time: 1.7722573280334473\nEpoch: 20, Steps: 16 | Train Loss: 0.2440347 Vali Loss: 0.0650515 Test Loss: 0.2274644\nEarlyStopping counter: 4 out of 5\nUpdating learning rate to 0.004876604695568118\n[TRAIN MEMORY] Max memory allocated in epoch 21: 320.50 MB\nEpoch: 21 cost time: 1.7964911460876465\nEpoch: 21, Steps: 16 | Train Loss: 0.2428211 Vali Loss: 0.0649805 Test Loss: 0.2283701\nEarlyStopping counter: 5 out of 5\nEarly stopping\nAverage epoch time: 1.82 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTm2_ftM_sl96_ll48_pl192_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 11329\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.84 MB\n[PARAMS] Total parameters: 217,473\nmse:0.22706767916679382, mae:0.2893195152282715\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTm2 --learning_rate 0.004 --data_path ETTm2.csv --seq_len 96 --pred_len 336 --enc_in 7 --batch_size 2048 --lradj 'sigmoid' --dropout 0.2 --period_len 4 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T05:56:03.303186Z","iopub.execute_input":"2025-10-19T05:56:03.303452Z","iopub.status.idle":"2025-10-19T05:58:03.214746Z","shell.execute_reply.started":"2025-10-19T05:56:03.303431Z","shell.execute_reply":"2025-10-19T05:58:03.213845Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=4, d_model=128, data='ETTm2', root_path='./dataset', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=336, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.004, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.2, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTm2_ftM_sl96_ll48_pl336_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 34129\nval 11185\ntest 11185\n[TRAIN MEMORY] Max memory allocated in epoch 1: 454.30 MB\nEpoch: 1 cost time: 2.9198553562164307\nEpoch: 1, Steps: 16 | Train Loss: 0.2986554 Vali Loss: 0.0719457 Test Loss: 0.2997235\nValidation loss decreased (inf --> 0.071946).  Saving model ...\nUpdating learning rate to 1.581342190239208e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 454.30 MB\nEpoch: 2 cost time: 2.290327787399292\nEpoch: 2, Steps: 16 | Train Loss: 0.2875897 Vali Loss: 0.0718299 Test Loss: 0.2984353\nValidation loss decreased (0.071946 --> 0.071830).  Saving model ...\nUpdating learning rate to 4.237867447123836e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 454.30 MB\nEpoch: 3 cost time: 2.2846906185150146\nEpoch: 3, Steps: 16 | Train Loss: 0.2864576 Vali Loss: 0.0715317 Test Loss: 0.2966318\nValidation loss decreased (0.071830 --> 0.071532).  Saving model ...\nUpdating learning rate to 8.61786426933653e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 454.30 MB\nEpoch: 4 cost time: 2.4101879596710205\nEpoch: 4, Steps: 16 | Train Loss: 0.2862213 Vali Loss: 0.0714359 Test Loss: 0.2958155\nValidation loss decreased (0.071532 --> 0.071436).  Saving model ...\nUpdating learning rate to 0.00015705320809762755\n[TRAIN MEMORY] Max memory allocated in epoch 5: 454.30 MB\nEpoch: 5 cost time: 2.322153329849243\nEpoch: 5, Steps: 16 | Train Loss: 0.2857185 Vali Loss: 0.0711854 Test Loss: 0.2954832\nValidation loss decreased (0.071436 --> 0.071185).  Saving model ...\nUpdating learning rate to 0.00026912277843012624\n[TRAIN MEMORY] Max memory allocated in epoch 6: 454.30 MB\nEpoch: 6 cost time: 2.3871889114379883\nEpoch: 6, Steps: 16 | Train Loss: 0.2852824 Vali Loss: 0.0713518 Test Loss: 0.2950851\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00044075849347707894\n[TRAIN MEMORY] Max memory allocated in epoch 7: 454.30 MB\nEpoch: 7 cost time: 2.3705387115478516\nEpoch: 7, Steps: 16 | Train Loss: 0.2847197 Vali Loss: 0.0711987 Test Loss: 0.2944084\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0006918179208976411\n[TRAIN MEMORY] Max memory allocated in epoch 8: 454.30 MB\nEpoch: 8 cost time: 2.337871789932251\nEpoch: 8, Steps: 16 | Train Loss: 0.2841120 Vali Loss: 0.0710159 Test Loss: 0.2933332\nValidation loss decreased (0.071185 --> 0.071016).  Saving model ...\nUpdating learning rate to 0.0010359584780123632\n[TRAIN MEMORY] Max memory allocated in epoch 9: 454.30 MB\nEpoch: 9 cost time: 2.330754518508911\nEpoch: 9, Steps: 16 | Train Loss: 0.2833334 Vali Loss: 0.0709253 Test Loss: 0.2924903\nValidation loss decreased (0.071016 --> 0.070925).  Saving model ...\nUpdating learning rate to 0.0014683358502649096\n[TRAIN MEMORY] Max memory allocated in epoch 10: 454.30 MB\nEpoch: 10 cost time: 2.3744075298309326\nEpoch: 10, Steps: 16 | Train Loss: 0.2821638 Vali Loss: 0.0709320 Test Loss: 0.2916014\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0019560522294776272\n[TRAIN MEMORY] Max memory allocated in epoch 11: 454.30 MB\nEpoch: 11 cost time: 2.2577672004699707\nEpoch: 11, Steps: 16 | Train Loss: 0.2817495 Vali Loss: 0.0709386 Test Loss: 0.2924062\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.002443662314871729\n[TRAIN MEMORY] Max memory allocated in epoch 12: 454.30 MB\nEpoch: 12 cost time: 2.29988169670105\nEpoch: 12, Steps: 16 | Train Loss: 0.2811100 Vali Loss: 0.0709195 Test Loss: 0.2922679\nValidation loss decreased (0.070925 --> 0.070919).  Saving model ...\nUpdating learning rate to 0.0028757205745829225\n[TRAIN MEMORY] Max memory allocated in epoch 13: 454.30 MB\nEpoch: 13 cost time: 2.3576204776763916\nEpoch: 13, Steps: 16 | Train Loss: 0.2799537 Vali Loss: 0.0711088 Test Loss: 0.2906774\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.003219328506918128\n[TRAIN MEMORY] Max memory allocated in epoch 14: 454.30 MB\nEpoch: 14 cost time: 2.264746904373169\nEpoch: 14, Steps: 16 | Train Loss: 0.2788746 Vali Loss: 0.0707754 Test Loss: 0.2893490\nValidation loss decreased (0.070919 --> 0.070775).  Saving model ...\nUpdating learning rate to 0.0034696406406008705\n[TRAIN MEMORY] Max memory allocated in epoch 15: 454.30 MB\nEpoch: 15 cost time: 2.3974263668060303\nEpoch: 15, Steps: 16 | Train Loss: 0.2776589 Vali Loss: 0.0711938 Test Loss: 0.2898386\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.003640312771742044\n[TRAIN MEMORY] Max memory allocated in epoch 16: 454.30 MB\nEpoch: 16 cost time: 2.3624701499938965\nEpoch: 16, Steps: 16 | Train Loss: 0.2762027 Vali Loss: 0.0710567 Test Loss: 0.2889830\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.003751200380516641\n[TRAIN MEMORY] Max memory allocated in epoch 17: 454.30 MB\nEpoch: 17 cost time: 2.3034653663635254\nEpoch: 17, Steps: 16 | Train Loss: 0.2756355 Vali Loss: 0.0710086 Test Loss: 0.2897096\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0038206720506809393\n[TRAIN MEMORY] Max memory allocated in epoch 18: 454.30 MB\nEpoch: 18 cost time: 2.319350004196167\nEpoch: 18, Steps: 16 | Train Loss: 0.2743813 Vali Loss: 0.0710965 Test Loss: 0.2889866\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.00386284516266587\n[TRAIN MEMORY] Max memory allocated in epoch 19: 454.30 MB\nEpoch: 19 cost time: 2.3238391876220703\nEpoch: 19, Steps: 16 | Train Loss: 0.2734276 Vali Loss: 0.0713734 Test Loss: 0.2917121\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0038875560962147167\n[TRAIN MEMORY] Max memory allocated in epoch 20: 454.30 MB\nEpoch: 20 cost time: 2.256183385848999\nEpoch: 20, Steps: 16 | Train Loss: 0.2731842 Vali Loss: 0.0711767 Test Loss: 0.2909028\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0039012837564544947\n[TRAIN MEMORY] Max memory allocated in epoch 21: 454.30 MB\nEpoch: 21 cost time: 2.2588388919830322\nEpoch: 21, Steps: 16 | Train Loss: 0.2715655 Vali Loss: 0.0712635 Test Loss: 0.2928826\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0039081556013882595\n[TRAIN MEMORY] Max memory allocated in epoch 22: 454.30 MB\nEpoch: 22 cost time: 2.322012186050415\nEpoch: 22, Steps: 16 | Train Loss: 0.2710072 Vali Loss: 0.0711946 Test Loss: 0.2920907\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.003910748284437151\n[TRAIN MEMORY] Max memory allocated in epoch 23: 454.30 MB\nEpoch: 23 cost time: 2.2985947132110596\nEpoch: 23, Steps: 16 | Train Loss: 0.2704540 Vali Loss: 0.0711308 Test Loss: 0.2921955\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.00391064989289833\n[TRAIN MEMORY] Max memory allocated in epoch 24: 454.30 MB\nEpoch: 24 cost time: 2.315894365310669\nEpoch: 24, Steps: 16 | Train Loss: 0.2693548 Vali Loss: 0.0712120 Test Loss: 0.2939389\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 2.35 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTm2_ftM_sl96_ll48_pl336_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 11185\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 1.85 MB\n[PARAMS] Total parameters: 483,621\nmse:0.2893489599227905, mae:0.32955503463745117\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data ETTm2 --learning_rate 0.005 --data_path ETTm2.csv --seq_len 96 --pred_len 720 --enc_in 7 --batch_size 2048 --lradj 'sigmoid' --dropout 0.5 --period_len 4 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T06:07:50.911713Z","iopub.execute_input":"2025-10-19T06:07:50.912227Z","iopub.status.idle":"2025-10-19T06:10:43.985720Z","shell.execute_reply.started":"2025-10-19T06:07:50.912203Z","shell.execute_reply":"2025-10-19T06:10:43.985023Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=4, d_model=128, data='ETTm2', root_path='./dataset', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=720, enc_in=7, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.005, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.5, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_ETTm2_ftM_sl96_ll48_pl720_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 33745\nval 10801\ntest 10801\n[TRAIN MEMORY] Max memory allocated in epoch 1: 840.89 MB\nEpoch: 1 cost time: 4.3656065464019775\nEpoch: 1, Steps: 16 | Train Loss: 0.4141163 Vali Loss: 0.0848847 Test Loss: 0.4166110\nValidation loss decreased (inf --> 0.084885).  Saving model ...\nUpdating learning rate to 1.9766777377990098e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 840.89 MB\nEpoch: 2 cost time: 3.6225976943969727\nEpoch: 2, Steps: 16 | Train Loss: 0.3518216 Vali Loss: 0.0835271 Test Loss: 0.4062701\nValidation loss decreased (0.084885 --> 0.083527).  Saving model ...\nUpdating learning rate to 5.297334308904794e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 840.89 MB\nEpoch: 3 cost time: 3.590622901916504\nEpoch: 3, Steps: 16 | Train Loss: 0.3429268 Vali Loss: 0.0812746 Test Loss: 0.3905703\nValidation loss decreased (0.083527 --> 0.081275).  Saving model ...\nUpdating learning rate to 0.00010772330336670662\n[TRAIN MEMORY] Max memory allocated in epoch 4: 840.89 MB\nEpoch: 4 cost time: 3.6135966777801514\nEpoch: 4, Steps: 16 | Train Loss: 0.3377067 Vali Loss: 0.0805510 Test Loss: 0.3864986\nValidation loss decreased (0.081275 --> 0.080551).  Saving model ...\nUpdating learning rate to 0.00019631651012203447\n[TRAIN MEMORY] Max memory allocated in epoch 5: 840.89 MB\nEpoch: 5 cost time: 3.679739475250244\nEpoch: 5, Steps: 16 | Train Loss: 0.3355487 Vali Loss: 0.0801803 Test Loss: 0.3855008\nValidation loss decreased (0.080551 --> 0.080180).  Saving model ...\nUpdating learning rate to 0.00033640347303765786\n[TRAIN MEMORY] Max memory allocated in epoch 6: 840.89 MB\nEpoch: 6 cost time: 3.642698287963867\nEpoch: 6, Steps: 16 | Train Loss: 0.3344495 Vali Loss: 0.0800619 Test Loss: 0.3846955\nValidation loss decreased (0.080180 --> 0.080062).  Saving model ...\nUpdating learning rate to 0.0005509481168463487\n[TRAIN MEMORY] Max memory allocated in epoch 7: 840.89 MB\nEpoch: 7 cost time: 3.614777088165283\nEpoch: 7, Steps: 16 | Train Loss: 0.3338619 Vali Loss: 0.0799072 Test Loss: 0.3837185\nValidation loss decreased (0.080062 --> 0.079907).  Saving model ...\nUpdating learning rate to 0.0008647724011220512\n[TRAIN MEMORY] Max memory allocated in epoch 8: 840.89 MB\nEpoch: 8 cost time: 3.640376567840576\nEpoch: 8, Steps: 16 | Train Loss: 0.3329298 Vali Loss: 0.0799149 Test Loss: 0.3831111\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.001294948097515454\n[TRAIN MEMORY] Max memory allocated in epoch 9: 840.89 MB\nEpoch: 9 cost time: 3.282679557800293\nEpoch: 9, Steps: 16 | Train Loss: 0.3321746 Vali Loss: 0.0797664 Test Loss: 0.3819458\nValidation loss decreased (0.079907 --> 0.079766).  Saving model ...\nUpdating learning rate to 0.0018354198128311366\n[TRAIN MEMORY] Max memory allocated in epoch 10: 840.89 MB\nEpoch: 10 cost time: 3.456469774246216\nEpoch: 10, Steps: 16 | Train Loss: 0.3312477 Vali Loss: 0.0796510 Test Loss: 0.3790356\nValidation loss decreased (0.079766 --> 0.079651).  Saving model ...\nUpdating learning rate to 0.002445065286847034\n[TRAIN MEMORY] Max memory allocated in epoch 11: 840.89 MB\nEpoch: 11 cost time: 3.649095058441162\nEpoch: 11, Steps: 16 | Train Loss: 0.3305109 Vali Loss: 0.0797849 Test Loss: 0.3794886\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0030545778935896616\n[TRAIN MEMORY] Max memory allocated in epoch 12: 840.89 MB\nEpoch: 12 cost time: 3.678542375564575\nEpoch: 12, Steps: 16 | Train Loss: 0.3292205 Vali Loss: 0.0796394 Test Loss: 0.3770336\nValidation loss decreased (0.079651 --> 0.079639).  Saving model ...\nUpdating learning rate to 0.003594650718228653\n[TRAIN MEMORY] Max memory allocated in epoch 13: 840.89 MB\nEpoch: 13 cost time: 3.605933666229248\nEpoch: 13, Steps: 16 | Train Loss: 0.3286598 Vali Loss: 0.0798380 Test Loss: 0.3794743\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00402416063364766\n[TRAIN MEMORY] Max memory allocated in epoch 14: 840.89 MB\nEpoch: 14 cost time: 3.5374929904937744\nEpoch: 14, Steps: 16 | Train Loss: 0.3281773 Vali Loss: 0.0795815 Test Loss: 0.3770438\nValidation loss decreased (0.079639 --> 0.079582).  Saving model ...\nUpdating learning rate to 0.004337050800751087\n[TRAIN MEMORY] Max memory allocated in epoch 15: 840.89 MB\nEpoch: 15 cost time: 3.2749335765838623\nEpoch: 15, Steps: 16 | Train Loss: 0.3270751 Vali Loss: 0.0802350 Test Loss: 0.3831343\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0045503909646775545\n[TRAIN MEMORY] Max memory allocated in epoch 16: 840.89 MB\nEpoch: 16 cost time: 3.5537872314453125\nEpoch: 16, Steps: 16 | Train Loss: 0.3260150 Vali Loss: 0.0797517 Test Loss: 0.3774692\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.004689000475645802\n[TRAIN MEMORY] Max memory allocated in epoch 17: 840.89 MB\nEpoch: 17 cost time: 3.5822224617004395\nEpoch: 17, Steps: 16 | Train Loss: 0.3252143 Vali Loss: 0.0798837 Test Loss: 0.3792124\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.004775840063351173\n[TRAIN MEMORY] Max memory allocated in epoch 18: 840.89 MB\nEpoch: 18 cost time: 3.5734822750091553\nEpoch: 18, Steps: 16 | Train Loss: 0.3246005 Vali Loss: 0.0797940 Test Loss: 0.3786396\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.004828556453332337\n[TRAIN MEMORY] Max memory allocated in epoch 19: 840.89 MB\nEpoch: 19 cost time: 3.563242197036743\nEpoch: 19, Steps: 16 | Train Loss: 0.3232153 Vali Loss: 0.0797165 Test Loss: 0.3782190\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0048594451202683955\n[TRAIN MEMORY] Max memory allocated in epoch 20: 840.89 MB\nEpoch: 20 cost time: 3.6152660846710205\nEpoch: 20, Steps: 16 | Train Loss: 0.3224602 Vali Loss: 0.0798363 Test Loss: 0.3816814\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.004876604695568118\n[TRAIN MEMORY] Max memory allocated in epoch 21: 840.89 MB\nEpoch: 21 cost time: 3.568336009979248\nEpoch: 21, Steps: 16 | Train Loss: 0.3217463 Vali Loss: 0.0796890 Test Loss: 0.3790747\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.004885194501735324\n[TRAIN MEMORY] Max memory allocated in epoch 22: 840.89 MB\nEpoch: 22 cost time: 3.557725191116333\nEpoch: 22, Steps: 16 | Train Loss: 0.3210872 Vali Loss: 0.0798878 Test Loss: 0.3801925\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0048884353555464395\n[TRAIN MEMORY] Max memory allocated in epoch 23: 840.89 MB\nEpoch: 23 cost time: 3.3718323707580566\nEpoch: 23, Steps: 16 | Train Loss: 0.3204720 Vali Loss: 0.0796510 Test Loss: 0.3793229\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.004888312366122913\n[TRAIN MEMORY] Max memory allocated in epoch 24: 840.89 MB\nEpoch: 24 cost time: 3.5941600799560547\nEpoch: 24, Steps: 16 | Train Loss: 0.3198857 Vali Loss: 0.0800066 Test Loss: 0.3805188\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 3.59 seconds\n>>>>>>>testing : transformer_test_xPatch_ETTm2_ftM_sl96_ll48_pl720_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 10801\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 6.88 MB\n[PARAMS] Total parameters: 1,801,605\nmse:0.37704381346702576, mae:0.38303354382514954\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data custom --learning_rate 0.007 --data_path weather.csv --seq_len 96 --pred_len 96 --enc_in 21 --batch_size 2048 --lradj 'sigmoid' --period_len 6 --d_model 128 --dropout 0.3 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T06:22:57.989529Z","iopub.execute_input":"2025-10-19T06:22:57.990236Z","iopub.status.idle":"2025-10-19T06:26:26.978369Z","shell.execute_reply.started":"2025-10-19T06:22:57.990212Z","shell.execute_reply":"2025-10-19T06:26:26.977661Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=6, d_model=128, data='custom', root_path='./dataset', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=96, enc_in=21, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.007, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.3, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_custom_ftM_sl96_ll48_pl96_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 36696\nval 5175\ntest 10444\n[TRAIN MEMORY] Max memory allocated in epoch 1: 695.88 MB\nEpoch: 1 cost time: 3.3841891288757324\nEpoch: 1, Steps: 17 | Train Loss: 0.3264396 Vali Loss: 0.0749858 Test Loss: 0.1958401\nValidation loss decreased (inf --> 0.074986).  Saving model ...\nUpdating learning rate to 2.7673488329186134e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 695.88 MB\nEpoch: 2 cost time: 2.7097690105438232\nEpoch: 2, Steps: 17 | Train Loss: 0.2940984 Vali Loss: 0.0739128 Test Loss: 0.1962997\nValidation loss decreased (0.074986 --> 0.073913).  Saving model ...\nUpdating learning rate to 7.416268032466711e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 695.88 MB\nEpoch: 3 cost time: 2.6327147483825684\nEpoch: 3, Steps: 17 | Train Loss: 0.2921770 Vali Loss: 0.0736093 Test Loss: 0.1968479\nValidation loss decreased (0.073913 --> 0.073609).  Saving model ...\nUpdating learning rate to 0.00015081262471338926\n[TRAIN MEMORY] Max memory allocated in epoch 4: 695.88 MB\nEpoch: 4 cost time: 2.5976157188415527\nEpoch: 4, Steps: 17 | Train Loss: 0.2910113 Vali Loss: 0.0724731 Test Loss: 0.1957727\nValidation loss decreased (0.073609 --> 0.072473).  Saving model ...\nUpdating learning rate to 0.00027484311417084827\n[TRAIN MEMORY] Max memory allocated in epoch 5: 695.88 MB\nEpoch: 5 cost time: 2.600156784057617\nEpoch: 5, Steps: 17 | Train Loss: 0.2895018 Vali Loss: 0.0723337 Test Loss: 0.1948581\nValidation loss decreased (0.072473 --> 0.072334).  Saving model ...\nUpdating learning rate to 0.000470964862252721\n[TRAIN MEMORY] Max memory allocated in epoch 6: 695.88 MB\nEpoch: 6 cost time: 2.58994460105896\nEpoch: 6, Steps: 17 | Train Loss: 0.2879083 Vali Loss: 0.0716787 Test Loss: 0.1927438\nValidation loss decreased (0.072334 --> 0.071679).  Saving model ...\nUpdating learning rate to 0.0007713273635848881\n[TRAIN MEMORY] Max memory allocated in epoch 7: 695.88 MB\nEpoch: 7 cost time: 2.5685911178588867\nEpoch: 7, Steps: 17 | Train Loss: 0.2860343 Vali Loss: 0.0710797 Test Loss: 0.1907198\nValidation loss decreased (0.071679 --> 0.071080).  Saving model ...\nUpdating learning rate to 0.0012106813615708717\n[TRAIN MEMORY] Max memory allocated in epoch 8: 695.88 MB\nEpoch: 8 cost time: 2.719696521759033\nEpoch: 8, Steps: 17 | Train Loss: 0.2827951 Vali Loss: 0.0705249 Test Loss: 0.1874473\nValidation loss decreased (0.071080 --> 0.070525).  Saving model ...\nUpdating learning rate to 0.0018129273365216358\n[TRAIN MEMORY] Max memory allocated in epoch 9: 695.88 MB\nEpoch: 9 cost time: 2.5750465393066406\nEpoch: 9, Steps: 17 | Train Loss: 0.2777959 Vali Loss: 0.0692670 Test Loss: 0.1809695\nValidation loss decreased (0.070525 --> 0.069267).  Saving model ...\nUpdating learning rate to 0.0025695877379635916\n[TRAIN MEMORY] Max memory allocated in epoch 10: 695.88 MB\nEpoch: 10 cost time: 2.6299326419830322\nEpoch: 10, Steps: 17 | Train Loss: 0.2713469 Vali Loss: 0.0689900 Test Loss: 0.1759632\nValidation loss decreased (0.069267 --> 0.068990).  Saving model ...\nUpdating learning rate to 0.003423091401585848\n[TRAIN MEMORY] Max memory allocated in epoch 11: 695.88 MB\nEpoch: 11 cost time: 2.578327178955078\nEpoch: 11, Steps: 17 | Train Loss: 0.2675725 Vali Loss: 0.0674825 Test Loss: 0.1729431\nValidation loss decreased (0.068990 --> 0.067482).  Saving model ...\nUpdating learning rate to 0.004276409051025526\n[TRAIN MEMORY] Max memory allocated in epoch 12: 695.88 MB\nEpoch: 12 cost time: 2.541840076446533\nEpoch: 12, Steps: 17 | Train Loss: 0.2655682 Vali Loss: 0.0679508 Test Loss: 0.1716428\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.005032511005520115\n[TRAIN MEMORY] Max memory allocated in epoch 13: 695.88 MB\nEpoch: 13 cost time: 2.616848945617676\nEpoch: 13, Steps: 17 | Train Loss: 0.2640571 Vali Loss: 0.0679793 Test Loss: 0.1688934\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.005633824887106724\n[TRAIN MEMORY] Max memory allocated in epoch 14: 695.88 MB\nEpoch: 14 cost time: 2.521254301071167\nEpoch: 14, Steps: 17 | Train Loss: 0.2626948 Vali Loss: 0.0670175 Test Loss: 0.1701913\nValidation loss decreased (0.067482 --> 0.067018).  Saving model ...\nUpdating learning rate to 0.0060718711210515225\n[TRAIN MEMORY] Max memory allocated in epoch 15: 695.88 MB\nEpoch: 15 cost time: 2.556063652038574\nEpoch: 15, Steps: 17 | Train Loss: 0.2611810 Vali Loss: 0.0668015 Test Loss: 0.1692287\nValidation loss decreased (0.067018 --> 0.066802).  Saving model ...\nUpdating learning rate to 0.006370547350548577\n[TRAIN MEMORY] Max memory allocated in epoch 16: 695.88 MB\nEpoch: 16 cost time: 2.5636072158813477\nEpoch: 16, Steps: 17 | Train Loss: 0.2608003 Vali Loss: 0.0681587 Test Loss: 0.1702601\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006564600665904122\n[TRAIN MEMORY] Max memory allocated in epoch 17: 695.88 MB\nEpoch: 17 cost time: 2.6536779403686523\nEpoch: 17, Steps: 17 | Train Loss: 0.2593317 Vali Loss: 0.0670518 Test Loss: 0.1684703\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.006686176088691644\n[TRAIN MEMORY] Max memory allocated in epoch 18: 695.88 MB\nEpoch: 18 cost time: 2.5733656883239746\nEpoch: 18, Steps: 17 | Train Loss: 0.2587513 Vali Loss: 0.0678074 Test Loss: 0.1689141\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0067599790346652726\n[TRAIN MEMORY] Max memory allocated in epoch 19: 695.88 MB\nEpoch: 19 cost time: 2.6619789600372314\nEpoch: 19, Steps: 17 | Train Loss: 0.2588024 Vali Loss: 0.0671774 Test Loss: 0.1677365\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.006803223168375754\n[TRAIN MEMORY] Max memory allocated in epoch 20: 695.88 MB\nEpoch: 20 cost time: 2.593627452850342\nEpoch: 20, Steps: 17 | Train Loss: 0.2567848 Vali Loss: 0.0667986 Test Loss: 0.1668732\nValidation loss decreased (0.066802 --> 0.066799).  Saving model ...\nUpdating learning rate to 0.006827246573795366\n[TRAIN MEMORY] Max memory allocated in epoch 21: 695.88 MB\nEpoch: 21 cost time: 2.616971254348755\nEpoch: 21, Steps: 17 | Train Loss: 0.2559870 Vali Loss: 0.0669064 Test Loss: 0.1676264\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006839272302429454\n[TRAIN MEMORY] Max memory allocated in epoch 22: 695.88 MB\nEpoch: 22 cost time: 2.586470603942871\nEpoch: 22, Steps: 17 | Train Loss: 0.2553843 Vali Loss: 0.0661937 Test Loss: 0.1668060\nValidation loss decreased (0.066799 --> 0.066194).  Saving model ...\nUpdating learning rate to 0.006843809497765015\n[TRAIN MEMORY] Max memory allocated in epoch 23: 695.88 MB\nEpoch: 23 cost time: 2.6061325073242188\nEpoch: 23, Steps: 17 | Train Loss: 0.2553833 Vali Loss: 0.0676249 Test Loss: 0.1664051\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006843637312572078\n[TRAIN MEMORY] Max memory allocated in epoch 24: 695.88 MB\nEpoch: 24 cost time: 2.6229567527770996\nEpoch: 24, Steps: 17 | Train Loss: 0.2552594 Vali Loss: 0.0669573 Test Loss: 0.1669328\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.006840453745086283\n[TRAIN MEMORY] Max memory allocated in epoch 25: 695.88 MB\nEpoch: 25 cost time: 2.5716593265533447\nEpoch: 25, Steps: 17 | Train Loss: 0.2534180 Vali Loss: 0.0660668 Test Loss: 0.1663514\nValidation loss decreased (0.066194 --> 0.066067).  Saving model ...\nUpdating learning rate to 0.006835288960171356\n[TRAIN MEMORY] Max memory allocated in epoch 26: 695.88 MB\nEpoch: 26 cost time: 2.667989492416382\nEpoch: 26, Steps: 17 | Train Loss: 0.2531635 Vali Loss: 0.0656786 Test Loss: 0.1660301\nValidation loss decreased (0.066067 --> 0.065679).  Saving model ...\nUpdating learning rate to 0.0068287633991630506\n[TRAIN MEMORY] Max memory allocated in epoch 27: 695.88 MB\nEpoch: 27 cost time: 2.6014230251312256\nEpoch: 27, Steps: 17 | Train Loss: 0.2524764 Vali Loss: 0.0674141 Test Loss: 0.1648701\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006821247088565512\n[TRAIN MEMORY] Max memory allocated in epoch 28: 695.88 MB\nEpoch: 28 cost time: 2.657278537750244\nEpoch: 28, Steps: 17 | Train Loss: 0.2526602 Vali Loss: 0.0660392 Test Loss: 0.1667521\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.006812957282930035\n[TRAIN MEMORY] Max memory allocated in epoch 29: 695.88 MB\nEpoch: 29 cost time: 2.608788251876831\nEpoch: 29, Steps: 17 | Train Loss: 0.2515710 Vali Loss: 0.0655379 Test Loss: 0.1670316\nValidation loss decreased (0.065679 --> 0.065538).  Saving model ...\nUpdating learning rate to 0.0068040180599189155\n[TRAIN MEMORY] Max memory allocated in epoch 30: 695.88 MB\nEpoch: 30 cost time: 2.633518695831299\nEpoch: 30, Steps: 17 | Train Loss: 0.2509158 Vali Loss: 0.0659156 Test Loss: 0.1656046\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006794496599659588\n[TRAIN MEMORY] Max memory allocated in epoch 31: 695.88 MB\nEpoch: 31 cost time: 2.6232964992523193\nEpoch: 31, Steps: 17 | Train Loss: 0.2511982 Vali Loss: 0.0660553 Test Loss: 0.1669301\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.006784425234662161\n[TRAIN MEMORY] Max memory allocated in epoch 32: 695.88 MB\nEpoch: 32 cost time: 2.5805764198303223\nEpoch: 32, Steps: 17 | Train Loss: 0.2503220 Vali Loss: 0.0659209 Test Loss: 0.1661649\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.00677381483715791\n[TRAIN MEMORY] Max memory allocated in epoch 33: 695.88 MB\nEpoch: 33 cost time: 2.5805470943450928\nEpoch: 33, Steps: 17 | Train Loss: 0.2503729 Vali Loss: 0.0661942 Test Loss: 0.1662965\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.006762662940948887\n[TRAIN MEMORY] Max memory allocated in epoch 34: 695.88 MB\nEpoch: 34 cost time: 2.607088565826416\nEpoch: 34, Steps: 17 | Train Loss: 0.2507152 Vali Loss: 0.0656189 Test Loss: 0.1664360\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0067509586658693315\n[TRAIN MEMORY] Max memory allocated in epoch 35: 695.88 MB\nEpoch: 35 cost time: 2.6336050033569336\nEpoch: 35, Steps: 17 | Train Loss: 0.2497801 Vali Loss: 0.0654400 Test Loss: 0.1659801\nValidation loss decreased (0.065538 --> 0.065440).  Saving model ...\nUpdating learning rate to 0.006738685702116105\n[TRAIN MEMORY] Max memory allocated in epoch 36: 695.88 MB\nEpoch: 36 cost time: 2.589569330215454\nEpoch: 36, Steps: 17 | Train Loss: 0.2482326 Vali Loss: 0.0660457 Test Loss: 0.1652893\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006725824118152564\n[TRAIN MEMORY] Max memory allocated in epoch 37: 695.88 MB\nEpoch: 37 cost time: 2.5831918716430664\nEpoch: 37, Steps: 17 | Train Loss: 0.2489754 Vali Loss: 0.0655643 Test Loss: 0.1662855\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0067123514558962965\n[TRAIN MEMORY] Max memory allocated in epoch 38: 695.88 MB\nEpoch: 38 cost time: 2.6108672618865967\nEpoch: 38, Steps: 17 | Train Loss: 0.2487324 Vali Loss: 0.0659729 Test Loss: 0.1654987\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.006698243394716203\n[TRAIN MEMORY] Max memory allocated in epoch 39: 695.88 MB\nEpoch: 39 cost time: 2.5816597938537598\nEpoch: 39, Steps: 17 | Train Loss: 0.2486183 Vali Loss: 0.0656683 Test Loss: 0.1657440\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.00668347415518513\n[TRAIN MEMORY] Max memory allocated in epoch 40: 695.88 MB\nEpoch: 40 cost time: 2.589907169342041\nEpoch: 40, Steps: 17 | Train Loss: 0.2485810 Vali Loss: 0.0665395 Test Loss: 0.1672376\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.006668016746441443\n[TRAIN MEMORY] Max memory allocated in epoch 41: 695.88 MB\nEpoch: 41 cost time: 2.64233136177063\nEpoch: 41, Steps: 17 | Train Loss: 0.2471996 Vali Loss: 0.0660843 Test Loss: 0.1652618\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.006651843120316391\n[TRAIN MEMORY] Max memory allocated in epoch 42: 695.88 MB\nEpoch: 42 cost time: 2.5175724029541016\nEpoch: 42, Steps: 17 | Train Loss: 0.2469947 Vali Loss: 0.0663297 Test Loss: 0.1660407\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0066349242707049415\n[TRAIN MEMORY] Max memory allocated in epoch 43: 695.88 MB\nEpoch: 43 cost time: 2.6180903911590576\nEpoch: 43, Steps: 17 | Train Loss: 0.2467449 Vali Loss: 0.0657920 Test Loss: 0.1661645\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.006617230301696211\n[TRAIN MEMORY] Max memory allocated in epoch 44: 695.88 MB\nEpoch: 44 cost time: 2.6654915809631348\nEpoch: 44, Steps: 17 | Train Loss: 0.2469303 Vali Loss: 0.0656643 Test Loss: 0.1663051\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.006598730478912291\n[TRAIN MEMORY] Max memory allocated in epoch 45: 695.88 MB\nEpoch: 45 cost time: 2.5811991691589355\nEpoch: 45, Steps: 17 | Train Loss: 0.2464931 Vali Loss: 0.0658019 Test Loss: 0.1658730\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 2.62 seconds\n>>>>>>>testing : transformer_test_xPatch_custom_ftM_sl96_ll48_pl96_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 10444\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.41 MB\n[PARAMS] Total parameters: 105,197\nmse:0.16598011553287506, mae:0.20178931951522827\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data custom --learning_rate 0.007 --data_path weather.csv --seq_len 96 --pred_len 192 --enc_in 21 --batch_size 2048 --lradj 'sigmoid' --period_len 6 --d_model 128 --dropout 0.3 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T06:32:20.666169Z","iopub.execute_input":"2025-10-19T06:32:20.666432Z","iopub.status.idle":"2025-10-19T06:39:24.968411Z","shell.execute_reply.started":"2025-10-19T06:32:20.666407Z","shell.execute_reply":"2025-10-19T06:39:24.967565Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=6, d_model=128, data='custom', root_path='./dataset', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=192, enc_in=21, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.007, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.3, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_custom_ftM_sl96_ll48_pl192_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 36600\nval 5079\ntest 10348\n[TRAIN MEMORY] Max memory allocated in epoch 1: 972.84 MB\nEpoch: 1 cost time: 4.410773038864136\nEpoch: 1, Steps: 17 | Train Loss: 0.3600578 Vali Loss: 0.0814233 Test Loss: 0.2437811\nValidation loss decreased (inf --> 0.081423).  Saving model ...\nUpdating learning rate to 2.7673488329186134e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 972.59 MB\nEpoch: 2 cost time: 3.509439468383789\nEpoch: 2, Steps: 17 | Train Loss: 0.3228398 Vali Loss: 0.0804944 Test Loss: 0.2421813\nValidation loss decreased (0.081423 --> 0.080494).  Saving model ...\nUpdating learning rate to 7.416268032466711e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 972.59 MB\nEpoch: 3 cost time: 3.3754920959472656\nEpoch: 3, Steps: 17 | Train Loss: 0.3199908 Vali Loss: 0.0799159 Test Loss: 0.2402088\nValidation loss decreased (0.080494 --> 0.079916).  Saving model ...\nUpdating learning rate to 0.00015081262471338926\n[TRAIN MEMORY] Max memory allocated in epoch 4: 972.59 MB\nEpoch: 4 cost time: 3.580310821533203\nEpoch: 4, Steps: 17 | Train Loss: 0.3180917 Vali Loss: 0.0792339 Test Loss: 0.2398240\nValidation loss decreased (0.079916 --> 0.079234).  Saving model ...\nUpdating learning rate to 0.00027484311417084827\n[TRAIN MEMORY] Max memory allocated in epoch 5: 972.59 MB\nEpoch: 5 cost time: 3.4221060276031494\nEpoch: 5, Steps: 17 | Train Loss: 0.3165439 Vali Loss: 0.0786414 Test Loss: 0.2387248\nValidation loss decreased (0.079234 --> 0.078641).  Saving model ...\nUpdating learning rate to 0.000470964862252721\n[TRAIN MEMORY] Max memory allocated in epoch 6: 972.59 MB\nEpoch: 6 cost time: 3.458447217941284\nEpoch: 6, Steps: 17 | Train Loss: 0.3152190 Vali Loss: 0.0785597 Test Loss: 0.2376728\nValidation loss decreased (0.078641 --> 0.078560).  Saving model ...\nUpdating learning rate to 0.0007713273635848881\n[TRAIN MEMORY] Max memory allocated in epoch 7: 972.59 MB\nEpoch: 7 cost time: 3.5310871601104736\nEpoch: 7, Steps: 17 | Train Loss: 0.3135094 Vali Loss: 0.0773648 Test Loss: 0.2348884\nValidation loss decreased (0.078560 --> 0.077365).  Saving model ...\nUpdating learning rate to 0.0012106813615708717\n[TRAIN MEMORY] Max memory allocated in epoch 8: 972.59 MB\nEpoch: 8 cost time: 3.439943313598633\nEpoch: 8, Steps: 17 | Train Loss: 0.3116369 Vali Loss: 0.0770630 Test Loss: 0.2329869\nValidation loss decreased (0.077365 --> 0.077063).  Saving model ...\nUpdating learning rate to 0.0018129273365216358\n[TRAIN MEMORY] Max memory allocated in epoch 9: 972.59 MB\nEpoch: 9 cost time: 3.5136876106262207\nEpoch: 9, Steps: 17 | Train Loss: 0.3091084 Vali Loss: 0.0760429 Test Loss: 0.2292387\nValidation loss decreased (0.077063 --> 0.076043).  Saving model ...\nUpdating learning rate to 0.0025695877379635916\n[TRAIN MEMORY] Max memory allocated in epoch 10: 972.59 MB\nEpoch: 10 cost time: 3.4459590911865234\nEpoch: 10, Steps: 17 | Train Loss: 0.3061578 Vali Loss: 0.0751822 Test Loss: 0.2252891\nValidation loss decreased (0.076043 --> 0.075182).  Saving model ...\nUpdating learning rate to 0.003423091401585848\n[TRAIN MEMORY] Max memory allocated in epoch 11: 972.59 MB\nEpoch: 11 cost time: 3.2945284843444824\nEpoch: 11, Steps: 17 | Train Loss: 0.3052882 Vali Loss: 0.0753083 Test Loss: 0.2206738\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.004276409051025526\n[TRAIN MEMORY] Max memory allocated in epoch 12: 972.59 MB\nEpoch: 12 cost time: 3.4457685947418213\nEpoch: 12, Steps: 17 | Train Loss: 0.3002195 Vali Loss: 0.0742730 Test Loss: 0.2186612\nValidation loss decreased (0.075182 --> 0.074273).  Saving model ...\nUpdating learning rate to 0.005032511005520115\n[TRAIN MEMORY] Max memory allocated in epoch 13: 972.59 MB\nEpoch: 13 cost time: 3.507819652557373\nEpoch: 13, Steps: 17 | Train Loss: 0.2980349 Vali Loss: 0.0747759 Test Loss: 0.2173368\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.005633824887106724\n[TRAIN MEMORY] Max memory allocated in epoch 14: 972.59 MB\nEpoch: 14 cost time: 3.706115484237671\nEpoch: 14, Steps: 17 | Train Loss: 0.2963636 Vali Loss: 0.0741561 Test Loss: 0.2157969\nValidation loss decreased (0.074273 --> 0.074156).  Saving model ...\nUpdating learning rate to 0.0060718711210515225\n[TRAIN MEMORY] Max memory allocated in epoch 15: 972.59 MB\nEpoch: 15 cost time: 3.4688031673431396\nEpoch: 15, Steps: 17 | Train Loss: 0.2960116 Vali Loss: 0.0740243 Test Loss: 0.2149103\nValidation loss decreased (0.074156 --> 0.074024).  Saving model ...\nUpdating learning rate to 0.006370547350548577\n[TRAIN MEMORY] Max memory allocated in epoch 16: 972.59 MB\nEpoch: 16 cost time: 3.4115729331970215\nEpoch: 16, Steps: 17 | Train Loss: 0.2942918 Vali Loss: 0.0737802 Test Loss: 0.2142757\nValidation loss decreased (0.074024 --> 0.073780).  Saving model ...\nUpdating learning rate to 0.006564600665904122\n[TRAIN MEMORY] Max memory allocated in epoch 17: 972.59 MB\nEpoch: 17 cost time: 3.4018678665161133\nEpoch: 17, Steps: 17 | Train Loss: 0.2920444 Vali Loss: 0.0735136 Test Loss: 0.2143328\nValidation loss decreased (0.073780 --> 0.073514).  Saving model ...\nUpdating learning rate to 0.006686176088691644\n[TRAIN MEMORY] Max memory allocated in epoch 18: 972.59 MB\nEpoch: 18 cost time: 3.3467249870300293\nEpoch: 18, Steps: 17 | Train Loss: 0.2919024 Vali Loss: 0.0726967 Test Loss: 0.2135755\nValidation loss decreased (0.073514 --> 0.072697).  Saving model ...\nUpdating learning rate to 0.0067599790346652726\n[TRAIN MEMORY] Max memory allocated in epoch 19: 972.59 MB\nEpoch: 19 cost time: 3.5576577186584473\nEpoch: 19, Steps: 17 | Train Loss: 0.2903321 Vali Loss: 0.0730821 Test Loss: 0.2142961\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006803223168375754\n[TRAIN MEMORY] Max memory allocated in epoch 20: 972.59 MB\nEpoch: 20 cost time: 3.298811912536621\nEpoch: 20, Steps: 17 | Train Loss: 0.2895001 Vali Loss: 0.0732144 Test Loss: 0.2141270\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.006827246573795366\n[TRAIN MEMORY] Max memory allocated in epoch 21: 972.59 MB\nEpoch: 21 cost time: 3.2616209983825684\nEpoch: 21, Steps: 17 | Train Loss: 0.2887345 Vali Loss: 0.0728888 Test Loss: 0.2123584\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.006839272302429454\n[TRAIN MEMORY] Max memory allocated in epoch 22: 972.59 MB\nEpoch: 22 cost time: 3.362175464630127\nEpoch: 22, Steps: 17 | Train Loss: 0.2881965 Vali Loss: 0.0728183 Test Loss: 0.2120288\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.006843809497765015\n[TRAIN MEMORY] Max memory allocated in epoch 23: 972.59 MB\nEpoch: 23 cost time: 3.503025531768799\nEpoch: 23, Steps: 17 | Train Loss: 0.2869843 Vali Loss: 0.0725330 Test Loss: 0.2114873\nValidation loss decreased (0.072697 --> 0.072533).  Saving model ...\nUpdating learning rate to 0.006843637312572078\n[TRAIN MEMORY] Max memory allocated in epoch 24: 972.59 MB\nEpoch: 24 cost time: 3.508314371109009\nEpoch: 24, Steps: 17 | Train Loss: 0.2871161 Vali Loss: 0.0729571 Test Loss: 0.2108464\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006840453745086283\n[TRAIN MEMORY] Max memory allocated in epoch 25: 972.59 MB\nEpoch: 25 cost time: 3.623938798904419\nEpoch: 25, Steps: 17 | Train Loss: 0.2860627 Vali Loss: 0.0725463 Test Loss: 0.2113269\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.006835288960171356\n[TRAIN MEMORY] Max memory allocated in epoch 26: 972.59 MB\nEpoch: 26 cost time: 3.575695037841797\nEpoch: 26, Steps: 17 | Train Loss: 0.2854908 Vali Loss: 0.0726444 Test Loss: 0.2111410\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0068287633991630506\n[TRAIN MEMORY] Max memory allocated in epoch 27: 972.59 MB\nEpoch: 27 cost time: 3.432558298110962\nEpoch: 27, Steps: 17 | Train Loss: 0.2856154 Vali Loss: 0.0723395 Test Loss: 0.2109533\nValidation loss decreased (0.072533 --> 0.072340).  Saving model ...\nUpdating learning rate to 0.006821247088565512\n[TRAIN MEMORY] Max memory allocated in epoch 28: 972.59 MB\nEpoch: 28 cost time: 3.537036418914795\nEpoch: 28, Steps: 17 | Train Loss: 0.2844889 Vali Loss: 0.0728853 Test Loss: 0.2119093\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006812957282930035\n[TRAIN MEMORY] Max memory allocated in epoch 29: 972.59 MB\nEpoch: 29 cost time: 3.470412254333496\nEpoch: 29, Steps: 17 | Train Loss: 0.2843953 Vali Loss: 0.0723039 Test Loss: 0.2120802\nValidation loss decreased (0.072340 --> 0.072304).  Saving model ...\nUpdating learning rate to 0.0068040180599189155\n[TRAIN MEMORY] Max memory allocated in epoch 30: 972.59 MB\nEpoch: 30 cost time: 3.605970621109009\nEpoch: 30, Steps: 17 | Train Loss: 0.2836769 Vali Loss: 0.0728154 Test Loss: 0.2107059\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006794496599659588\n[TRAIN MEMORY] Max memory allocated in epoch 31: 972.59 MB\nEpoch: 31 cost time: 3.4111971855163574\nEpoch: 31, Steps: 17 | Train Loss: 0.2836822 Vali Loss: 0.0722500 Test Loss: 0.2116982\nValidation loss decreased (0.072304 --> 0.072250).  Saving model ...\nUpdating learning rate to 0.006784425234662161\n[TRAIN MEMORY] Max memory allocated in epoch 32: 972.59 MB\nEpoch: 32 cost time: 3.5322964191436768\nEpoch: 32, Steps: 17 | Train Loss: 0.2833356 Vali Loss: 0.0728550 Test Loss: 0.2117137\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00677381483715791\n[TRAIN MEMORY] Max memory allocated in epoch 33: 972.59 MB\nEpoch: 33 cost time: 3.449951648712158\nEpoch: 33, Steps: 17 | Train Loss: 0.2835277 Vali Loss: 0.0726122 Test Loss: 0.2111071\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.006762662940948887\n[TRAIN MEMORY] Max memory allocated in epoch 34: 972.59 MB\nEpoch: 34 cost time: 3.5904974937438965\nEpoch: 34, Steps: 17 | Train Loss: 0.2827434 Vali Loss: 0.0725367 Test Loss: 0.2106102\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0067509586658693315\n[TRAIN MEMORY] Max memory allocated in epoch 35: 972.59 MB\nEpoch: 35 cost time: 3.4780454635620117\nEpoch: 35, Steps: 17 | Train Loss: 0.2817615 Vali Loss: 0.0722767 Test Loss: 0.2107693\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.006738685702116105\n[TRAIN MEMORY] Max memory allocated in epoch 36: 972.59 MB\nEpoch: 36 cost time: 3.5550193786621094\nEpoch: 36, Steps: 17 | Train Loss: 0.2814732 Vali Loss: 0.0723430 Test Loss: 0.2099959\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.006725824118152564\n[TRAIN MEMORY] Max memory allocated in epoch 37: 972.59 MB\nEpoch: 37 cost time: 3.6060373783111572\nEpoch: 37, Steps: 17 | Train Loss: 0.2807521 Vali Loss: 0.0720926 Test Loss: 0.2099499\nValidation loss decreased (0.072250 --> 0.072093).  Saving model ...\nUpdating learning rate to 0.0067123514558962965\n[TRAIN MEMORY] Max memory allocated in epoch 38: 972.59 MB\nEpoch: 38 cost time: 3.412307024002075\nEpoch: 38, Steps: 17 | Train Loss: 0.2805862 Vali Loss: 0.0720405 Test Loss: 0.2105731\nValidation loss decreased (0.072093 --> 0.072041).  Saving model ...\nUpdating learning rate to 0.006698243394716203\n[TRAIN MEMORY] Max memory allocated in epoch 39: 972.59 MB\nEpoch: 39 cost time: 3.4217543601989746\nEpoch: 39, Steps: 17 | Train Loss: 0.2804982 Vali Loss: 0.0720599 Test Loss: 0.2098597\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00668347415518513\n[TRAIN MEMORY] Max memory allocated in epoch 40: 972.59 MB\nEpoch: 40 cost time: 3.403066635131836\nEpoch: 40, Steps: 17 | Train Loss: 0.2800572 Vali Loss: 0.0719774 Test Loss: 0.2106800\nValidation loss decreased (0.072041 --> 0.071977).  Saving model ...\nUpdating learning rate to 0.006668016746441443\n[TRAIN MEMORY] Max memory allocated in epoch 41: 972.59 MB\nEpoch: 41 cost time: 3.5296924114227295\nEpoch: 41, Steps: 17 | Train Loss: 0.2792922 Vali Loss: 0.0717543 Test Loss: 0.2108337\nValidation loss decreased (0.071977 --> 0.071754).  Saving model ...\nUpdating learning rate to 0.006651843120316391\n[TRAIN MEMORY] Max memory allocated in epoch 42: 972.59 MB\nEpoch: 42 cost time: 3.5524911880493164\nEpoch: 42, Steps: 17 | Train Loss: 0.2800416 Vali Loss: 0.0717021 Test Loss: 0.2108335\nValidation loss decreased (0.071754 --> 0.071702).  Saving model ...\nUpdating learning rate to 0.0066349242707049415\n[TRAIN MEMORY] Max memory allocated in epoch 43: 972.59 MB\nEpoch: 43 cost time: 3.474973201751709\nEpoch: 43, Steps: 17 | Train Loss: 0.2789527 Vali Loss: 0.0719972 Test Loss: 0.2105474\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006617230301696211\n[TRAIN MEMORY] Max memory allocated in epoch 44: 972.59 MB\nEpoch: 44 cost time: 3.3206207752227783\nEpoch: 44, Steps: 17 | Train Loss: 0.2792292 Vali Loss: 0.0721654 Test Loss: 0.2115342\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.006598730478912291\n[TRAIN MEMORY] Max memory allocated in epoch 45: 972.59 MB\nEpoch: 45 cost time: 3.4060122966766357\nEpoch: 45, Steps: 17 | Train Loss: 0.2795272 Vali Loss: 0.0719211 Test Loss: 0.2108471\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.00657939327301201\n[TRAIN MEMORY] Max memory allocated in epoch 46: 972.59 MB\nEpoch: 46 cost time: 3.4356253147125244\nEpoch: 46, Steps: 17 | Train Loss: 0.2788281 Vali Loss: 0.0720206 Test Loss: 0.2100307\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.006559186400991168\n[TRAIN MEMORY] Max memory allocated in epoch 47: 972.59 MB\nEpoch: 47 cost time: 3.562138557434082\nEpoch: 47, Steps: 17 | Train Loss: 0.2788122 Vali Loss: 0.0723581 Test Loss: 0.2106150\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.006538076868899322\n[TRAIN MEMORY] Max memory allocated in epoch 48: 972.59 MB\nEpoch: 48 cost time: 3.4138526916503906\nEpoch: 48, Steps: 17 | Train Loss: 0.2783391 Vali Loss: 0.0720565 Test Loss: 0.2106413\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.006516031018376998\n[TRAIN MEMORY] Max memory allocated in epoch 49: 972.59 MB\nEpoch: 49 cost time: 3.455404758453369\nEpoch: 49, Steps: 17 | Train Loss: 0.2777745 Vali Loss: 0.0720060 Test Loss: 0.2097820\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.006493014578681501\n[TRAIN MEMORY] Max memory allocated in epoch 50: 972.59 MB\nEpoch: 50 cost time: 3.398280143737793\nEpoch: 50, Steps: 17 | Train Loss: 0.2777686 Vali Loss: 0.0719597 Test Loss: 0.2101826\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0064689927254232196\n[TRAIN MEMORY] Max memory allocated in epoch 51: 972.59 MB\nEpoch: 51 cost time: 3.390049934387207\nEpoch: 51, Steps: 17 | Train Loss: 0.2777227 Vali Loss: 0.0716757 Test Loss: 0.2109163\nValidation loss decreased (0.071702 --> 0.071676).  Saving model ...\nUpdating learning rate to 0.006443930146961082\n[TRAIN MEMORY] Max memory allocated in epoch 52: 972.59 MB\nEpoch: 52 cost time: 3.5445125102996826\nEpoch: 52, Steps: 17 | Train Loss: 0.2771549 Vali Loss: 0.0719318 Test Loss: 0.2102370\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0064177911192347505\n[TRAIN MEMORY] Max memory allocated in epoch 53: 972.59 MB\nEpoch: 53 cost time: 3.44183349609375\nEpoch: 53, Steps: 17 | Train Loss: 0.2775238 Vali Loss: 0.0726431 Test Loss: 0.2102438\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0063905395896987635\n[TRAIN MEMORY] Max memory allocated in epoch 54: 972.59 MB\nEpoch: 54 cost time: 3.6887123584747314\nEpoch: 54, Steps: 17 | Train Loss: 0.2777165 Vali Loss: 0.0729493 Test Loss: 0.2114068\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.006362139270943379\n[TRAIN MEMORY] Max memory allocated in epoch 55: 972.59 MB\nEpoch: 55 cost time: 3.5123131275177\nEpoch: 55, Steps: 17 | Train Loss: 0.2774003 Vali Loss: 0.0720659 Test Loss: 0.2106514\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.006332553744521905\n[TRAIN MEMORY] Max memory allocated in epoch 56: 972.59 MB\nEpoch: 56 cost time: 3.5082156658172607\nEpoch: 56, Steps: 17 | Train Loss: 0.2770884 Vali Loss: 0.0722730 Test Loss: 0.2106209\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.006301746575443873\n[TRAIN MEMORY] Max memory allocated in epoch 57: 972.59 MB\nEpoch: 57 cost time: 3.525564432144165\nEpoch: 57, Steps: 17 | Train Loss: 0.2759176 Vali Loss: 0.0715925 Test Loss: 0.2097605\nValidation loss decreased (0.071676 --> 0.071593).  Saving model ...\nUpdating learning rate to 0.0062696814377313\n[TRAIN MEMORY] Max memory allocated in epoch 58: 972.59 MB\nEpoch: 58 cost time: 3.466334342956543\nEpoch: 58, Steps: 17 | Train Loss: 0.2758135 Vali Loss: 0.0721042 Test Loss: 0.2102146\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006236322251366451\n[TRAIN MEMORY] Max memory allocated in epoch 59: 972.59 MB\nEpoch: 59 cost time: 3.405289649963379\nEpoch: 59, Steps: 17 | Train Loss: 0.2756710 Vali Loss: 0.0715575 Test Loss: 0.2109474\nValidation loss decreased (0.071593 --> 0.071557).  Saving model ...\nUpdating learning rate to 0.006201633330881183\n[TRAIN MEMORY] Max memory allocated in epoch 60: 972.59 MB\nEpoch: 60 cost time: 3.427931547164917\nEpoch: 60, Steps: 17 | Train Loss: 0.2757899 Vali Loss: 0.0720589 Test Loss: 0.2103477\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.006165579545747962\n[TRAIN MEMORY] Max memory allocated in epoch 61: 972.59 MB\nEpoch: 61 cost time: 3.3985660076141357\nEpoch: 61, Steps: 17 | Train Loss: 0.2761376 Vali Loss: 0.0724156 Test Loss: 0.2107027\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.006128126492629121\n[TRAIN MEMORY] Max memory allocated in epoch 62: 972.59 MB\nEpoch: 62 cost time: 3.55936598777771\nEpoch: 62, Steps: 17 | Train Loss: 0.2757517 Vali Loss: 0.0719914 Test Loss: 0.2113130\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.006089240679423252\n[TRAIN MEMORY] Max memory allocated in epoch 63: 972.59 MB\nEpoch: 63 cost time: 3.5602824687957764\nEpoch: 63, Steps: 17 | Train Loss: 0.2749819 Vali Loss: 0.0716544 Test Loss: 0.2107367\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.006048889720914648\n[TRAIN MEMORY] Max memory allocated in epoch 64: 972.59 MB\nEpoch: 64 cost time: 3.559370279312134\nEpoch: 64, Steps: 17 | Train Loss: 0.2753201 Vali Loss: 0.0721682 Test Loss: 0.2105395\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.006007042545683429\n[TRAIN MEMORY] Max memory allocated in epoch 65: 972.59 MB\nEpoch: 65 cost time: 3.3561465740203857\nEpoch: 65, Steps: 17 | Train Loss: 0.2752442 Vali Loss: 0.0723643 Test Loss: 0.2120321\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.005963669613770194\n[TRAIN MEMORY] Max memory allocated in epoch 66: 972.59 MB\nEpoch: 66 cost time: 3.5469765663146973\nEpoch: 66, Steps: 17 | Train Loss: 0.2753000 Vali Loss: 0.0721189 Test Loss: 0.2108889\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.005918743144410417\n[TRAIN MEMORY] Max memory allocated in epoch 67: 972.59 MB\nEpoch: 67 cost time: 3.4797775745391846\nEpoch: 67, Steps: 17 | Train Loss: 0.2746517 Vali Loss: 0.0718545 Test Loss: 0.2098627\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0058722373529609675\n[TRAIN MEMORY] Max memory allocated in epoch 68: 972.59 MB\nEpoch: 68 cost time: 3.505993604660034\nEpoch: 68, Steps: 17 | Train Loss: 0.2744358 Vali Loss: 0.0717628 Test Loss: 0.2111243\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.00582412869593569\n[TRAIN MEMORY] Max memory allocated in epoch 69: 972.59 MB\nEpoch: 69 cost time: 3.502326011657715\nEpoch: 69, Steps: 17 | Train Loss: 0.2747112 Vali Loss: 0.0723318 Test Loss: 0.2113210\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 3.49 seconds\n>>>>>>>testing : transformer_test_xPatch_custom_ftM_sl96_ll48_pl192_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 10348\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.81 MB\n[PARAMS] Total parameters: 211,453\nmse:0.2109474092721939, mae:0.24325022101402283\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data custom --learning_rate 0.005 --data_path weather.csv --seq_len 96 --pred_len 336 --enc_in 21 --batch_size 2048 --lradj 'sigmoid' --period_len 6 --d_model 128 --dropout 0.3 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T07:52:48.108925Z","iopub.execute_input":"2025-10-19T07:52:48.109212Z","iopub.status.idle":"2025-10-19T07:58:22.407746Z","shell.execute_reply.started":"2025-10-19T07:52:48.109185Z","shell.execute_reply":"2025-10-19T07:58:22.407041Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=6, d_model=128, data='custom', root_path='./dataset', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=336, enc_in=21, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.005, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.3, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_custom_ftM_sl96_ll48_pl336_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 36456\nval 4935\ntest 10204\n[TRAIN MEMORY] Max memory allocated in epoch 1: 1376.55 MB\nEpoch: 1 cost time: 5.445539712905884\nEpoch: 1, Steps: 17 | Train Loss: 0.3934490 Vali Loss: 0.0916382 Test Loss: 0.2588378\nValidation loss decreased (inf --> 0.091638).  Saving model ...\nUpdating learning rate to 1.9766777377990098e-05\n[TRAIN MEMORY] Max memory allocated in epoch 2: 1375.80 MB\nEpoch: 2 cost time: 4.658593654632568\nEpoch: 2, Steps: 17 | Train Loss: 0.3554760 Vali Loss: 0.0910016 Test Loss: 0.2579168\nValidation loss decreased (0.091638 --> 0.091002).  Saving model ...\nUpdating learning rate to 5.297334308904794e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 1375.80 MB\nEpoch: 3 cost time: 4.665334939956665\nEpoch: 3, Steps: 17 | Train Loss: 0.3516279 Vali Loss: 0.0900100 Test Loss: 0.2581656\nValidation loss decreased (0.091002 --> 0.090010).  Saving model ...\nUpdating learning rate to 0.00010772330336670662\n[TRAIN MEMORY] Max memory allocated in epoch 4: 1375.80 MB\nEpoch: 4 cost time: 4.607022285461426\nEpoch: 4, Steps: 17 | Train Loss: 0.3491753 Vali Loss: 0.0893159 Test Loss: 0.2568627\nValidation loss decreased (0.090010 --> 0.089316).  Saving model ...\nUpdating learning rate to 0.00019631651012203447\n[TRAIN MEMORY] Max memory allocated in epoch 5: 1375.80 MB\nEpoch: 5 cost time: 4.802812337875366\nEpoch: 5, Steps: 17 | Train Loss: 0.3476700 Vali Loss: 0.0891033 Test Loss: 0.2564611\nValidation loss decreased (0.089316 --> 0.089103).  Saving model ...\nUpdating learning rate to 0.00033640347303765786\n[TRAIN MEMORY] Max memory allocated in epoch 6: 1375.80 MB\nEpoch: 6 cost time: 4.691898822784424\nEpoch: 6, Steps: 17 | Train Loss: 0.3463980 Vali Loss: 0.0883837 Test Loss: 0.2555525\nValidation loss decreased (0.089103 --> 0.088384).  Saving model ...\nUpdating learning rate to 0.0005509481168463487\n[TRAIN MEMORY] Max memory allocated in epoch 7: 1375.80 MB\nEpoch: 7 cost time: 4.656135082244873\nEpoch: 7, Steps: 17 | Train Loss: 0.3450607 Vali Loss: 0.0877521 Test Loss: 0.2537640\nValidation loss decreased (0.088384 --> 0.087752).  Saving model ...\nUpdating learning rate to 0.0008647724011220512\n[TRAIN MEMORY] Max memory allocated in epoch 8: 1375.80 MB\nEpoch: 8 cost time: 4.699296474456787\nEpoch: 8, Steps: 17 | Train Loss: 0.3432878 Vali Loss: 0.0871666 Test Loss: 0.2522983\nValidation loss decreased (0.087752 --> 0.087167).  Saving model ...\nUpdating learning rate to 0.001294948097515454\n[TRAIN MEMORY] Max memory allocated in epoch 9: 1375.80 MB\nEpoch: 9 cost time: 4.764791488647461\nEpoch: 9, Steps: 17 | Train Loss: 0.3413332 Vali Loss: 0.0866055 Test Loss: 0.2499303\nValidation loss decreased (0.087167 --> 0.086605).  Saving model ...\nUpdating learning rate to 0.0018354198128311366\n[TRAIN MEMORY] Max memory allocated in epoch 10: 1375.80 MB\nEpoch: 10 cost time: 4.697956323623657\nEpoch: 10, Steps: 17 | Train Loss: 0.3393676 Vali Loss: 0.0857045 Test Loss: 0.2475341\nValidation loss decreased (0.086605 --> 0.085705).  Saving model ...\nUpdating learning rate to 0.002445065286847034\n[TRAIN MEMORY] Max memory allocated in epoch 11: 1375.80 MB\nEpoch: 11 cost time: 4.646849870681763\nEpoch: 11, Steps: 17 | Train Loss: 0.3376577 Vali Loss: 0.0851416 Test Loss: 0.2452263\nValidation loss decreased (0.085705 --> 0.085142).  Saving model ...\nUpdating learning rate to 0.0030545778935896616\n[TRAIN MEMORY] Max memory allocated in epoch 12: 1375.80 MB\nEpoch: 12 cost time: 4.687336444854736\nEpoch: 12, Steps: 17 | Train Loss: 0.3347984 Vali Loss: 0.0848829 Test Loss: 0.2421796\nValidation loss decreased (0.085142 --> 0.084883).  Saving model ...\nUpdating learning rate to 0.003594650718228653\n[TRAIN MEMORY] Max memory allocated in epoch 13: 1375.80 MB\nEpoch: 13 cost time: 4.778891324996948\nEpoch: 13, Steps: 17 | Train Loss: 0.3329202 Vali Loss: 0.0849294 Test Loss: 0.2418143\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00402416063364766\n[TRAIN MEMORY] Max memory allocated in epoch 14: 1375.80 MB\nEpoch: 14 cost time: 4.726681232452393\nEpoch: 14, Steps: 17 | Train Loss: 0.3303282 Vali Loss: 0.0841505 Test Loss: 0.2400422\nValidation loss decreased (0.084883 --> 0.084151).  Saving model ...\nUpdating learning rate to 0.004337050800751087\n[TRAIN MEMORY] Max memory allocated in epoch 15: 1375.80 MB\nEpoch: 15 cost time: 4.70568585395813\nEpoch: 15, Steps: 17 | Train Loss: 0.3288972 Vali Loss: 0.0838457 Test Loss: 0.2398239\nValidation loss decreased (0.084151 --> 0.083846).  Saving model ...\nUpdating learning rate to 0.0045503909646775545\n[TRAIN MEMORY] Max memory allocated in epoch 16: 1375.80 MB\nEpoch: 16 cost time: 4.719385385513306\nEpoch: 16, Steps: 17 | Train Loss: 0.3282015 Vali Loss: 0.0846213 Test Loss: 0.2391991\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.004689000475645802\n[TRAIN MEMORY] Max memory allocated in epoch 17: 1375.80 MB\nEpoch: 17 cost time: 4.643478631973267\nEpoch: 17, Steps: 17 | Train Loss: 0.3273110 Vali Loss: 0.0836739 Test Loss: 0.2393881\nValidation loss decreased (0.083846 --> 0.083674).  Saving model ...\nUpdating learning rate to 0.004775840063351173\n[TRAIN MEMORY] Max memory allocated in epoch 18: 1375.80 MB\nEpoch: 18 cost time: 4.648330926895142\nEpoch: 18, Steps: 17 | Train Loss: 0.3261404 Vali Loss: 0.0838329 Test Loss: 0.2380315\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.004828556453332337\n[TRAIN MEMORY] Max memory allocated in epoch 19: 1375.80 MB\nEpoch: 19 cost time: 4.677377700805664\nEpoch: 19, Steps: 17 | Train Loss: 0.3257021 Vali Loss: 0.0836316 Test Loss: 0.2389001\nValidation loss decreased (0.083674 --> 0.083632).  Saving model ...\nUpdating learning rate to 0.0048594451202683955\n[TRAIN MEMORY] Max memory allocated in epoch 20: 1375.80 MB\nEpoch: 20 cost time: 4.691478729248047\nEpoch: 20, Steps: 17 | Train Loss: 0.3241793 Vali Loss: 0.0828581 Test Loss: 0.2382552\nValidation loss decreased (0.083632 --> 0.082858).  Saving model ...\nUpdating learning rate to 0.004876604695568118\n[TRAIN MEMORY] Max memory allocated in epoch 21: 1375.80 MB\nEpoch: 21 cost time: 4.7409987449646\nEpoch: 21, Steps: 17 | Train Loss: 0.3239040 Vali Loss: 0.0833865 Test Loss: 0.2387030\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.004885194501735324\n[TRAIN MEMORY] Max memory allocated in epoch 22: 1375.80 MB\nEpoch: 22 cost time: 4.714990139007568\nEpoch: 22, Steps: 17 | Train Loss: 0.3224315 Vali Loss: 0.0825898 Test Loss: 0.2372608\nValidation loss decreased (0.082858 --> 0.082590).  Saving model ...\nUpdating learning rate to 0.0048884353555464395\n[TRAIN MEMORY] Max memory allocated in epoch 23: 1375.80 MB\nEpoch: 23 cost time: 4.716947317123413\nEpoch: 23, Steps: 17 | Train Loss: 0.3216133 Vali Loss: 0.0826229 Test Loss: 0.2367101\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.004888312366122913\n[TRAIN MEMORY] Max memory allocated in epoch 24: 1375.80 MB\nEpoch: 24 cost time: 4.790459394454956\nEpoch: 24, Steps: 17 | Train Loss: 0.3212307 Vali Loss: 0.0824074 Test Loss: 0.2374692\nValidation loss decreased (0.082590 --> 0.082407).  Saving model ...\nUpdating learning rate to 0.004886038389347345\n[TRAIN MEMORY] Max memory allocated in epoch 25: 1375.80 MB\nEpoch: 25 cost time: 4.744612693786621\nEpoch: 25, Steps: 17 | Train Loss: 0.3206086 Vali Loss: 0.0828998 Test Loss: 0.2378945\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0048823492572652545\n[TRAIN MEMORY] Max memory allocated in epoch 26: 1375.80 MB\nEpoch: 26 cost time: 4.664195537567139\nEpoch: 26, Steps: 17 | Train Loss: 0.3203292 Vali Loss: 0.0827325 Test Loss: 0.2365342\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.004877688142259322\n[TRAIN MEMORY] Max memory allocated in epoch 27: 1375.80 MB\nEpoch: 27 cost time: 4.740495204925537\nEpoch: 27, Steps: 17 | Train Loss: 0.3205186 Vali Loss: 0.0829535 Test Loss: 0.2362289\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.004872319348975366\n[TRAIN MEMORY] Max memory allocated in epoch 28: 1375.80 MB\nEpoch: 28 cost time: 4.694717884063721\nEpoch: 28, Steps: 17 | Train Loss: 0.3200890 Vali Loss: 0.0827331 Test Loss: 0.2371717\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.00486639805923574\n[TRAIN MEMORY] Max memory allocated in epoch 29: 1375.80 MB\nEpoch: 29 cost time: 4.724829196929932\nEpoch: 29, Steps: 17 | Train Loss: 0.3201018 Vali Loss: 0.0825978 Test Loss: 0.2370425\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.004860012899942082\n[TRAIN MEMORY] Max memory allocated in epoch 30: 1375.80 MB\nEpoch: 30 cost time: 4.704673528671265\nEpoch: 30, Steps: 17 | Train Loss: 0.3185409 Vali Loss: 0.0821069 Test Loss: 0.2363327\nValidation loss decreased (0.082407 --> 0.082107).  Saving model ...\nUpdating learning rate to 0.004853211856899706\n[TRAIN MEMORY] Max memory allocated in epoch 31: 1375.80 MB\nEpoch: 31 cost time: 4.6738786697387695\nEpoch: 31, Steps: 17 | Train Loss: 0.3191072 Vali Loss: 0.0826624 Test Loss: 0.2367204\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.004846018024758686\n[TRAIN MEMORY] Max memory allocated in epoch 32: 1375.80 MB\nEpoch: 32 cost time: 4.74843955039978\nEpoch: 32, Steps: 17 | Train Loss: 0.3189925 Vali Loss: 0.0820160 Test Loss: 0.2361710\nValidation loss decreased (0.082107 --> 0.082016).  Saving model ...\nUpdating learning rate to 0.004838439169398508\n[TRAIN MEMORY] Max memory allocated in epoch 33: 1375.80 MB\nEpoch: 33 cost time: 4.748191833496094\nEpoch: 33, Steps: 17 | Train Loss: 0.3178133 Vali Loss: 0.0825084 Test Loss: 0.2362725\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.004830473529249204\n[TRAIN MEMORY] Max memory allocated in epoch 34: 1375.80 MB\nEpoch: 34 cost time: 4.649903774261475\nEpoch: 34, Steps: 17 | Train Loss: 0.3183414 Vali Loss: 0.0818576 Test Loss: 0.2360473\nValidation loss decreased (0.082016 --> 0.081858).  Saving model ...\nUpdating learning rate to 0.004822113332763808\n[TRAIN MEMORY] Max memory allocated in epoch 35: 1375.80 MB\nEpoch: 35 cost time: 4.71538233757019\nEpoch: 35, Steps: 17 | Train Loss: 0.3177102 Vali Loss: 0.0825219 Test Loss: 0.2371320\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.004813346930082932\n[TRAIN MEMORY] Max memory allocated in epoch 36: 1375.80 MB\nEpoch: 36 cost time: 4.699994087219238\nEpoch: 36, Steps: 17 | Train Loss: 0.3178292 Vali Loss: 0.0821733 Test Loss: 0.2364338\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.004804160084394689\n[TRAIN MEMORY] Max memory allocated in epoch 37: 1375.80 MB\nEpoch: 37 cost time: 4.736499071121216\nEpoch: 37, Steps: 17 | Train Loss: 0.3170377 Vali Loss: 0.0821185 Test Loss: 0.2361442\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0047945367542116405\n[TRAIN MEMORY] Max memory allocated in epoch 38: 1375.80 MB\nEpoch: 38 cost time: 4.702181339263916\nEpoch: 38, Steps: 17 | Train Loss: 0.3160556 Vali Loss: 0.0825567 Test Loss: 0.2364331\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.00478445956765443\n[TRAIN MEMORY] Max memory allocated in epoch 39: 1375.80 MB\nEpoch: 39 cost time: 4.761743783950806\nEpoch: 39, Steps: 17 | Train Loss: 0.3168687 Vali Loss: 0.0823025 Test Loss: 0.2360040\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.004773910110846522\n[TRAIN MEMORY] Max memory allocated in epoch 40: 1375.80 MB\nEpoch: 40 cost time: 4.713432312011719\nEpoch: 40, Steps: 17 | Train Loss: 0.3157438 Vali Loss: 0.0822661 Test Loss: 0.2361605\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.004762869104601031\n[TRAIN MEMORY] Max memory allocated in epoch 41: 1375.80 MB\nEpoch: 41 cost time: 4.710219144821167\nEpoch: 41, Steps: 17 | Train Loss: 0.3155684 Vali Loss: 0.0824717 Test Loss: 0.2362771\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.004751316514511708\n[TRAIN MEMORY] Max memory allocated in epoch 42: 1375.80 MB\nEpoch: 42 cost time: 4.684413433074951\nEpoch: 42, Steps: 17 | Train Loss: 0.3150841 Vali Loss: 0.0820846 Test Loss: 0.2361931\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.004739231621932101\n[TRAIN MEMORY] Max memory allocated in epoch 43: 1375.80 MB\nEpoch: 43 cost time: 4.7084572315216064\nEpoch: 43, Steps: 17 | Train Loss: 0.3149937 Vali Loss: 0.0820501 Test Loss: 0.2355352\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.00472659307264015\n[TRAIN MEMORY] Max memory allocated in epoch 44: 1375.80 MB\nEpoch: 44 cost time: 4.7039711475372314\nEpoch: 44, Steps: 17 | Train Loss: 0.3153713 Vali Loss: 0.0822301 Test Loss: 0.2360174\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 4.72 seconds\n>>>>>>>testing : transformer_test_xPatch_custom_ftM_sl96_ll48_pl336_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 10204\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 1.82 MB\n[PARAMS] Total parameters: 474,517\nmse:0.23604725301265717, mae:0.2733359932899475\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data custom --learning_rate 0.001 --data_path weather.csv --seq_len 96 --pred_len 720 --enc_in 21 --batch_size 2048 --lradj 'sigmoid' --period_len 6 --d_model 128 --dropout 0.4 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T07:59:25.323344Z","iopub.execute_input":"2025-10-19T07:59:25.323648Z","iopub.status.idle":"2025-10-19T08:19:49.298178Z","shell.execute_reply.started":"2025-10-19T07:59:25.323620Z","shell.execute_reply":"2025-10-19T08:19:49.297490Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=6, d_model=128, data='custom', root_path='./dataset', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=720, enc_in=21, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=2048, patience=10, learning_rate=0.001, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.4, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_custom_ftM_sl96_ll48_pl720_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 36072\nval 4551\ntest 9820\n[TRAIN MEMORY] Max memory allocated in epoch 1: 2470.57 MB\nEpoch: 1 cost time: 8.831066370010376\nEpoch: 1, Steps: 17 | Train Loss: 0.4002302 Vali Loss: 0.1055455 Test Loss: 0.3361278\nValidation loss decreased (inf --> 0.105545).  Saving model ...\nUpdating learning rate to 3.95335547559802e-06\n[TRAIN MEMORY] Max memory allocated in epoch 2: 2470.57 MB\nEpoch: 2 cost time: 7.857110500335693\nEpoch: 2, Steps: 17 | Train Loss: 0.3833140 Vali Loss: 0.1051771 Test Loss: 0.3350897\nValidation loss decreased (0.105545 --> 0.105177).  Saving model ...\nUpdating learning rate to 1.059466861780959e-05\n[TRAIN MEMORY] Max memory allocated in epoch 3: 2470.57 MB\nEpoch: 3 cost time: 7.823410511016846\nEpoch: 3, Steps: 17 | Train Loss: 0.3820638 Vali Loss: 0.1050741 Test Loss: 0.3343091\nValidation loss decreased (0.105177 --> 0.105074).  Saving model ...\nUpdating learning rate to 2.1544660673341324e-05\n[TRAIN MEMORY] Max memory allocated in epoch 4: 2470.57 MB\nEpoch: 4 cost time: 7.857776641845703\nEpoch: 4, Steps: 17 | Train Loss: 0.3807719 Vali Loss: 0.1046365 Test Loss: 0.3338050\nValidation loss decreased (0.105074 --> 0.104637).  Saving model ...\nUpdating learning rate to 3.926330202440689e-05\n[TRAIN MEMORY] Max memory allocated in epoch 5: 2470.57 MB\nEpoch: 5 cost time: 7.810027122497559\nEpoch: 5, Steps: 17 | Train Loss: 0.3800701 Vali Loss: 0.1045261 Test Loss: 0.3334599\nValidation loss decreased (0.104637 --> 0.104526).  Saving model ...\nUpdating learning rate to 6.728069460753156e-05\n[TRAIN MEMORY] Max memory allocated in epoch 6: 2470.57 MB\nEpoch: 6 cost time: 7.924606084823608\nEpoch: 6, Steps: 17 | Train Loss: 0.3791767 Vali Loss: 0.1044131 Test Loss: 0.3322192\nValidation loss decreased (0.104526 --> 0.104413).  Saving model ...\nUpdating learning rate to 0.00011018962336926974\n[TRAIN MEMORY] Max memory allocated in epoch 7: 2470.57 MB\nEpoch: 7 cost time: 7.799994945526123\nEpoch: 7, Steps: 17 | Train Loss: 0.3781167 Vali Loss: 0.1038367 Test Loss: 0.3308760\nValidation loss decreased (0.104413 --> 0.103837).  Saving model ...\nUpdating learning rate to 0.00017295448022441027\n[TRAIN MEMORY] Max memory allocated in epoch 8: 2470.57 MB\nEpoch: 8 cost time: 7.790163040161133\nEpoch: 8, Steps: 17 | Train Loss: 0.3767947 Vali Loss: 0.1034732 Test Loss: 0.3287588\nValidation loss decreased (0.103837 --> 0.103473).  Saving model ...\nUpdating learning rate to 0.0002589896195030908\n[TRAIN MEMORY] Max memory allocated in epoch 9: 2470.57 MB\nEpoch: 9 cost time: 7.876847743988037\nEpoch: 9, Steps: 17 | Train Loss: 0.3751775 Vali Loss: 0.1029393 Test Loss: 0.3255707\nValidation loss decreased (0.103473 --> 0.102939).  Saving model ...\nUpdating learning rate to 0.0003670839625662274\n[TRAIN MEMORY] Max memory allocated in epoch 10: 2470.57 MB\nEpoch: 10 cost time: 8.012170314788818\nEpoch: 10, Steps: 17 | Train Loss: 0.3738258 Vali Loss: 0.1025171 Test Loss: 0.3234271\nValidation loss decreased (0.102939 --> 0.102517).  Saving model ...\nUpdating learning rate to 0.0004890130573694068\n[TRAIN MEMORY] Max memory allocated in epoch 11: 2470.57 MB\nEpoch: 11 cost time: 7.879045009613037\nEpoch: 11, Steps: 17 | Train Loss: 0.3733814 Vali Loss: 0.1026265 Test Loss: 0.3225781\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0006109155787179323\n[TRAIN MEMORY] Max memory allocated in epoch 12: 2470.57 MB\nEpoch: 12 cost time: 7.878396987915039\nEpoch: 12, Steps: 17 | Train Loss: 0.3721365 Vali Loss: 0.1020558 Test Loss: 0.3203624\nValidation loss decreased (0.102517 --> 0.102056).  Saving model ...\nUpdating learning rate to 0.0007189301436457306\n[TRAIN MEMORY] Max memory allocated in epoch 13: 2470.57 MB\nEpoch: 13 cost time: 7.850528717041016\nEpoch: 13, Steps: 17 | Train Loss: 0.3718992 Vali Loss: 0.1022551 Test Loss: 0.3205823\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.000804832126729532\n[TRAIN MEMORY] Max memory allocated in epoch 14: 2470.57 MB\nEpoch: 14 cost time: 7.832098960876465\nEpoch: 14, Steps: 17 | Train Loss: 0.3707586 Vali Loss: 0.1019857 Test Loss: 0.3206873\nValidation loss decreased (0.102056 --> 0.101986).  Saving model ...\nUpdating learning rate to 0.0008674101601502176\n[TRAIN MEMORY] Max memory allocated in epoch 15: 2470.57 MB\nEpoch: 15 cost time: 7.862362861633301\nEpoch: 15, Steps: 17 | Train Loss: 0.3701732 Vali Loss: 0.1021858 Test Loss: 0.3198924\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.000910078192935511\n[TRAIN MEMORY] Max memory allocated in epoch 16: 2470.57 MB\nEpoch: 16 cost time: 7.815749168395996\nEpoch: 16, Steps: 17 | Train Loss: 0.3698433 Vali Loss: 0.1013950 Test Loss: 0.3178975\nValidation loss decreased (0.101986 --> 0.101395).  Saving model ...\nUpdating learning rate to 0.0009378000951291602\n[TRAIN MEMORY] Max memory allocated in epoch 17: 2470.57 MB\nEpoch: 17 cost time: 7.910191297531128\nEpoch: 17, Steps: 17 | Train Loss: 0.3692340 Vali Loss: 0.1012717 Test Loss: 0.3183673\nValidation loss decreased (0.101395 --> 0.101272).  Saving model ...\nUpdating learning rate to 0.0009551680126702348\n[TRAIN MEMORY] Max memory allocated in epoch 18: 2470.57 MB\nEpoch: 18 cost time: 7.850696325302124\nEpoch: 18, Steps: 17 | Train Loss: 0.3683245 Vali Loss: 0.1014678 Test Loss: 0.3184110\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009657112906664675\n[TRAIN MEMORY] Max memory allocated in epoch 19: 2470.57 MB\nEpoch: 19 cost time: 7.832660436630249\nEpoch: 19, Steps: 17 | Train Loss: 0.3676574 Vali Loss: 0.1013384 Test Loss: 0.3171564\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0009718890240536792\n[TRAIN MEMORY] Max memory allocated in epoch 20: 2470.57 MB\nEpoch: 20 cost time: 7.871237754821777\nEpoch: 20, Steps: 17 | Train Loss: 0.3672580 Vali Loss: 0.1013245 Test Loss: 0.3169642\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0009753209391136237\n[TRAIN MEMORY] Max memory allocated in epoch 21: 2470.57 MB\nEpoch: 21 cost time: 7.812939405441284\nEpoch: 21, Steps: 17 | Train Loss: 0.3667064 Vali Loss: 0.1009294 Test Loss: 0.3161930\nValidation loss decreased (0.101272 --> 0.100929).  Saving model ...\nUpdating learning rate to 0.0009770389003470649\n[TRAIN MEMORY] Max memory allocated in epoch 22: 2470.57 MB\nEpoch: 22 cost time: 7.796604633331299\nEpoch: 22, Steps: 17 | Train Loss: 0.3665832 Vali Loss: 0.1009227 Test Loss: 0.3149355\nValidation loss decreased (0.100929 --> 0.100923).  Saving model ...\nUpdating learning rate to 0.0009776870711092878\n[TRAIN MEMORY] Max memory allocated in epoch 23: 2470.57 MB\nEpoch: 23 cost time: 7.878406763076782\nEpoch: 23, Steps: 17 | Train Loss: 0.3660887 Vali Loss: 0.1009739 Test Loss: 0.3152495\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009776624732245826\n[TRAIN MEMORY] Max memory allocated in epoch 24: 2470.57 MB\nEpoch: 24 cost time: 7.942118406295776\nEpoch: 24, Steps: 17 | Train Loss: 0.3653376 Vali Loss: 0.1008653 Test Loss: 0.3150965\nValidation loss decreased (0.100923 --> 0.100865).  Saving model ...\nUpdating learning rate to 0.000977207677869469\n[TRAIN MEMORY] Max memory allocated in epoch 25: 2470.57 MB\nEpoch: 25 cost time: 7.849926233291626\nEpoch: 25, Steps: 17 | Train Loss: 0.3650939 Vali Loss: 0.1007125 Test Loss: 0.3146020\nValidation loss decreased (0.100865 --> 0.100713).  Saving model ...\nUpdating learning rate to 0.0009764698514530507\n[TRAIN MEMORY] Max memory allocated in epoch 26: 2470.57 MB\nEpoch: 26 cost time: 7.893754005432129\nEpoch: 26, Steps: 17 | Train Loss: 0.3647143 Vali Loss: 0.1003435 Test Loss: 0.3141278\nValidation loss decreased (0.100713 --> 0.100343).  Saving model ...\nUpdating learning rate to 0.0009755376284518644\n[TRAIN MEMORY] Max memory allocated in epoch 27: 2470.57 MB\nEpoch: 27 cost time: 7.904714584350586\nEpoch: 27, Steps: 17 | Train Loss: 0.3642835 Vali Loss: 0.1003173 Test Loss: 0.3139322\nValidation loss decreased (0.100343 --> 0.100317).  Saving model ...\nUpdating learning rate to 0.0009744638697950732\n[TRAIN MEMORY] Max memory allocated in epoch 28: 2470.57 MB\nEpoch: 28 cost time: 7.853476524353027\nEpoch: 28, Steps: 17 | Train Loss: 0.3639784 Vali Loss: 0.1004243 Test Loss: 0.3136768\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009732796118471478\n[TRAIN MEMORY] Max memory allocated in epoch 29: 2470.57 MB\nEpoch: 29 cost time: 7.892375230789185\nEpoch: 29, Steps: 17 | Train Loss: 0.3638612 Vali Loss: 0.1004668 Test Loss: 0.3136233\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0009720025799884166\n[TRAIN MEMORY] Max memory allocated in epoch 30: 2470.57 MB\nEpoch: 30 cost time: 7.915320158004761\nEpoch: 30, Steps: 17 | Train Loss: 0.3634497 Vali Loss: 0.1002110 Test Loss: 0.3131329\nValidation loss decreased (0.100317 --> 0.100211).  Saving model ...\nUpdating learning rate to 0.0009706423713799413\n[TRAIN MEMORY] Max memory allocated in epoch 31: 2470.57 MB\nEpoch: 31 cost time: 7.88399863243103\nEpoch: 31, Steps: 17 | Train Loss: 0.3630883 Vali Loss: 0.1002237 Test Loss: 0.3126361\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009692036049517374\n[TRAIN MEMORY] Max memory allocated in epoch 32: 2470.57 MB\nEpoch: 32 cost time: 7.943671464920044\nEpoch: 32, Steps: 17 | Train Loss: 0.3626869 Vali Loss: 0.1000714 Test Loss: 0.3123782\nValidation loss decreased (0.100211 --> 0.100071).  Saving model ...\nUpdating learning rate to 0.0009676878338797016\n[TRAIN MEMORY] Max memory allocated in epoch 33: 2470.57 MB\nEpoch: 33 cost time: 7.894447088241577\nEpoch: 33, Steps: 17 | Train Loss: 0.3623480 Vali Loss: 0.1001648 Test Loss: 0.3128012\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009660947058498411\n[TRAIN MEMORY] Max memory allocated in epoch 34: 2470.57 MB\nEpoch: 34 cost time: 7.91627311706543\nEpoch: 34, Steps: 17 | Train Loss: 0.3617707 Vali Loss: 0.0998774 Test Loss: 0.3117242\nValidation loss decreased (0.100071 --> 0.099877).  Saving model ...\nUpdating learning rate to 0.0009644226665527616\n[TRAIN MEMORY] Max memory allocated in epoch 35: 2470.57 MB\nEpoch: 35 cost time: 7.931992530822754\nEpoch: 35, Steps: 17 | Train Loss: 0.3616785 Vali Loss: 0.0999911 Test Loss: 0.3118520\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009626693860165865\n[TRAIN MEMORY] Max memory allocated in epoch 36: 2470.57 MB\nEpoch: 36 cost time: 7.890029430389404\nEpoch: 36, Steps: 17 | Train Loss: 0.3617926 Vali Loss: 0.1000967 Test Loss: 0.3115104\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0009608320168789377\n[TRAIN MEMORY] Max memory allocated in epoch 37: 2470.57 MB\nEpoch: 37 cost time: 7.8323585987091064\nEpoch: 37, Steps: 17 | Train Loss: 0.3616476 Vali Loss: 0.0998173 Test Loss: 0.3117636\nValidation loss decreased (0.099877 --> 0.099817).  Saving model ...\nUpdating learning rate to 0.0009589073508423281\n[TRAIN MEMORY] Max memory allocated in epoch 38: 2470.57 MB\nEpoch: 38 cost time: 7.885080337524414\nEpoch: 38, Steps: 17 | Train Loss: 0.3609464 Vali Loss: 0.0998687 Test Loss: 0.3112350\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.000956891913530886\n[TRAIN MEMORY] Max memory allocated in epoch 39: 2470.57 MB\nEpoch: 39 cost time: 7.953037977218628\nEpoch: 39, Steps: 17 | Train Loss: 0.3607787 Vali Loss: 0.0997709 Test Loss: 0.3114224\nValidation loss decreased (0.099817 --> 0.099771).  Saving model ...\nUpdating learning rate to 0.0009547820221693043\n[TRAIN MEMORY] Max memory allocated in epoch 40: 2470.57 MB\nEpoch: 40 cost time: 7.812126636505127\nEpoch: 40, Steps: 17 | Train Loss: 0.3604462 Vali Loss: 0.0997323 Test Loss: 0.3110691\nValidation loss decreased (0.099771 --> 0.099732).  Saving model ...\nUpdating learning rate to 0.0009525738209202063\n[TRAIN MEMORY] Max memory allocated in epoch 41: 2470.57 MB\nEpoch: 41 cost time: 7.924843072891235\nEpoch: 41, Steps: 17 | Train Loss: 0.3600666 Vali Loss: 0.0998355 Test Loss: 0.3109668\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009502633029023415\n[TRAIN MEMORY] Max memory allocated in epoch 42: 2470.57 MB\nEpoch: 42 cost time: 7.854693174362183\nEpoch: 42, Steps: 17 | Train Loss: 0.3600975 Vali Loss: 0.0997297 Test Loss: 0.3103542\nValidation loss decreased (0.099732 --> 0.099730).  Saving model ...\nUpdating learning rate to 0.0009478463243864201\n[TRAIN MEMORY] Max memory allocated in epoch 43: 2470.57 MB\nEpoch: 43 cost time: 7.837930202484131\nEpoch: 43, Steps: 17 | Train Loss: 0.3597340 Vali Loss: 0.0997129 Test Loss: 0.3104275\nValidation loss decreased (0.099730 --> 0.099713).  Saving model ...\nUpdating learning rate to 0.0009453186145280301\n[TRAIN MEMORY] Max memory allocated in epoch 44: 2470.57 MB\nEpoch: 44 cost time: 8.012480974197388\nEpoch: 44, Steps: 17 | Train Loss: 0.3596834 Vali Loss: 0.0994992 Test Loss: 0.3103986\nValidation loss decreased (0.099713 --> 0.099499).  Saving model ...\nUpdating learning rate to 0.0009426757827017559\n[TRAIN MEMORY] Max memory allocated in epoch 45: 2470.57 MB\nEpoch: 45 cost time: 7.957553863525391\nEpoch: 45, Steps: 17 | Train Loss: 0.3594567 Vali Loss: 0.0996597 Test Loss: 0.3107838\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009399133247160015\n[TRAIN MEMORY] Max memory allocated in epoch 46: 2470.57 MB\nEpoch: 46 cost time: 7.909112930297852\nEpoch: 46, Steps: 17 | Train Loss: 0.3591402 Vali Loss: 0.0995117 Test Loss: 0.3104476\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0009370266287130241\n[TRAIN MEMORY] Max memory allocated in epoch 47: 2470.57 MB\nEpoch: 47 cost time: 7.939417123794556\nEpoch: 47, Steps: 17 | Train Loss: 0.3587339 Vali Loss: 0.0996694 Test Loss: 0.3100599\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0009340109812713316\n[TRAIN MEMORY] Max memory allocated in epoch 48: 2470.57 MB\nEpoch: 48 cost time: 7.8728249073028564\nEpoch: 48, Steps: 17 | Train Loss: 0.3586597 Vali Loss: 0.0992804 Test Loss: 0.3102471\nValidation loss decreased (0.099499 --> 0.099280).  Saving model ...\nUpdating learning rate to 0.0009308615740538568\n[TRAIN MEMORY] Max memory allocated in epoch 49: 2470.57 MB\nEpoch: 49 cost time: 7.93430757522583\nEpoch: 49, Steps: 17 | Train Loss: 0.3586354 Vali Loss: 0.0991577 Test Loss: 0.3099030\nValidation loss decreased (0.099280 --> 0.099158).  Saving model ...\nUpdating learning rate to 0.0009275735112402146\n[TRAIN MEMORY] Max memory allocated in epoch 50: 2470.57 MB\nEpoch: 50 cost time: 7.9444215297698975\nEpoch: 50, Steps: 17 | Train Loss: 0.3586159 Vali Loss: 0.0993703 Test Loss: 0.3103460\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009241418179176027\n[TRAIN MEMORY] Max memory allocated in epoch 51: 2470.57 MB\nEpoch: 51 cost time: 7.90345311164856\nEpoch: 51, Steps: 17 | Train Loss: 0.3582661 Vali Loss: 0.0998255 Test Loss: 0.3100689\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0009205614495658687\n[TRAIN MEMORY] Max memory allocated in epoch 52: 2470.57 MB\nEpoch: 52 cost time: 7.911715507507324\nEpoch: 52, Steps: 17 | Train Loss: 0.3581006 Vali Loss: 0.0993443 Test Loss: 0.3101815\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0009168273027478215\n[TRAIN MEMORY] Max memory allocated in epoch 53: 2470.57 MB\nEpoch: 53 cost time: 7.834183692932129\nEpoch: 53, Steps: 17 | Train Loss: 0.3577624 Vali Loss: 0.0991041 Test Loss: 0.3098910\nValidation loss decreased (0.099158 --> 0.099104).  Saving model ...\nUpdating learning rate to 0.0009129342270998232\n[TRAIN MEMORY] Max memory allocated in epoch 54: 2470.57 MB\nEpoch: 54 cost time: 7.932852029800415\nEpoch: 54, Steps: 17 | Train Loss: 0.3575194 Vali Loss: 0.0992469 Test Loss: 0.3097808\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009088770387061971\n[TRAIN MEMORY] Max memory allocated in epoch 55: 2470.57 MB\nEpoch: 55 cost time: 7.845205545425415\nEpoch: 55, Steps: 17 | Train Loss: 0.3573924 Vali Loss: 0.0991987 Test Loss: 0.3097822\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0009046505349317008\n[TRAIN MEMORY] Max memory allocated in epoch 56: 2470.57 MB\nEpoch: 56 cost time: 7.918546438217163\nEpoch: 56, Steps: 17 | Train Loss: 0.3573522 Vali Loss: 0.0991142 Test Loss: 0.3093219\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0009002495107776961\n[TRAIN MEMORY] Max memory allocated in epoch 57: 2470.57 MB\nEpoch: 57 cost time: 7.835029363632202\nEpoch: 57, Steps: 17 | Train Loss: 0.3572604 Vali Loss: 0.0991019 Test Loss: 0.3096318\nValidation loss decreased (0.099104 --> 0.099102).  Saving model ...\nUpdating learning rate to 0.0008956687768187573\n[TRAIN MEMORY] Max memory allocated in epoch 58: 2470.57 MB\nEpoch: 58 cost time: 7.927054166793823\nEpoch: 58, Steps: 17 | Train Loss: 0.3570448 Vali Loss: 0.0991586 Test Loss: 0.3095994\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008909031787666358\n[TRAIN MEMORY] Max memory allocated in epoch 59: 2470.57 MB\nEpoch: 59 cost time: 7.955727577209473\nEpoch: 59, Steps: 17 | Train Loss: 0.3568071 Vali Loss: 0.0991088 Test Loss: 0.3094499\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0008859476186973119\n[TRAIN MEMORY] Max memory allocated in epoch 60: 2470.57 MB\nEpoch: 60 cost time: 7.9950644969940186\nEpoch: 60, Steps: 17 | Train Loss: 0.3568287 Vali Loss: 0.0991830 Test Loss: 0.3091745\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0008807970779639945\n[TRAIN MEMORY] Max memory allocated in epoch 61: 2470.57 MB\nEpoch: 61 cost time: 7.847813844680786\nEpoch: 61, Steps: 17 | Train Loss: 0.3565826 Vali Loss: 0.0988959 Test Loss: 0.3097475\nValidation loss decreased (0.099102 --> 0.098896).  Saving model ...\nUpdating learning rate to 0.0008754466418041603\n[TRAIN MEMORY] Max memory allocated in epoch 62: 2470.57 MB\nEpoch: 62 cost time: 7.838252305984497\nEpoch: 62, Steps: 17 | Train Loss: 0.3564503 Vali Loss: 0.0989922 Test Loss: 0.3096817\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008698915256318932\n[TRAIN MEMORY] Max memory allocated in epoch 63: 2470.57 MB\nEpoch: 63 cost time: 7.84149694442749\nEpoch: 63, Steps: 17 | Train Loss: 0.3562805 Vali Loss: 0.0988736 Test Loss: 0.3090743\nValidation loss decreased (0.098896 --> 0.098874).  Saving model ...\nUpdating learning rate to 0.0008641271029878069\n[TRAIN MEMORY] Max memory allocated in epoch 64: 2470.57 MB\nEpoch: 64 cost time: 7.923806428909302\nEpoch: 64, Steps: 17 | Train Loss: 0.3559257 Vali Loss: 0.0989422 Test Loss: 0.3090534\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008581489350976327\n[TRAIN MEMORY] Max memory allocated in epoch 65: 2470.57 MB\nEpoch: 65 cost time: 7.921257734298706\nEpoch: 65, Steps: 17 | Train Loss: 0.3561757 Vali Loss: 0.0989993 Test Loss: 0.3090539\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0008519528019671706\n[TRAIN MEMORY] Max memory allocated in epoch 66: 2470.57 MB\nEpoch: 66 cost time: 7.829864978790283\nEpoch: 66, Steps: 17 | Train Loss: 0.3557638 Vali Loss: 0.0988838 Test Loss: 0.3092987\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0008455347349157739\n[TRAIN MEMORY] Max memory allocated in epoch 67: 2470.57 MB\nEpoch: 67 cost time: 7.842340707778931\nEpoch: 67, Steps: 17 | Train Loss: 0.3556521 Vali Loss: 0.0988148 Test Loss: 0.3094596\nValidation loss decreased (0.098874 --> 0.098815).  Saving model ...\nUpdating learning rate to 0.0008388910504229955\n[TRAIN MEMORY] Max memory allocated in epoch 68: 2470.57 MB\nEpoch: 68 cost time: 7.9953131675720215\nEpoch: 68, Steps: 17 | Train Loss: 0.3558045 Vali Loss: 0.0990255 Test Loss: 0.3093424\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008320183851336699\n[TRAIN MEMORY] Max memory allocated in epoch 69: 2470.57 MB\nEpoch: 69 cost time: 8.008517742156982\nEpoch: 69, Steps: 17 | Train Loss: 0.3557019 Vali Loss: 0.0987854 Test Loss: 0.3091110\nValidation loss decreased (0.098815 --> 0.098785).  Saving model ...\nUpdating learning rate to 0.0008249137318358059\n[TRAIN MEMORY] Max memory allocated in epoch 70: 2470.57 MB\nEpoch: 70 cost time: 7.882352352142334\nEpoch: 70, Steps: 17 | Train Loss: 0.3551773 Vali Loss: 0.0988391 Test Loss: 0.3091022\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008175744761935503\n[TRAIN MEMORY] Max memory allocated in epoch 71: 2470.57 MB\nEpoch: 71 cost time: 7.887305498123169\nEpoch: 71, Steps: 17 | Train Loss: 0.3553764 Vali Loss: 0.0987534 Test Loss: 0.3085352\nValidation loss decreased (0.098785 --> 0.098753).  Saving model ...\nUpdating learning rate to 0.0008099984339846303\n[TRAIN MEMORY] Max memory allocated in epoch 72: 2470.57 MB\nEpoch: 72 cost time: 7.95203971862793\nEpoch: 72, Steps: 17 | Train Loss: 0.3552900 Vali Loss: 0.0988015 Test Loss: 0.3091133\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0008021838885585473\n[TRAIN MEMORY] Max memory allocated in epoch 73: 2470.57 MB\nEpoch: 73 cost time: 7.970579147338867\nEpoch: 73, Steps: 17 | Train Loss: 0.3550437 Vali Loss: 0.0987277 Test Loss: 0.3089708\nValidation loss decreased (0.098753 --> 0.098728).  Saving model ...\nUpdating learning rate to 0.0007941296281990318\n[TRAIN MEMORY] Max memory allocated in epoch 74: 2470.57 MB\nEpoch: 74 cost time: 8.080093383789062\nEpoch: 74, Steps: 17 | Train Loss: 0.3553267 Vali Loss: 0.0985632 Test Loss: 0.3088313\nValidation loss decreased (0.098728 --> 0.098563).  Saving model ...\nUpdating learning rate to 0.000785834983042546\n[TRAIN MEMORY] Max memory allocated in epoch 75: 2470.57 MB\nEpoch: 75 cost time: 7.948101282119751\nEpoch: 75, Steps: 17 | Train Loss: 0.3549745 Vali Loss: 0.0988088 Test Loss: 0.3086477\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0007772998611746834\n[TRAIN MEMORY] Max memory allocated in epoch 76: 2470.57 MB\nEpoch: 76 cost time: 7.934367895126343\nEpoch: 76, Steps: 17 | Train Loss: 0.3550914 Vali Loss: 0.0986020 Test Loss: 0.3091967\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0007685247834990129\n[TRAIN MEMORY] Max memory allocated in epoch 77: 2470.57 MB\nEpoch: 77 cost time: 7.890356779098511\nEpoch: 77, Steps: 17 | Train Loss: 0.3548306 Vali Loss: 0.0987222 Test Loss: 0.3087230\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0007595109169491084\n[TRAIN MEMORY] Max memory allocated in epoch 78: 2470.57 MB\nEpoch: 78 cost time: 7.95972752571106\nEpoch: 78, Steps: 17 | Train Loss: 0.3546713 Vali Loss: 0.0986248 Test Loss: 0.3087703\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.000750260105595116\n[TRAIN MEMORY] Max memory allocated in epoch 79: 2470.57 MB\nEpoch: 79 cost time: 7.9238245487213135\nEpoch: 79, Steps: 17 | Train Loss: 0.3545102 Vali Loss: 0.0986543 Test Loss: 0.3088358\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0007407748991821529\n[TRAIN MEMORY] Max memory allocated in epoch 80: 2470.57 MB\nEpoch: 80 cost time: 7.933051586151123\nEpoch: 80, Steps: 17 | Train Loss: 0.3544048 Vali Loss: 0.0985369 Test Loss: 0.3083668\nValidation loss decreased (0.098563 --> 0.098537).  Saving model ...\nUpdating learning rate to 0.0007310585786300043\n[TRAIN MEMORY] Max memory allocated in epoch 81: 2470.57 MB\nEpoch: 81 cost time: 7.878793954849243\nEpoch: 81, Steps: 17 | Train Loss: 0.3542855 Vali Loss: 0.0986014 Test Loss: 0.3087520\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0007211151780228628\n[TRAIN MEMORY] Max memory allocated in epoch 82: 2470.57 MB\nEpoch: 82 cost time: 8.030537843704224\nEpoch: 82, Steps: 17 | Train Loss: 0.3540305 Vali Loss: 0.0983019 Test Loss: 0.3085586\nValidation loss decreased (0.098537 --> 0.098302).  Saving model ...\nUpdating learning rate to 0.0007109495026250038\n[TRAIN MEMORY] Max memory allocated in epoch 83: 2470.57 MB\nEpoch: 83 cost time: 7.940736770629883\nEpoch: 83, Steps: 17 | Train Loss: 0.3541095 Vali Loss: 0.0983992 Test Loss: 0.3087825\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0007005671424739728\n[TRAIN MEMORY] Max memory allocated in epoch 84: 2470.57 MB\nEpoch: 84 cost time: 7.983998537063599\nEpoch: 84, Steps: 17 | Train Loss: 0.3539970 Vali Loss: 0.0985774 Test Loss: 0.3084694\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0006899744811276125\n[TRAIN MEMORY] Max memory allocated in epoch 85: 2470.57 MB\nEpoch: 85 cost time: 8.008869409561157\nEpoch: 85, Steps: 17 | Train Loss: 0.3539748 Vali Loss: 0.0985685 Test Loss: 0.3084546\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.000679178699175393\n[TRAIN MEMORY] Max memory allocated in epoch 86: 2470.57 MB\nEpoch: 86 cost time: 7.843147277832031\nEpoch: 86, Steps: 17 | Train Loss: 0.3538853 Vali Loss: 0.0985371 Test Loss: 0.3085122\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0006681877721681662\n[TRAIN MEMORY] Max memory allocated in epoch 87: 2470.57 MB\nEpoch: 87 cost time: 7.909883499145508\nEpoch: 87, Steps: 17 | Train Loss: 0.3540721 Vali Loss: 0.0986669 Test Loss: 0.3084541\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0006570104626734989\n[TRAIN MEMORY] Max memory allocated in epoch 88: 2470.57 MB\nEpoch: 88 cost time: 7.967179536819458\nEpoch: 88, Steps: 17 | Train Loss: 0.3537160 Vali Loss: 0.0982882 Test Loss: 0.3082382\nValidation loss decreased (0.098302 --> 0.098288).  Saving model ...\nUpdating learning rate to 0.0006456563062257955\n[TRAIN MEMORY] Max memory allocated in epoch 89: 2470.57 MB\nEpoch: 89 cost time: 7.99457573890686\nEpoch: 89, Steps: 17 | Train Loss: 0.3534319 Vali Loss: 0.0984121 Test Loss: 0.3081347\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0006341355910108007\n[TRAIN MEMORY] Max memory allocated in epoch 90: 2470.57 MB\nEpoch: 90 cost time: 7.86722469329834\nEpoch: 90, Steps: 17 | Train Loss: 0.3535997 Vali Loss: 0.0984449 Test Loss: 0.3083829\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0006224593312018546\n[TRAIN MEMORY] Max memory allocated in epoch 91: 2470.57 MB\nEpoch: 91 cost time: 7.918420791625977\nEpoch: 91, Steps: 17 | Train Loss: 0.3535933 Vali Loss: 0.0985179 Test Loss: 0.3081996\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.000610639233949222\n[TRAIN MEMORY] Max memory allocated in epoch 92: 2470.57 MB\nEpoch: 92 cost time: 7.893377304077148\nEpoch: 92, Steps: 17 | Train Loss: 0.3534781 Vali Loss: 0.0983837 Test Loss: 0.3084942\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.000598687660112452\n[TRAIN MEMORY] Max memory allocated in epoch 93: 2470.57 MB\nEpoch: 93 cost time: 8.036581039428711\nEpoch: 93, Steps: 17 | Train Loss: 0.3535207 Vali Loss: 0.0983863 Test Loss: 0.3081469\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0005866175789173301\n[TRAIN MEMORY] Max memory allocated in epoch 94: 2470.57 MB\nEpoch: 94 cost time: 7.928886890411377\nEpoch: 94, Steps: 17 | Train Loss: 0.3533691 Vali Loss: 0.0985420 Test Loss: 0.3083210\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.000574442516811659\n[TRAIN MEMORY] Max memory allocated in epoch 95: 2470.57 MB\nEpoch: 95 cost time: 7.985290765762329\nEpoch: 95, Steps: 17 | Train Loss: 0.3532550 Vali Loss: 0.0983103 Test Loss: 0.3083821\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0005621765008857981\n[TRAIN MEMORY] Max memory allocated in epoch 96: 2470.57 MB\nEpoch: 96 cost time: 7.902218818664551\nEpoch: 96, Steps: 17 | Train Loss: 0.3532836 Vali Loss: 0.0983085 Test Loss: 0.3085856\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0005498339973124778\n[TRAIN MEMORY] Max memory allocated in epoch 97: 2470.57 MB\nEpoch: 97 cost time: 7.9320714473724365\nEpoch: 97, Steps: 17 | Train Loss: 0.3529312 Vali Loss: 0.0983614 Test Loss: 0.3083541\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.0005374298453437495\n[TRAIN MEMORY] Max memory allocated in epoch 98: 2470.57 MB\nEpoch: 98 cost time: 7.988547325134277\nEpoch: 98, Steps: 17 | Train Loss: 0.3529956 Vali Loss: 0.0983326 Test Loss: 0.3080885\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 7.91 seconds\n>>>>>>>testing : transformer_test_xPatch_custom_ftM_sl96_ll48_pl720_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 9820\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 6.81 MB\n[PARAMS] Total parameters: 1,784,277\nmse:0.3082381784915924, mae:0.32069718837738037\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data Solar --learning_rate 0.002 --data_path solar.txt --seq_len 720 --pred_len 96 --enc_in 137 --dropout 0.3 --batch_size 128 --lradj 'sigmoid' --period_len 6 --d_model 128 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data Solar --learning_rate 0.002 --data_path solar.txt --seq_len 720 --pred_len 192 --enc_in 137 --dropout 0 --batch_size 128 --lradj 'sigmoid' --period_len 6 --d_model 128 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T05:48:56.758131Z","iopub.execute_input":"2025-10-20T05:48:56.758435Z","iopub.status.idle":"2025-10-20T06:06:46.143568Z","shell.execute_reply.started":"2025-10-20T05:48:56.758408Z","shell.execute_reply":"2025-10-20T06:06:46.142885Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=6, d_model=128, data='Solar', root_path='./dataset', data_path='solar.txt', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=720, label_len=48, pred_len=192, enc_in=137, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.002, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.0, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_Solar_ftM_sl720_ll48_pl192_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 35881\nval 5065\ntest 10321\n\titers: 100, epoch: 1 | loss: 0.2019655\n\tspeed: 0.1744s/iter; left time: 4864.6370s\n\titers: 200, epoch: 1 | loss: 0.1934783\n\tspeed: 0.1433s/iter; left time: 3985.1367s\n[TRAIN MEMORY] Max memory allocated in epoch 1: 638.32 MB\nEpoch: 1 cost time: 41.67357587814331\nEpoch: 1, Steps: 280 | Train Loss: 0.2083975 Vali Loss: 0.0477488 Test Loss: 0.1979711\nValidation loss decreased (inf --> 0.047749).  Saving model ...\nUpdating learning rate to 7.90671095119604e-06\n\titers: 100, epoch: 2 | loss: 0.1873657\n\tspeed: 0.4285s/iter; left time: 11835.3666s\n\titers: 200, epoch: 2 | loss: 0.1743569\n\tspeed: 0.1156s/iter; left time: 3182.3944s\n[TRAIN MEMORY] Max memory allocated in epoch 2: 638.32 MB\nEpoch: 2 cost time: 33.992069482803345\nEpoch: 2, Steps: 280 | Train Loss: 0.1794962 Vali Loss: 0.0447485 Test Loss: 0.1948878\nValidation loss decreased (0.047749 --> 0.044748).  Saving model ...\nUpdating learning rate to 2.118933723561918e-05\n\titers: 100, epoch: 3 | loss: 0.1868203\n\tspeed: 0.4137s/iter; left time: 11311.6447s\n\titers: 200, epoch: 3 | loss: 0.1739246\n\tspeed: 0.1183s/iter; left time: 3221.8090s\n[TRAIN MEMORY] Max memory allocated in epoch 3: 638.32 MB\nEpoch: 3 cost time: 34.340951919555664\nEpoch: 3, Steps: 280 | Train Loss: 0.1777645 Vali Loss: 0.0444895 Test Loss: 0.1937302\nValidation loss decreased (0.044748 --> 0.044490).  Saving model ...\nUpdating learning rate to 4.308932134668265e-05\n\titers: 100, epoch: 4 | loss: 0.1786848\n\tspeed: 0.4155s/iter; left time: 11244.4553s\n\titers: 200, epoch: 4 | loss: 0.1728766\n\tspeed: 0.1192s/iter; left time: 3213.0216s\n[TRAIN MEMORY] Max memory allocated in epoch 4: 638.32 MB\nEpoch: 4 cost time: 34.73863077163696\nEpoch: 4, Steps: 280 | Train Loss: 0.1770294 Vali Loss: 0.0442951 Test Loss: 0.1932740\nValidation loss decreased (0.044490 --> 0.044295).  Saving model ...\nUpdating learning rate to 7.852660404881378e-05\n\titers: 100, epoch: 5 | loss: 0.1734825\n\tspeed: 0.4163s/iter; left time: 11149.3928s\n\titers: 200, epoch: 5 | loss: 0.1656120\n\tspeed: 0.1183s/iter; left time: 3155.6963s\n[TRAIN MEMORY] Max memory allocated in epoch 5: 638.32 MB\nEpoch: 5 cost time: 34.21809148788452\nEpoch: 5, Steps: 280 | Train Loss: 0.1761656 Vali Loss: 0.0440604 Test Loss: 0.1906727\nValidation loss decreased (0.044295 --> 0.044060).  Saving model ...\nUpdating learning rate to 0.00013456138921506312\n\titers: 100, epoch: 6 | loss: 0.1781842\n\tspeed: 0.4113s/iter; left time: 10899.9166s\n\titers: 200, epoch: 6 | loss: 0.1788031\n\tspeed: 0.1173s/iter; left time: 3095.9269s\n[TRAIN MEMORY] Max memory allocated in epoch 6: 638.32 MB\nEpoch: 6 cost time: 34.25926327705383\nEpoch: 6, Steps: 280 | Train Loss: 0.1750881 Vali Loss: 0.0438466 Test Loss: 0.1909159\nValidation loss decreased (0.044060 --> 0.043847).  Saving model ...\nUpdating learning rate to 0.00022037924673853947\n\titers: 100, epoch: 7 | loss: 0.1737647\n\tspeed: 0.4156s/iter; left time: 10896.9923s\n\titers: 200, epoch: 7 | loss: 0.1746329\n\tspeed: 0.1176s/iter; left time: 3072.7788s\n[TRAIN MEMORY] Max memory allocated in epoch 7: 638.32 MB\nEpoch: 7 cost time: 34.120022773742676\nEpoch: 7, Steps: 280 | Train Loss: 0.1739687 Vali Loss: 0.0439455 Test Loss: 0.1861267\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00034590896044882054\n\titers: 100, epoch: 8 | loss: 0.1745680\n\tspeed: 0.4134s/iter; left time: 10722.8421s\n\titers: 200, epoch: 8 | loss: 0.1798652\n\tspeed: 0.1173s/iter; left time: 3031.7164s\n[TRAIN MEMORY] Max memory allocated in epoch 8: 638.32 MB\nEpoch: 8 cost time: 34.24930214881897\nEpoch: 8, Steps: 280 | Train Loss: 0.1733365 Vali Loss: 0.0436532 Test Loss: 0.1852265\nValidation loss decreased (0.043847 --> 0.043653).  Saving model ...\nUpdating learning rate to 0.0005179792390061816\n\titers: 100, epoch: 9 | loss: 0.1584484\n\tspeed: 0.4093s/iter; left time: 10502.6153s\n\titers: 200, epoch: 9 | loss: 0.1703895\n\tspeed: 0.1168s/iter; left time: 2984.4997s\n[TRAIN MEMORY] Max memory allocated in epoch 9: 638.32 MB\nEpoch: 9 cost time: 33.928038597106934\nEpoch: 9, Steps: 280 | Train Loss: 0.1735283 Vali Loss: 0.0435360 Test Loss: 0.1859332\nValidation loss decreased (0.043653 --> 0.043536).  Saving model ...\nUpdating learning rate to 0.0007341679251324548\n\titers: 100, epoch: 10 | loss: 0.1764603\n\tspeed: 0.4124s/iter; left time: 10465.9325s\n\titers: 200, epoch: 10 | loss: 0.1862697\n\tspeed: 0.1173s/iter; left time: 2964.9794s\n[TRAIN MEMORY] Max memory allocated in epoch 10: 638.32 MB\nEpoch: 10 cost time: 34.090288162231445\nEpoch: 10, Steps: 280 | Train Loss: 0.1737104 Vali Loss: 0.0445236 Test Loss: 0.1853334\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0009780261147388136\n\titers: 100, epoch: 11 | loss: 0.1702476\n\tspeed: 0.4150s/iter; left time: 10415.7621s\n\titers: 200, epoch: 11 | loss: 0.1798539\n\tspeed: 0.1169s/iter; left time: 2923.7211s\n[TRAIN MEMORY] Max memory allocated in epoch 11: 638.32 MB\nEpoch: 11 cost time: 34.04262733459473\nEpoch: 11, Steps: 280 | Train Loss: 0.1738958 Vali Loss: 0.0440124 Test Loss: 0.1891823\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0012218311574358645\n\titers: 100, epoch: 12 | loss: 0.1745311\n\tspeed: 0.4097s/iter; left time: 10169.5592s\n\titers: 200, epoch: 12 | loss: 0.1652545\n\tspeed: 0.1182s/iter; left time: 2922.3838s\n[TRAIN MEMORY] Max memory allocated in epoch 12: 638.32 MB\nEpoch: 12 cost time: 33.96903872489929\nEpoch: 12, Steps: 280 | Train Loss: 0.1734090 Vali Loss: 0.0444418 Test Loss: 0.1884265\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0014378602872914612\n\titers: 100, epoch: 13 | loss: 0.1755378\n\tspeed: 0.4019s/iter; left time: 9862.3144s\n\titers: 200, epoch: 13 | loss: 0.1782016\n\tspeed: 0.1174s/iter; left time: 2868.4942s\n[TRAIN MEMORY] Max memory allocated in epoch 13: 638.32 MB\nEpoch: 13 cost time: 34.0818989276886\nEpoch: 13, Steps: 280 | Train Loss: 0.1727739 Vali Loss: 0.0447169 Test Loss: 0.1874645\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.001609664253459064\n\titers: 100, epoch: 14 | loss: 0.1730306\n\tspeed: 0.4105s/iter; left time: 9958.0286s\n\titers: 200, epoch: 14 | loss: 0.1653014\n\tspeed: 0.1180s/iter; left time: 2850.4138s\n[TRAIN MEMORY] Max memory allocated in epoch 14: 638.32 MB\nEpoch: 14 cost time: 34.283238649368286\nEpoch: 14, Steps: 280 | Train Loss: 0.1711640 Vali Loss: 0.0447242 Test Loss: 0.1866426\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0017348203203004352\n\titers: 100, epoch: 15 | loss: 0.1700813\n\tspeed: 0.4197s/iter; left time: 10063.6515s\n\titers: 200, epoch: 15 | loss: 0.1642609\n\tspeed: 0.1185s/iter; left time: 2830.1853s\n[TRAIN MEMORY] Max memory allocated in epoch 15: 638.32 MB\nEpoch: 15 cost time: 34.2809636592865\nEpoch: 15, Steps: 280 | Train Loss: 0.1698892 Vali Loss: 0.0438774 Test Loss: 0.1909405\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.001820156385871022\n\titers: 100, epoch: 16 | loss: 0.1578789\n\tspeed: 0.4109s/iter; left time: 9739.2454s\n\titers: 200, epoch: 16 | loss: 0.1640466\n\tspeed: 0.1206s/iter; left time: 2846.3513s\n[TRAIN MEMORY] Max memory allocated in epoch 16: 638.32 MB\nEpoch: 16 cost time: 35.11617302894592\nEpoch: 16, Steps: 280 | Train Loss: 0.1680799 Vali Loss: 0.0449873 Test Loss: 0.1906542\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.0018756001902583204\n\titers: 100, epoch: 17 | loss: 0.1695729\n\tspeed: 0.4168s/iter; left time: 9760.8112s\n\titers: 200, epoch: 17 | loss: 0.1606022\n\tspeed: 0.1181s/iter; left time: 2754.9330s\n[TRAIN MEMORY] Max memory allocated in epoch 17: 638.32 MB\nEpoch: 17 cost time: 34.213587284088135\nEpoch: 17, Steps: 280 | Train Loss: 0.1666916 Vali Loss: 0.0446218 Test Loss: 0.1898191\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0019103360253404696\n\titers: 100, epoch: 18 | loss: 0.1637121\n\tspeed: 0.4195s/iter; left time: 9708.7591s\n\titers: 200, epoch: 18 | loss: 0.1672728\n\tspeed: 0.1194s/iter; left time: 2751.0699s\n[TRAIN MEMORY] Max memory allocated in epoch 18: 638.32 MB\nEpoch: 18 cost time: 34.652501344680786\nEpoch: 18, Steps: 280 | Train Loss: 0.1649095 Vali Loss: 0.0442695 Test Loss: 0.1948404\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.001931422581332935\n\titers: 100, epoch: 19 | loss: 0.1631594\n\tspeed: 0.4122s/iter; left time: 9423.6368s\n\titers: 200, epoch: 19 | loss: 0.1573964\n\tspeed: 0.1186s/iter; left time: 2699.9825s\n[TRAIN MEMORY] Max memory allocated in epoch 19: 638.32 MB\nEpoch: 19 cost time: 34.44533967971802\nEpoch: 19, Steps: 280 | Train Loss: 0.1638197 Vali Loss: 0.0436762 Test Loss: 0.1905496\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 34.67 seconds\n>>>>>>>testing : transformer_test_xPatch_Solar_ftM_sl720_ll48_pl192_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 10321\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 3.06 MB\n[PARAMS] Total parameters: 800,097\nmse:0.1859332174062729, mae:0.20903003215789795\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data Solar --learning_rate 0.002 --data_path solar.txt --seq_len 720 --pred_len 336 --enc_in 137 --dropout 0 --batch_size 16 --lradj 'sigmoid' --period_len 6 --d_model 128 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T17:37:37.127214Z","iopub.execute_input":"2025-10-21T17:37:37.127547Z","iopub.status.idle":"2025-10-21T18:02:28.944479Z","shell.execute_reply.started":"2025-10-21T17:37:37.127514Z","shell.execute_reply":"2025-10-21T18:02:28.943615Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=6, d_model=128, data='Solar', root_path='./dataset', data_path='solar.txt', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=720, label_len=48, pred_len=336, enc_in=137, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=16, patience=10, learning_rate=0.002, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.0, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_Solar_ftM_sl720_ll48_pl336_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 35737\nval 4921\ntest 10177\n\titers: 100, epoch: 1 | loss: 0.2317959\n\tspeed: 0.0526s/iter; left time: 11749.4570s\n\titers: 200, epoch: 1 | loss: 0.2241740\n\tspeed: 0.0287s/iter; left time: 6402.4820s\n\titers: 300, epoch: 1 | loss: 0.2064004\n\tspeed: 0.0282s/iter; left time: 6286.7480s\n\titers: 400, epoch: 1 | loss: 0.1921412\n\tspeed: 0.0279s/iter; left time: 6208.4844s\n\titers: 500, epoch: 1 | loss: 0.1818175\n\tspeed: 0.0291s/iter; left time: 6482.5696s\n\titers: 600, epoch: 1 | loss: 0.2033658\n\tspeed: 0.0287s/iter; left time: 6396.9762s\n\titers: 700, epoch: 1 | loss: 0.2174829\n\tspeed: 0.0286s/iter; left time: 6362.0545s\n\titers: 800, epoch: 1 | loss: 0.1759263\n\tspeed: 0.0287s/iter; left time: 6377.5798s\n\titers: 900, epoch: 1 | loss: 0.2048726\n\tspeed: 0.0296s/iter; left time: 6573.9198s\n\titers: 1000, epoch: 1 | loss: 0.1783031\n\tspeed: 0.0294s/iter; left time: 6535.4073s\n\titers: 1100, epoch: 1 | loss: 0.1906187\n\tspeed: 0.0289s/iter; left time: 6427.2888s\n\titers: 1200, epoch: 1 | loss: 0.2025383\n\tspeed: 0.0294s/iter; left time: 6529.7769s\n\titers: 1300, epoch: 1 | loss: 0.1913669\n\tspeed: 0.0297s/iter; left time: 6595.6369s\n\titers: 1400, epoch: 1 | loss: 0.1725902\n\tspeed: 0.0290s/iter; left time: 6427.2897s\n\titers: 1500, epoch: 1 | loss: 0.1918811\n\tspeed: 0.0281s/iter; left time: 6238.1812s\n\titers: 1600, epoch: 1 | loss: 0.1898046\n\tspeed: 0.0288s/iter; left time: 6375.9139s\n\titers: 1700, epoch: 1 | loss: 0.2110828\n\tspeed: 0.0287s/iter; left time: 6363.9094s\n\titers: 1800, epoch: 1 | loss: 0.1891730\n\tspeed: 0.0281s/iter; left time: 6218.7121s\n\titers: 1900, epoch: 1 | loss: 0.1847847\n\tspeed: 0.0290s/iter; left time: 6413.8599s\n\titers: 2000, epoch: 1 | loss: 0.1875024\n\tspeed: 0.0301s/iter; left time: 6652.0449s\n\titers: 2100, epoch: 1 | loss: 0.1988549\n\tspeed: 0.0294s/iter; left time: 6508.4695s\n\titers: 2200, epoch: 1 | loss: 0.1864668\n\tspeed: 0.0296s/iter; left time: 6534.6440s\n[TRAIN MEMORY] Max memory allocated in epoch 1: 128.39 MB\nEpoch: 1 cost time: 65.89294815063477\nEpoch: 1, Steps: 2233 | Train Loss: 0.2026391 Vali Loss: 0.0484784 Test Loss: 0.2106528\nValidation loss decreased (inf --> 0.048478).  Saving model ...\nUpdating learning rate to 7.90671095119604e-06\n\titers: 100, epoch: 2 | loss: 0.1615809\n\tspeed: 0.2895s/iter; left time: 63966.0025s\n\titers: 200, epoch: 2 | loss: 0.1703342\n\tspeed: 0.0292s/iter; left time: 6445.3821s\n\titers: 300, epoch: 2 | loss: 0.1740210\n\tspeed: 0.0283s/iter; left time: 6245.3367s\n\titers: 400, epoch: 2 | loss: 0.1508572\n\tspeed: 0.0297s/iter; left time: 6560.6949s\n\titers: 500, epoch: 2 | loss: 0.1695789\n\tspeed: 0.0292s/iter; left time: 6431.3872s\n\titers: 600, epoch: 2 | loss: 0.1690735\n\tspeed: 0.0301s/iter; left time: 6643.5314s\n\titers: 700, epoch: 2 | loss: 0.2042115\n\tspeed: 0.0294s/iter; left time: 6474.3624s\n\titers: 800, epoch: 2 | loss: 0.1684282\n\tspeed: 0.0294s/iter; left time: 6468.3853s\n\titers: 900, epoch: 2 | loss: 0.1759394\n\tspeed: 0.0302s/iter; left time: 6655.3973s\n\titers: 1000, epoch: 2 | loss: 0.2239129\n\tspeed: 0.0285s/iter; left time: 6273.7165s\n\titers: 1100, epoch: 2 | loss: 0.1443770\n\tspeed: 0.0297s/iter; left time: 6524.3418s\n\titers: 1200, epoch: 2 | loss: 0.1765549\n\tspeed: 0.0289s/iter; left time: 6354.7044s\n\titers: 1300, epoch: 2 | loss: 0.1811204\n\tspeed: 0.0293s/iter; left time: 6437.7506s\n\titers: 1400, epoch: 2 | loss: 0.2009986\n\tspeed: 0.0297s/iter; left time: 6532.5638s\n\titers: 1500, epoch: 2 | loss: 0.1690006\n\tspeed: 0.0297s/iter; left time: 6511.8939s\n\titers: 1600, epoch: 2 | loss: 0.2015984\n\tspeed: 0.0300s/iter; left time: 6573.1668s\n\titers: 1700, epoch: 2 | loss: 0.1937816\n\tspeed: 0.0302s/iter; left time: 6619.0181s\n\titers: 1800, epoch: 2 | loss: 0.1712884\n\tspeed: 0.0303s/iter; left time: 6647.6115s\n\titers: 1900, epoch: 2 | loss: 0.1942520\n\tspeed: 0.0308s/iter; left time: 6753.9489s\n\titers: 2000, epoch: 2 | loss: 0.1774407\n\tspeed: 0.0301s/iter; left time: 6596.1222s\n\titers: 2100, epoch: 2 | loss: 0.1726376\n\tspeed: 0.0292s/iter; left time: 6393.1051s\n\titers: 2200, epoch: 2 | loss: 0.1775072\n\tspeed: 0.0294s/iter; left time: 6434.0790s\n[TRAIN MEMORY] Max memory allocated in epoch 2: 128.39 MB\nEpoch: 2 cost time: 66.51304292678833\nEpoch: 2, Steps: 2233 | Train Loss: 0.1752292 Vali Loss: 0.0435197 Test Loss: 0.2094588\nValidation loss decreased (0.048478 --> 0.043520).  Saving model ...\nUpdating learning rate to 2.118933723561918e-05\n\titers: 100, epoch: 3 | loss: 0.1754286\n\tspeed: 0.2864s/iter; left time: 62635.7278s\n\titers: 200, epoch: 3 | loss: 0.1675345\n\tspeed: 0.0290s/iter; left time: 6351.0172s\n\titers: 300, epoch: 3 | loss: 0.1802402\n\tspeed: 0.0285s/iter; left time: 6231.1802s\n\titers: 400, epoch: 3 | loss: 0.1632880\n\tspeed: 0.0282s/iter; left time: 6167.5501s\n\titers: 500, epoch: 3 | loss: 0.1578882\n\tspeed: 0.0287s/iter; left time: 6273.8747s\n\titers: 600, epoch: 3 | loss: 0.1407980\n\tspeed: 0.0292s/iter; left time: 6372.5475s\n\titers: 700, epoch: 3 | loss: 0.1940607\n\tspeed: 0.0291s/iter; left time: 6339.9245s\n\titers: 800, epoch: 3 | loss: 0.1686205\n\tspeed: 0.0291s/iter; left time: 6345.0261s\n\titers: 900, epoch: 3 | loss: 0.1798572\n\tspeed: 0.0288s/iter; left time: 6286.8069s\n\titers: 1000, epoch: 3 | loss: 0.1429491\n\tspeed: 0.0293s/iter; left time: 6377.7933s\n\titers: 1100, epoch: 3 | loss: 0.1392533\n\tspeed: 0.0290s/iter; left time: 6311.4076s\n\titers: 1200, epoch: 3 | loss: 0.1683813\n\tspeed: 0.0298s/iter; left time: 6474.6836s\n\titers: 1300, epoch: 3 | loss: 0.1806266\n\tspeed: 0.0284s/iter; left time: 6185.2932s\n\titers: 1400, epoch: 3 | loss: 0.1647244\n\tspeed: 0.0288s/iter; left time: 6270.4678s\n\titers: 1500, epoch: 3 | loss: 0.1841628\n\tspeed: 0.0301s/iter; left time: 6544.9795s\n\titers: 1600, epoch: 3 | loss: 0.1916905\n\tspeed: 0.0289s/iter; left time: 6271.2861s\n\titers: 1700, epoch: 3 | loss: 0.1986269\n\tspeed: 0.0288s/iter; left time: 6258.8137s\n\titers: 1800, epoch: 3 | loss: 0.1240331\n\tspeed: 0.0287s/iter; left time: 6228.0965s\n\titers: 1900, epoch: 3 | loss: 0.1765129\n\tspeed: 0.0294s/iter; left time: 6367.3157s\n\titers: 2000, epoch: 3 | loss: 0.2103907\n\tspeed: 0.0283s/iter; left time: 6140.6199s\n\titers: 2100, epoch: 3 | loss: 0.1927799\n\tspeed: 0.0282s/iter; left time: 6107.8421s\n\titers: 2200, epoch: 3 | loss: 0.1650032\n\tspeed: 0.0288s/iter; left time: 6233.9475s\n[TRAIN MEMORY] Max memory allocated in epoch 3: 128.39 MB\nEpoch: 3 cost time: 65.09379482269287\nEpoch: 3, Steps: 2233 | Train Loss: 0.1719504 Vali Loss: 0.0428571 Test Loss: 0.2067499\nValidation loss decreased (0.043520 --> 0.042857).  Saving model ...\nUpdating learning rate to 4.308932134668265e-05\n\titers: 100, epoch: 4 | loss: 0.1875329\n\tspeed: 0.2917s/iter; left time: 63147.9700s\n\titers: 200, epoch: 4 | loss: 0.1620092\n\tspeed: 0.0296s/iter; left time: 6407.7658s\n\titers: 300, epoch: 4 | loss: 0.1619681\n\tspeed: 0.0284s/iter; left time: 6143.3660s\n\titers: 400, epoch: 4 | loss: 0.1856026\n\tspeed: 0.0291s/iter; left time: 6281.9670s\n\titers: 500, epoch: 4 | loss: 0.1360150\n\tspeed: 0.0290s/iter; left time: 6262.7452s\n\titers: 600, epoch: 4 | loss: 0.1587437\n\tspeed: 0.0288s/iter; left time: 6219.2696s\n\titers: 700, epoch: 4 | loss: 0.1723852\n\tspeed: 0.0285s/iter; left time: 6149.8244s\n\titers: 800, epoch: 4 | loss: 0.1724724\n\tspeed: 0.0285s/iter; left time: 6141.5164s\n\titers: 900, epoch: 4 | loss: 0.1803598\n\tspeed: 0.0289s/iter; left time: 6231.4959s\n\titers: 1000, epoch: 4 | loss: 0.1610235\n\tspeed: 0.0294s/iter; left time: 6333.4023s\n\titers: 1100, epoch: 4 | loss: 0.1918335\n\tspeed: 0.0288s/iter; left time: 6213.4460s\n\titers: 1200, epoch: 4 | loss: 0.1741802\n\tspeed: 0.0288s/iter; left time: 6209.8320s\n\titers: 1300, epoch: 4 | loss: 0.1514580\n\tspeed: 0.0287s/iter; left time: 6188.2298s\n\titers: 1400, epoch: 4 | loss: 0.1870977\n\tspeed: 0.0287s/iter; left time: 6175.7343s\n\titers: 1500, epoch: 4 | loss: 0.1774880\n\tspeed: 0.0287s/iter; left time: 6182.4845s\n\titers: 1600, epoch: 4 | loss: 0.1362505\n\tspeed: 0.0293s/iter; left time: 6307.7609s\n\titers: 1700, epoch: 4 | loss: 0.1817623\n\tspeed: 0.0284s/iter; left time: 6103.7669s\n\titers: 1800, epoch: 4 | loss: 0.1973277\n\tspeed: 0.0289s/iter; left time: 6216.8222s\n\titers: 1900, epoch: 4 | loss: 0.1531540\n\tspeed: 0.0287s/iter; left time: 6153.8153s\n\titers: 2000, epoch: 4 | loss: 0.1400574\n\tspeed: 0.0293s/iter; left time: 6281.2446s\n\titers: 2100, epoch: 4 | loss: 0.2117740\n\tspeed: 0.0284s/iter; left time: 6096.7708s\n\titers: 2200, epoch: 4 | loss: 0.1789612\n\tspeed: 0.0294s/iter; left time: 6294.6380s\n[TRAIN MEMORY] Max memory allocated in epoch 4: 128.39 MB\nEpoch: 4 cost time: 64.96197772026062\nEpoch: 4, Steps: 2233 | Train Loss: 0.1703060 Vali Loss: 0.0425099 Test Loss: 0.2034503\nValidation loss decreased (0.042857 --> 0.042510).  Saving model ...\nUpdating learning rate to 7.852660404881378e-05\n\titers: 100, epoch: 5 | loss: 0.1485136\n\tspeed: 0.2855s/iter; left time: 61168.0850s\n\titers: 200, epoch: 5 | loss: 0.1417357\n\tspeed: 0.0296s/iter; left time: 6349.3876s\n\titers: 300, epoch: 5 | loss: 0.1786114\n\tspeed: 0.0298s/iter; left time: 6382.6962s\n\titers: 400, epoch: 5 | loss: 0.1788001\n\tspeed: 0.0293s/iter; left time: 6276.6614s\n\titers: 500, epoch: 5 | loss: 0.1879827\n\tspeed: 0.0293s/iter; left time: 6261.6039s\n\titers: 600, epoch: 5 | loss: 0.1355464\n\tspeed: 0.0290s/iter; left time: 6191.8173s\n\titers: 700, epoch: 5 | loss: 0.1805968\n\tspeed: 0.0290s/iter; left time: 6193.3553s\n\titers: 800, epoch: 5 | loss: 0.1544644\n\tspeed: 0.0289s/iter; left time: 6175.2875s\n\titers: 900, epoch: 5 | loss: 0.1438798\n\tspeed: 0.0287s/iter; left time: 6126.6405s\n\titers: 1000, epoch: 5 | loss: 0.1571429\n\tspeed: 0.0289s/iter; left time: 6162.6696s\n\titers: 1100, epoch: 5 | loss: 0.1988448\n\tspeed: 0.0283s/iter; left time: 6029.5298s\n\titers: 1200, epoch: 5 | loss: 0.1717682\n\tspeed: 0.0290s/iter; left time: 6192.1655s\n\titers: 1300, epoch: 5 | loss: 0.1569326\n\tspeed: 0.0280s/iter; left time: 5973.5305s\n\titers: 1400, epoch: 5 | loss: 0.1624767\n\tspeed: 0.0290s/iter; left time: 6174.6642s\n\titers: 1500, epoch: 5 | loss: 0.1625453\n\tspeed: 0.0288s/iter; left time: 6125.9207s\n\titers: 1600, epoch: 5 | loss: 0.1607869\n\tspeed: 0.0299s/iter; left time: 6355.1731s\n\titers: 1700, epoch: 5 | loss: 0.1840203\n\tspeed: 0.0297s/iter; left time: 6314.6123s\n\titers: 1800, epoch: 5 | loss: 0.1733042\n\tspeed: 0.0286s/iter; left time: 6069.7608s\n\titers: 1900, epoch: 5 | loss: 0.1610897\n\tspeed: 0.0301s/iter; left time: 6385.0787s\n\titers: 2000, epoch: 5 | loss: 0.1512888\n\tspeed: 0.0291s/iter; left time: 6181.7847s\n\titers: 2100, epoch: 5 | loss: 0.1742333\n\tspeed: 0.0291s/iter; left time: 6175.8312s\n\titers: 2200, epoch: 5 | loss: 0.1467949\n\tspeed: 0.0293s/iter; left time: 6205.9851s\n[TRAIN MEMORY] Max memory allocated in epoch 5: 128.39 MB\nEpoch: 5 cost time: 65.46205568313599\nEpoch: 5, Steps: 2233 | Train Loss: 0.1689637 Vali Loss: 0.0426271 Test Loss: 0.1995009\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00013456138921506312\n\titers: 100, epoch: 6 | loss: 0.1553528\n\tspeed: 0.2923s/iter; left time: 61969.9777s\n\titers: 200, epoch: 6 | loss: 0.1457749\n\tspeed: 0.0294s/iter; left time: 6225.4351s\n\titers: 300, epoch: 6 | loss: 0.1621090\n\tspeed: 0.0280s/iter; left time: 5930.8978s\n\titers: 400, epoch: 6 | loss: 0.1864312\n\tspeed: 0.0282s/iter; left time: 5960.6527s\n\titers: 500, epoch: 6 | loss: 0.1421270\n\tspeed: 0.0285s/iter; left time: 6041.5332s\n\titers: 600, epoch: 6 | loss: 0.1834448\n\tspeed: 0.0279s/iter; left time: 5905.1108s\n\titers: 700, epoch: 6 | loss: 0.1492494\n\tspeed: 0.0290s/iter; left time: 6121.7880s\n\titers: 800, epoch: 6 | loss: 0.1650181\n\tspeed: 0.0284s/iter; left time: 6002.4864s\n\titers: 900, epoch: 6 | loss: 0.1748650\n\tspeed: 0.0291s/iter; left time: 6149.9739s\n\titers: 1000, epoch: 6 | loss: 0.1861063\n\tspeed: 0.0291s/iter; left time: 6149.7617s\n\titers: 1100, epoch: 6 | loss: 0.1602782\n\tspeed: 0.0289s/iter; left time: 6104.1910s\n\titers: 1200, epoch: 6 | loss: 0.1764200\n\tspeed: 0.0289s/iter; left time: 6086.8730s\n\titers: 1300, epoch: 6 | loss: 0.1653066\n\tspeed: 0.0286s/iter; left time: 6040.4399s\n\titers: 1400, epoch: 6 | loss: 0.1560434\n\tspeed: 0.0284s/iter; left time: 5994.4349s\n\titers: 1500, epoch: 6 | loss: 0.1703847\n\tspeed: 0.0285s/iter; left time: 6000.8077s\n\titers: 1600, epoch: 6 | loss: 0.1660833\n\tspeed: 0.0288s/iter; left time: 6056.5527s\n\titers: 1700, epoch: 6 | loss: 0.1612871\n\tspeed: 0.0289s/iter; left time: 6078.7624s\n\titers: 1800, epoch: 6 | loss: 0.1278749\n\tspeed: 0.0291s/iter; left time: 6112.4733s\n\titers: 1900, epoch: 6 | loss: 0.1696284\n\tspeed: 0.0294s/iter; left time: 6189.4331s\n\titers: 2000, epoch: 6 | loss: 0.1911503\n\tspeed: 0.0288s/iter; left time: 6050.6445s\n\titers: 2100, epoch: 6 | loss: 0.1705063\n\tspeed: 0.0290s/iter; left time: 6092.7683s\n\titers: 2200, epoch: 6 | loss: 0.1405914\n\tspeed: 0.0296s/iter; left time: 6203.7027s\n[TRAIN MEMORY] Max memory allocated in epoch 6: 128.39 MB\nEpoch: 6 cost time: 64.89981937408447\nEpoch: 6, Steps: 2233 | Train Loss: 0.1681065 Vali Loss: 0.0425083 Test Loss: 0.1979055\nValidation loss decreased (0.042510 --> 0.042508).  Saving model ...\nUpdating learning rate to 0.00022037924673853947\n\titers: 100, epoch: 7 | loss: 0.1400295\n\tspeed: 0.2871s/iter; left time: 60224.2811s\n\titers: 200, epoch: 7 | loss: 0.1384407\n\tspeed: 0.0291s/iter; left time: 6109.1421s\n\titers: 300, epoch: 7 | loss: 0.1816194\n\tspeed: 0.0288s/iter; left time: 6034.9915s\n\titers: 400, epoch: 7 | loss: 0.1922566\n\tspeed: 0.0283s/iter; left time: 5933.3543s\n\titers: 500, epoch: 7 | loss: 0.1604943\n\tspeed: 0.0299s/iter; left time: 6261.6280s\n\titers: 600, epoch: 7 | loss: 0.1621743\n\tspeed: 0.0293s/iter; left time: 6123.3966s\n\titers: 700, epoch: 7 | loss: 0.1755508\n\tspeed: 0.0293s/iter; left time: 6124.0941s\n\titers: 800, epoch: 7 | loss: 0.1626722\n\tspeed: 0.0283s/iter; left time: 5908.2207s\n\titers: 900, epoch: 7 | loss: 0.1759066\n\tspeed: 0.0283s/iter; left time: 5922.6405s\n\titers: 1000, epoch: 7 | loss: 0.1662951\n\tspeed: 0.0294s/iter; left time: 6148.9767s\n\titers: 1100, epoch: 7 | loss: 0.1637815\n\tspeed: 0.0289s/iter; left time: 6036.2489s\n\titers: 1200, epoch: 7 | loss: 0.1817164\n\tspeed: 0.0300s/iter; left time: 6257.6552s\n\titers: 1300, epoch: 7 | loss: 0.1540970\n\tspeed: 0.0286s/iter; left time: 5976.0786s\n\titers: 1400, epoch: 7 | loss: 0.1405837\n\tspeed: 0.0288s/iter; left time: 6014.0688s\n\titers: 1500, epoch: 7 | loss: 0.1830399\n\tspeed: 0.0286s/iter; left time: 5965.8617s\n\titers: 1600, epoch: 7 | loss: 0.1836488\n\tspeed: 0.0295s/iter; left time: 6148.0232s\n\titers: 1700, epoch: 7 | loss: 0.1837038\n\tspeed: 0.0292s/iter; left time: 6087.0610s\n\titers: 1800, epoch: 7 | loss: 0.1514076\n\tspeed: 0.0291s/iter; left time: 6055.1118s\n\titers: 1900, epoch: 7 | loss: 0.1689748\n\tspeed: 0.0288s/iter; left time: 5984.0982s\n\titers: 2000, epoch: 7 | loss: 0.1616019\n\tspeed: 0.0291s/iter; left time: 6055.6897s\n\titers: 2100, epoch: 7 | loss: 0.1545252\n\tspeed: 0.0296s/iter; left time: 6155.6984s\n\titers: 2200, epoch: 7 | loss: 0.1773637\n\tspeed: 0.0291s/iter; left time: 6052.0579s\n[TRAIN MEMORY] Max memory allocated in epoch 7: 128.39 MB\nEpoch: 7 cost time: 65.33693838119507\nEpoch: 7, Steps: 2233 | Train Loss: 0.1677010 Vali Loss: 0.0428158 Test Loss: 0.1991676\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00034590896044882054\n\titers: 100, epoch: 8 | loss: 0.1465296\n\tspeed: 0.2959s/iter; left time: 61427.4826s\n\titers: 200, epoch: 8 | loss: 0.1645486\n\tspeed: 0.0305s/iter; left time: 6326.3960s\n\titers: 300, epoch: 8 | loss: 0.1720211\n\tspeed: 0.0289s/iter; left time: 5984.8726s\n\titers: 400, epoch: 8 | loss: 0.1668494\n\tspeed: 0.0271s/iter; left time: 5621.4918s\n\titers: 500, epoch: 8 | loss: 0.1541022\n\tspeed: 0.0290s/iter; left time: 5997.7320s\n\titers: 600, epoch: 8 | loss: 0.1984744\n\tspeed: 0.0288s/iter; left time: 5965.5658s\n\titers: 700, epoch: 8 | loss: 0.1954649\n\tspeed: 0.0285s/iter; left time: 5905.9164s\n\titers: 800, epoch: 8 | loss: 0.1486840\n\tspeed: 0.0295s/iter; left time: 6110.7164s\n\titers: 900, epoch: 8 | loss: 0.1731202\n\tspeed: 0.0294s/iter; left time: 6068.8266s\n\titers: 1000, epoch: 8 | loss: 0.1206809\n\tspeed: 0.0294s/iter; left time: 6075.1590s\n\titers: 1100, epoch: 8 | loss: 0.1776213\n\tspeed: 0.0291s/iter; left time: 6001.5928s\n\titers: 1200, epoch: 8 | loss: 0.1646666\n\tspeed: 0.0290s/iter; left time: 5997.5655s\n\titers: 1300, epoch: 8 | loss: 0.1742068\n\tspeed: 0.0293s/iter; left time: 6055.5079s\n\titers: 1400, epoch: 8 | loss: 0.1703298\n\tspeed: 0.0293s/iter; left time: 6047.9838s\n\titers: 1500, epoch: 8 | loss: 0.1633537\n\tspeed: 0.0298s/iter; left time: 6139.3005s\n\titers: 1600, epoch: 8 | loss: 0.1454401\n\tspeed: 0.0298s/iter; left time: 6149.5223s\n\titers: 1700, epoch: 8 | loss: 0.1776330\n\tspeed: 0.0283s/iter; left time: 5836.8983s\n\titers: 1800, epoch: 8 | loss: 0.1963036\n\tspeed: 0.0291s/iter; left time: 5981.3475s\n\titers: 1900, epoch: 8 | loss: 0.1681394\n\tspeed: 0.0292s/iter; left time: 6010.6202s\n\titers: 2000, epoch: 8 | loss: 0.1327634\n\tspeed: 0.0285s/iter; left time: 5867.1093s\n\titers: 2100, epoch: 8 | loss: 0.1920628\n\tspeed: 0.0278s/iter; left time: 5711.9998s\n\titers: 2200, epoch: 8 | loss: 0.1506276\n\tspeed: 0.0280s/iter; left time: 5749.9605s\n[TRAIN MEMORY] Max memory allocated in epoch 8: 128.39 MB\nEpoch: 8 cost time: 65.24133896827698\nEpoch: 8, Steps: 2233 | Train Loss: 0.1677907 Vali Loss: 0.0427670 Test Loss: 0.1973169\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0005179792390061816\n\titers: 100, epoch: 9 | loss: 0.1664299\n\tspeed: 0.2872s/iter; left time: 58979.6746s\n\titers: 200, epoch: 9 | loss: 0.1458781\n\tspeed: 0.0286s/iter; left time: 5866.6776s\n\titers: 300, epoch: 9 | loss: 0.1619883\n\tspeed: 0.0285s/iter; left time: 5839.4539s\n\titers: 400, epoch: 9 | loss: 0.1836300\n\tspeed: 0.0285s/iter; left time: 5833.6967s\n\titers: 500, epoch: 9 | loss: 0.1539656\n\tspeed: 0.0295s/iter; left time: 6055.2755s\n\titers: 600, epoch: 9 | loss: 0.1642738\n\tspeed: 0.0288s/iter; left time: 5904.7477s\n\titers: 700, epoch: 9 | loss: 0.1648122\n\tspeed: 0.0285s/iter; left time: 5836.0870s\n\titers: 800, epoch: 9 | loss: 0.1537916\n\tspeed: 0.0287s/iter; left time: 5870.2055s\n\titers: 900, epoch: 9 | loss: 0.1833108\n\tspeed: 0.0293s/iter; left time: 5988.1006s\n\titers: 1000, epoch: 9 | loss: 0.1630747\n\tspeed: 0.0290s/iter; left time: 5926.6621s\n\titers: 1100, epoch: 9 | loss: 0.1749195\n\tspeed: 0.0290s/iter; left time: 5916.5363s\n\titers: 1200, epoch: 9 | loss: 0.1425343\n\tspeed: 0.0289s/iter; left time: 5892.4478s\n\titers: 1300, epoch: 9 | loss: 0.1881587\n\tspeed: 0.0297s/iter; left time: 6054.6695s\n\titers: 1400, epoch: 9 | loss: 0.2020644\n\tspeed: 0.0300s/iter; left time: 6111.1774s\n\titers: 1500, epoch: 9 | loss: 0.1681028\n\tspeed: 0.0300s/iter; left time: 6117.6705s\n\titers: 1600, epoch: 9 | loss: 0.1654044\n\tspeed: 0.0290s/iter; left time: 5919.9398s\n\titers: 1700, epoch: 9 | loss: 0.1663169\n\tspeed: 0.0294s/iter; left time: 5991.5408s\n\titers: 1800, epoch: 9 | loss: 0.1856960\n\tspeed: 0.0284s/iter; left time: 5788.2229s\n\titers: 1900, epoch: 9 | loss: 0.1471523\n\tspeed: 0.0296s/iter; left time: 6030.1002s\n\titers: 2000, epoch: 9 | loss: 0.1789098\n\tspeed: 0.0282s/iter; left time: 5744.2055s\n\titers: 2100, epoch: 9 | loss: 0.1706679\n\tspeed: 0.0283s/iter; left time: 5760.7567s\n\titers: 2200, epoch: 9 | loss: 0.1333612\n\tspeed: 0.0287s/iter; left time: 5837.8262s\n[TRAIN MEMORY] Max memory allocated in epoch 9: 128.39 MB\nEpoch: 9 cost time: 65.16068291664124\nEpoch: 9, Steps: 2233 | Train Loss: 0.1682091 Vali Loss: 0.0435041 Test Loss: 0.1985644\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.0007341679251324548\n\titers: 100, epoch: 10 | loss: 0.1833144\n\tspeed: 0.2778s/iter; left time: 56421.3154s\n\titers: 200, epoch: 10 | loss: 0.1912554\n\tspeed: 0.0297s/iter; left time: 6019.4145s\n\titers: 300, epoch: 10 | loss: 0.1945900\n\tspeed: 0.0289s/iter; left time: 5862.5814s\n\titers: 400, epoch: 10 | loss: 0.1864700\n\tspeed: 0.0277s/iter; left time: 5609.7802s\n\titers: 500, epoch: 10 | loss: 0.1478608\n\tspeed: 0.0275s/iter; left time: 5575.3488s\n\titers: 600, epoch: 10 | loss: 0.1477807\n\tspeed: 0.0284s/iter; left time: 5757.9328s\n\titers: 700, epoch: 10 | loss: 0.1690514\n\tspeed: 0.0278s/iter; left time: 5638.1404s\n\titers: 800, epoch: 10 | loss: 0.2126699\n\tspeed: 0.0280s/iter; left time: 5670.0780s\n\titers: 900, epoch: 10 | loss: 0.1434877\n\tspeed: 0.0284s/iter; left time: 5748.6530s\n\titers: 1000, epoch: 10 | loss: 0.1604590\n\tspeed: 0.0284s/iter; left time: 5737.1198s\n\titers: 1100, epoch: 10 | loss: 0.1735100\n\tspeed: 0.0284s/iter; left time: 5731.4525s\n\titers: 1200, epoch: 10 | loss: 0.1736817\n\tspeed: 0.0286s/iter; left time: 5776.1323s\n\titers: 1300, epoch: 10 | loss: 0.1504470\n\tspeed: 0.0289s/iter; left time: 5828.7564s\n\titers: 1400, epoch: 10 | loss: 0.1648779\n\tspeed: 0.0289s/iter; left time: 5830.6400s\n\titers: 1500, epoch: 10 | loss: 0.1770839\n\tspeed: 0.0287s/iter; left time: 5782.8520s\n\titers: 1600, epoch: 10 | loss: 0.1853846\n\tspeed: 0.0288s/iter; left time: 5803.7700s\n\titers: 1700, epoch: 10 | loss: 0.1560448\n\tspeed: 0.0284s/iter; left time: 5728.9652s\n\titers: 1800, epoch: 10 | loss: 0.1672489\n\tspeed: 0.0286s/iter; left time: 5757.5141s\n\titers: 1900, epoch: 10 | loss: 0.1423694\n\tspeed: 0.0278s/iter; left time: 5606.1803s\n\titers: 2000, epoch: 10 | loss: 0.1668117\n\tspeed: 0.0288s/iter; left time: 5796.1313s\n\titers: 2100, epoch: 10 | loss: 0.1723442\n\tspeed: 0.0291s/iter; left time: 5856.4643s\n\titers: 2200, epoch: 10 | loss: 0.1630851\n\tspeed: 0.0287s/iter; left time: 5771.2179s\n[TRAIN MEMORY] Max memory allocated in epoch 10: 128.39 MB\nEpoch: 10 cost time: 64.00527477264404\nEpoch: 10, Steps: 2233 | Train Loss: 0.1688241 Vali Loss: 0.0438981 Test Loss: 0.1987627\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 0.0009780261147388136\n\titers: 100, epoch: 11 | loss: 0.1851385\n\tspeed: 0.2837s/iter; left time: 56978.7512s\n\titers: 200, epoch: 11 | loss: 0.1594920\n\tspeed: 0.0282s/iter; left time: 5666.3589s\n\titers: 300, epoch: 11 | loss: 0.1648971\n\tspeed: 0.0288s/iter; left time: 5769.3152s\n\titers: 400, epoch: 11 | loss: 0.1546325\n\tspeed: 0.0280s/iter; left time: 5624.7978s\n\titers: 500, epoch: 11 | loss: 0.1868231\n\tspeed: 0.0283s/iter; left time: 5679.4069s\n\titers: 600, epoch: 11 | loss: 0.1440544\n\tspeed: 0.0289s/iter; left time: 5780.9293s\n\titers: 700, epoch: 11 | loss: 0.1519110\n\tspeed: 0.0290s/iter; left time: 5813.7819s\n\titers: 800, epoch: 11 | loss: 0.1591399\n\tspeed: 0.0284s/iter; left time: 5675.3471s\n\titers: 900, epoch: 11 | loss: 0.1822665\n\tspeed: 0.0283s/iter; left time: 5663.6787s\n\titers: 1000, epoch: 11 | loss: 0.1735615\n\tspeed: 0.0282s/iter; left time: 5646.3155s\n\titers: 1100, epoch: 11 | loss: 0.1825616\n\tspeed: 0.0284s/iter; left time: 5685.0033s\n\titers: 1200, epoch: 11 | loss: 0.1714548\n\tspeed: 0.0290s/iter; left time: 5783.9834s\n\titers: 1300, epoch: 11 | loss: 0.1732584\n\tspeed: 0.0286s/iter; left time: 5715.9679s\n\titers: 1400, epoch: 11 | loss: 0.1878365\n\tspeed: 0.0291s/iter; left time: 5801.9169s\n\titers: 1500, epoch: 11 | loss: 0.1616582\n\tspeed: 0.0282s/iter; left time: 5618.2633s\n\titers: 1600, epoch: 11 | loss: 0.2085554\n\tspeed: 0.0282s/iter; left time: 5629.0375s\n\titers: 1700, epoch: 11 | loss: 0.2028140\n\tspeed: 0.0283s/iter; left time: 5634.4072s\n\titers: 1800, epoch: 11 | loss: 0.1628949\n\tspeed: 0.0294s/iter; left time: 5855.7477s\n\titers: 1900, epoch: 11 | loss: 0.1579898\n\tspeed: 0.0290s/iter; left time: 5772.3838s\n\titers: 2000, epoch: 11 | loss: 0.1404698\n\tspeed: 0.0287s/iter; left time: 5713.6349s\n\titers: 2100, epoch: 11 | loss: 0.1823307\n\tspeed: 0.0292s/iter; left time: 5809.3567s\n\titers: 2200, epoch: 11 | loss: 0.1530947\n\tspeed: 0.0282s/iter; left time: 5600.5670s\n[TRAIN MEMORY] Max memory allocated in epoch 11: 128.39 MB\nEpoch: 11 cost time: 64.22043919563293\nEpoch: 11, Steps: 2233 | Train Loss: 0.1691001 Vali Loss: 0.0433363 Test Loss: 0.2024464\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 0.0012218311574358645\n\titers: 100, epoch: 12 | loss: 0.1557653\n\tspeed: 0.2880s/iter; left time: 57206.9985s\n\titers: 200, epoch: 12 | loss: 0.1721237\n\tspeed: 0.0284s/iter; left time: 5644.3590s\n\titers: 300, epoch: 12 | loss: 0.1796193\n\tspeed: 0.0282s/iter; left time: 5594.3121s\n\titers: 400, epoch: 12 | loss: 0.1801053\n\tspeed: 0.0285s/iter; left time: 5648.8928s\n\titers: 500, epoch: 12 | loss: 0.1750070\n\tspeed: 0.0289s/iter; left time: 5732.9304s\n\titers: 600, epoch: 12 | loss: 0.1878375\n\tspeed: 0.0297s/iter; left time: 5887.4948s\n\titers: 700, epoch: 12 | loss: 0.2021460\n\tspeed: 0.0297s/iter; left time: 5874.0896s\n\titers: 800, epoch: 12 | loss: 0.1368507\n\tspeed: 0.0292s/iter; left time: 5774.6727s\n\titers: 900, epoch: 12 | loss: 0.1721560\n\tspeed: 0.0293s/iter; left time: 5798.7091s\n\titers: 1000, epoch: 12 | loss: 0.1434859\n\tspeed: 0.0296s/iter; left time: 5855.6703s\n\titers: 1100, epoch: 12 | loss: 0.1642510\n\tspeed: 0.0291s/iter; left time: 5751.4936s\n\titers: 1200, epoch: 12 | loss: 0.1896415\n\tspeed: 0.0293s/iter; left time: 5789.9622s\n\titers: 1300, epoch: 12 | loss: 0.1665654\n\tspeed: 0.0288s/iter; left time: 5692.7568s\n\titers: 1400, epoch: 12 | loss: 0.1808118\n\tspeed: 0.0289s/iter; left time: 5701.8141s\n\titers: 1500, epoch: 12 | loss: 0.1609049\n\tspeed: 0.0295s/iter; left time: 5814.8987s\n\titers: 1600, epoch: 12 | loss: 0.1572420\n\tspeed: 0.0300s/iter; left time: 5908.4702s\n\titers: 1700, epoch: 12 | loss: 0.1803677\n\tspeed: 0.0293s/iter; left time: 5775.2834s\n\titers: 1800, epoch: 12 | loss: 0.1543010\n\tspeed: 0.0287s/iter; left time: 5648.5533s\n\titers: 1900, epoch: 12 | loss: 0.1587817\n\tspeed: 0.0290s/iter; left time: 5700.8427s\n\titers: 2000, epoch: 12 | loss: 0.1644964\n\tspeed: 0.0282s/iter; left time: 5556.1698s\n\titers: 2100, epoch: 12 | loss: 0.1832137\n\tspeed: 0.0295s/iter; left time: 5794.3359s\n\titers: 2200, epoch: 12 | loss: 0.1819673\n\tspeed: 0.0286s/iter; left time: 5628.5055s\n[TRAIN MEMORY] Max memory allocated in epoch 12: 128.39 MB\nEpoch: 12 cost time: 65.37430930137634\nEpoch: 12, Steps: 2233 | Train Loss: 0.1693132 Vali Loss: 0.0447130 Test Loss: 0.2081997\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 0.0014378602872914612\n\titers: 100, epoch: 13 | loss: 0.1464823\n\tspeed: 0.2865s/iter; left time: 56261.4836s\n\titers: 200, epoch: 13 | loss: 0.1923385\n\tspeed: 0.0293s/iter; left time: 5756.5887s\n\titers: 300, epoch: 13 | loss: 0.1865792\n\tspeed: 0.0288s/iter; left time: 5659.7742s\n\titers: 400, epoch: 13 | loss: 0.1500838\n\tspeed: 0.0285s/iter; left time: 5597.0959s\n\titers: 500, epoch: 13 | loss: 0.1753526\n\tspeed: 0.0286s/iter; left time: 5598.0602s\n\titers: 600, epoch: 13 | loss: 0.1812125\n\tspeed: 0.0289s/iter; left time: 5663.1942s\n\titers: 700, epoch: 13 | loss: 0.1826888\n\tspeed: 0.0288s/iter; left time: 5639.7300s\n\titers: 800, epoch: 13 | loss: 0.1694135\n\tspeed: 0.0294s/iter; left time: 5754.6038s\n\titers: 900, epoch: 13 | loss: 0.1597915\n\tspeed: 0.0294s/iter; left time: 5760.3111s\n\titers: 1000, epoch: 13 | loss: 0.1600135\n\tspeed: 0.0289s/iter; left time: 5641.4365s\n\titers: 1100, epoch: 13 | loss: 0.2034080\n\tspeed: 0.0296s/iter; left time: 5782.9217s\n\titers: 1200, epoch: 13 | loss: 0.1617675\n\tspeed: 0.0287s/iter; left time: 5613.7252s\n\titers: 1300, epoch: 13 | loss: 0.1666723\n\tspeed: 0.0290s/iter; left time: 5656.3251s\n\titers: 1400, epoch: 13 | loss: 0.1821029\n\tspeed: 0.0283s/iter; left time: 5515.8056s\n\titers: 1500, epoch: 13 | loss: 0.1715751\n\tspeed: 0.0281s/iter; left time: 5478.1317s\n\titers: 1600, epoch: 13 | loss: 0.1883976\n\tspeed: 0.0281s/iter; left time: 5484.2831s\n\titers: 1700, epoch: 13 | loss: 0.2018658\n\tspeed: 0.0282s/iter; left time: 5491.0184s\n\titers: 1800, epoch: 13 | loss: 0.1738108\n\tspeed: 0.0285s/iter; left time: 5548.1848s\n\titers: 1900, epoch: 13 | loss: 0.1485071\n\tspeed: 0.0288s/iter; left time: 5613.7376s\n\titers: 2000, epoch: 13 | loss: 0.1842942\n\tspeed: 0.0279s/iter; left time: 5417.2960s\n\titers: 2100, epoch: 13 | loss: 0.1627281\n\tspeed: 0.0278s/iter; left time: 5413.9746s\n\titers: 2200, epoch: 13 | loss: 0.1866180\n\tspeed: 0.0283s/iter; left time: 5490.2708s\n[TRAIN MEMORY] Max memory allocated in epoch 13: 128.39 MB\nEpoch: 13 cost time: 64.51834893226624\nEpoch: 13, Steps: 2233 | Train Loss: 0.1687536 Vali Loss: 0.0435260 Test Loss: 0.2136956\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 0.001609664253459064\n\titers: 100, epoch: 14 | loss: 0.1635327\n\tspeed: 0.2827s/iter; left time: 54887.8135s\n\titers: 200, epoch: 14 | loss: 0.1868727\n\tspeed: 0.0284s/iter; left time: 5520.4587s\n\titers: 300, epoch: 14 | loss: 0.1527397\n\tspeed: 0.0288s/iter; left time: 5581.3432s\n\titers: 400, epoch: 14 | loss: 0.1569381\n\tspeed: 0.0288s/iter; left time: 5587.9600s\n\titers: 500, epoch: 14 | loss: 0.1623251\n\tspeed: 0.0280s/iter; left time: 5422.3658s\n\titers: 600, epoch: 14 | loss: 0.1781730\n\tspeed: 0.0284s/iter; left time: 5504.4021s\n\titers: 700, epoch: 14 | loss: 0.1669876\n\tspeed: 0.0275s/iter; left time: 5329.7064s\n\titers: 800, epoch: 14 | loss: 0.1755518\n\tspeed: 0.0281s/iter; left time: 5438.9580s\n\titers: 900, epoch: 14 | loss: 0.1920852\n\tspeed: 0.0279s/iter; left time: 5392.6352s\n\titers: 1000, epoch: 14 | loss: 0.1788883\n\tspeed: 0.0280s/iter; left time: 5404.2386s\n\titers: 1100, epoch: 14 | loss: 0.1667249\n\tspeed: 0.0289s/iter; left time: 5581.7393s\n\titers: 1200, epoch: 14 | loss: 0.1639337\n\tspeed: 0.0285s/iter; left time: 5500.8693s\n\titers: 1300, epoch: 14 | loss: 0.1706254\n\tspeed: 0.0293s/iter; left time: 5662.8383s\n\titers: 1400, epoch: 14 | loss: 0.2006995\n\tspeed: 0.0284s/iter; left time: 5473.5277s\n\titers: 1500, epoch: 14 | loss: 0.1724359\n\tspeed: 0.0294s/iter; left time: 5658.9478s\n\titers: 1600, epoch: 14 | loss: 0.1598672\n\tspeed: 0.0286s/iter; left time: 5517.1046s\n\titers: 1700, epoch: 14 | loss: 0.1662305\n\tspeed: 0.0290s/iter; left time: 5587.2286s\n\titers: 1800, epoch: 14 | loss: 0.1789066\n\tspeed: 0.0284s/iter; left time: 5469.4214s\n\titers: 1900, epoch: 14 | loss: 0.1572763\n\tspeed: 0.0296s/iter; left time: 5692.0946s\n\titers: 2000, epoch: 14 | loss: 0.2011320\n\tspeed: 0.0282s/iter; left time: 5416.3406s\n\titers: 2100, epoch: 14 | loss: 0.1688489\n\tspeed: 0.0278s/iter; left time: 5345.0092s\n\titers: 2200, epoch: 14 | loss: 0.1675372\n\tspeed: 0.0287s/iter; left time: 5520.5588s\n[TRAIN MEMORY] Max memory allocated in epoch 14: 128.39 MB\nEpoch: 14 cost time: 64.14766836166382\nEpoch: 14, Steps: 2233 | Train Loss: 0.1678719 Vali Loss: 0.0443618 Test Loss: 0.2173963\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 0.0017348203203004352\n\titers: 100, epoch: 15 | loss: 0.1596643\n\tspeed: 0.2804s/iter; left time: 53810.2604s\n\titers: 200, epoch: 15 | loss: 0.1618849\n\tspeed: 0.0292s/iter; left time: 5609.0971s\n\titers: 300, epoch: 15 | loss: 0.1778154\n\tspeed: 0.0282s/iter; left time: 5399.9051s\n\titers: 400, epoch: 15 | loss: 0.2205917\n\tspeed: 0.0280s/iter; left time: 5368.9559s\n\titers: 500, epoch: 15 | loss: 0.1525986\n\tspeed: 0.0285s/iter; left time: 5467.8293s\n\titers: 600, epoch: 15 | loss: 0.1848874\n\tspeed: 0.0286s/iter; left time: 5477.1073s\n\titers: 700, epoch: 15 | loss: 0.1714377\n\tspeed: 0.0277s/iter; left time: 5304.4423s\n\titers: 800, epoch: 15 | loss: 0.1640569\n\tspeed: 0.0279s/iter; left time: 5337.3909s\n\titers: 900, epoch: 15 | loss: 0.1532099\n\tspeed: 0.0287s/iter; left time: 5489.9573s\n\titers: 1000, epoch: 15 | loss: 0.1926685\n\tspeed: 0.0286s/iter; left time: 5465.6578s\n\titers: 1100, epoch: 15 | loss: 0.1574395\n\tspeed: 0.0282s/iter; left time: 5386.5674s\n\titers: 1200, epoch: 15 | loss: 0.1923196\n\tspeed: 0.0285s/iter; left time: 5442.9562s\n\titers: 1300, epoch: 15 | loss: 0.1834018\n\tspeed: 0.0286s/iter; left time: 5450.8832s\n\titers: 1400, epoch: 15 | loss: 0.1471387\n\tspeed: 0.0288s/iter; left time: 5494.5673s\n\titers: 1500, epoch: 15 | loss: 0.1537397\n\tspeed: 0.0284s/iter; left time: 5409.3608s\n\titers: 1600, epoch: 15 | loss: 0.1656085\n\tspeed: 0.0285s/iter; left time: 5435.0244s\n\titers: 1700, epoch: 15 | loss: 0.1723160\n\tspeed: 0.0290s/iter; left time: 5511.6300s\n\titers: 1800, epoch: 15 | loss: 0.1820111\n\tspeed: 0.0283s/iter; left time: 5378.4863s\n\titers: 1900, epoch: 15 | loss: 0.1453558\n\tspeed: 0.0285s/iter; left time: 5412.6370s\n\titers: 2000, epoch: 15 | loss: 0.1719375\n\tspeed: 0.0287s/iter; left time: 5460.6549s\n\titers: 2100, epoch: 15 | loss: 0.1700520\n\tspeed: 0.0286s/iter; left time: 5427.1833s\n\titers: 2200, epoch: 15 | loss: 0.1964984\n\tspeed: 0.0291s/iter; left time: 5520.7775s\n[TRAIN MEMORY] Max memory allocated in epoch 15: 128.39 MB\nEpoch: 15 cost time: 64.1597511768341\nEpoch: 15, Steps: 2233 | Train Loss: 0.1668726 Vali Loss: 0.0444055 Test Loss: 0.2180771\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 0.001820156385871022\n\titers: 100, epoch: 16 | loss: 0.1667373\n\tspeed: 0.2876s/iter; left time: 54558.4569s\n\titers: 200, epoch: 16 | loss: 0.1680118\n\tspeed: 0.0293s/iter; left time: 5556.4608s\n\titers: 300, epoch: 16 | loss: 0.1628082\n\tspeed: 0.0287s/iter; left time: 5434.5017s\n\titers: 400, epoch: 16 | loss: 0.1535726\n\tspeed: 0.0287s/iter; left time: 5441.4680s\n\titers: 500, epoch: 16 | loss: 0.1921868\n\tspeed: 0.0287s/iter; left time: 5424.4424s\n\titers: 600, epoch: 16 | loss: 0.1568506\n\tspeed: 0.0286s/iter; left time: 5411.9866s\n\titers: 700, epoch: 16 | loss: 0.1584495\n\tspeed: 0.0285s/iter; left time: 5392.6728s\n\titers: 800, epoch: 16 | loss: 0.1530770\n\tspeed: 0.0284s/iter; left time: 5369.6466s\n\titers: 900, epoch: 16 | loss: 0.1967391\n\tspeed: 0.0286s/iter; left time: 5399.0574s\n\titers: 1000, epoch: 16 | loss: 0.1523409\n\tspeed: 0.0286s/iter; left time: 5392.7102s\n\titers: 1100, epoch: 16 | loss: 0.1661799\n\tspeed: 0.0282s/iter; left time: 5323.0694s\n\titers: 1200, epoch: 16 | loss: 0.1480583\n\tspeed: 0.0290s/iter; left time: 5471.7605s\n\titers: 1300, epoch: 16 | loss: 0.1724168\n\tspeed: 0.0297s/iter; left time: 5606.0358s\n\titers: 1400, epoch: 16 | loss: 0.1605427\n\tspeed: 0.0297s/iter; left time: 5600.3371s\n\titers: 1500, epoch: 16 | loss: 0.1968263\n\tspeed: 0.0290s/iter; left time: 5457.1959s\n\titers: 1600, epoch: 16 | loss: 0.1601210\n\tspeed: 0.0285s/iter; left time: 5372.9251s\n\titers: 1700, epoch: 16 | loss: 0.2174193\n\tspeed: 0.0280s/iter; left time: 5260.9022s\n\titers: 1800, epoch: 16 | loss: 0.1634605\n\tspeed: 0.0284s/iter; left time: 5347.0168s\n\titers: 1900, epoch: 16 | loss: 0.1569659\n\tspeed: 0.0284s/iter; left time: 5341.6594s\n\titers: 2000, epoch: 16 | loss: 0.1851989\n\tspeed: 0.0288s/iter; left time: 5417.7428s\n\titers: 2100, epoch: 16 | loss: 0.1575055\n\tspeed: 0.0286s/iter; left time: 5372.5136s\n\titers: 2200, epoch: 16 | loss: 0.1702965\n\tspeed: 0.0292s/iter; left time: 5484.8387s\n[TRAIN MEMORY] Max memory allocated in epoch 16: 128.39 MB\nEpoch: 16 cost time: 64.65884399414062\nEpoch: 16, Steps: 2233 | Train Loss: 0.1655055 Vali Loss: 0.0448264 Test Loss: 0.2218751\nEarlyStopping counter: 10 out of 10\nEarly stopping\nAverage epoch time: 64.98 seconds\n>>>>>>>testing : transformer_test_xPatch_Solar_ftM_sl720_ll48_pl336_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 10177\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 4.75 MB\n[PARAMS] Total parameters: 1,242,873\nmse:0.19790540635585785, mae:0.21375037729740143\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data Solar --learning_rate 0.003 --data_path solar.txt --seq_len 720 --pred_len 720 --patience 5 --enc_in 137 --dropout 0 --batch_size 16 --lradj 'sigmoid' --period_len 6 --d_model 128 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T05:30:51.095440Z","iopub.execute_input":"2025-10-22T05:30:51.095763Z","iopub.status.idle":"2025-10-22T05:50:18.954026Z","shell.execute_reply.started":"2025-10-22T05:30:51.095742Z","shell.execute_reply":"2025-10-22T05:50:18.953238Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=6, d_model=128, data='Solar', root_path='./dataset', data_path='solar.txt', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=720, label_len=48, pred_len=720, enc_in=137, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=16, patience=5, learning_rate=0.003, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.0, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_Solar_ftM_sl720_ll48_pl720_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 35353\nval 4537\ntest 9793\n\titers: 100, epoch: 1 | loss: 0.2864817\n\tspeed: 0.0591s/iter; left time: 13059.4041s\n\titers: 200, epoch: 1 | loss: 0.2613615\n\tspeed: 0.0356s/iter; left time: 7849.7040s\n\titers: 300, epoch: 1 | loss: 0.2527219\n\tspeed: 0.0353s/iter; left time: 7784.7422s\n\titers: 400, epoch: 1 | loss: 0.2049317\n\tspeed: 0.0339s/iter; left time: 7464.2117s\n\titers: 500, epoch: 1 | loss: 0.2209897\n\tspeed: 0.0342s/iter; left time: 7546.6096s\n\titers: 600, epoch: 1 | loss: 0.1955116\n\tspeed: 0.0351s/iter; left time: 7733.8740s\n\titers: 700, epoch: 1 | loss: 0.2135805\n\tspeed: 0.0356s/iter; left time: 7828.9227s\n\titers: 800, epoch: 1 | loss: 0.2144139\n\tspeed: 0.0351s/iter; left time: 7718.4618s\n\titers: 900, epoch: 1 | loss: 0.1922523\n\tspeed: 0.0355s/iter; left time: 7800.9475s\n\titers: 1000, epoch: 1 | loss: 0.2014076\n\tspeed: 0.0348s/iter; left time: 7663.3514s\n\titers: 1100, epoch: 1 | loss: 0.1904534\n\tspeed: 0.0355s/iter; left time: 7799.7309s\n\titers: 1200, epoch: 1 | loss: 0.1673407\n\tspeed: 0.0343s/iter; left time: 7544.4905s\n\titers: 1300, epoch: 1 | loss: 0.1811885\n\tspeed: 0.0347s/iter; left time: 7615.5997s\n\titers: 1400, epoch: 1 | loss: 0.1843882\n\tspeed: 0.0355s/iter; left time: 7790.0079s\n\titers: 1500, epoch: 1 | loss: 0.1989372\n\tspeed: 0.0355s/iter; left time: 7799.6026s\n\titers: 1600, epoch: 1 | loss: 0.1807409\n\tspeed: 0.0349s/iter; left time: 7645.4573s\n\titers: 1700, epoch: 1 | loss: 0.1898749\n\tspeed: 0.0354s/iter; left time: 7762.5144s\n\titers: 1800, epoch: 1 | loss: 0.1901316\n\tspeed: 0.0353s/iter; left time: 7733.2811s\n\titers: 1900, epoch: 1 | loss: 0.1925713\n\tspeed: 0.0349s/iter; left time: 7638.2978s\n\titers: 2000, epoch: 1 | loss: 0.1761373\n\tspeed: 0.0358s/iter; left time: 7831.3369s\n\titers: 2100, epoch: 1 | loss: 0.1776799\n\tspeed: 0.0350s/iter; left time: 7662.4743s\n\titers: 2200, epoch: 1 | loss: 0.1873500\n\tspeed: 0.0347s/iter; left time: 7587.7881s\n[TRAIN MEMORY] Max memory allocated in epoch 1: 203.55 MB\nEpoch: 1 cost time: 78.77231478691101\nEpoch: 1, Steps: 2209 | Train Loss: 0.2108898 Vali Loss: 0.0460274 Test Loss: 0.2205143\nValidation loss decreased (inf --> 0.046027).  Saving model ...\nUpdating learning rate to 1.186006642679406e-05\n\titers: 100, epoch: 2 | loss: 0.1528205\n\tspeed: 0.3484s/iter; left time: 76168.0729s\n\titers: 200, epoch: 2 | loss: 0.1651942\n\tspeed: 0.0350s/iter; left time: 7651.4176s\n\titers: 300, epoch: 2 | loss: 0.1937676\n\tspeed: 0.0351s/iter; left time: 7673.1586s\n\titers: 400, epoch: 2 | loss: 0.1676256\n\tspeed: 0.0353s/iter; left time: 7712.8106s\n\titers: 500, epoch: 2 | loss: 0.1615831\n\tspeed: 0.0350s/iter; left time: 7641.6334s\n\titers: 600, epoch: 2 | loss: 0.1764867\n\tspeed: 0.0357s/iter; left time: 7789.4367s\n\titers: 700, epoch: 2 | loss: 0.1619567\n\tspeed: 0.0351s/iter; left time: 7641.3386s\n\titers: 800, epoch: 2 | loss: 0.1844101\n\tspeed: 0.0355s/iter; left time: 7741.3884s\n\titers: 900, epoch: 2 | loss: 0.1594439\n\tspeed: 0.0356s/iter; left time: 7744.4540s\n\titers: 1000, epoch: 2 | loss: 0.1619802\n\tspeed: 0.0352s/iter; left time: 7663.4954s\n\titers: 1100, epoch: 2 | loss: 0.1802998\n\tspeed: 0.0354s/iter; left time: 7695.0051s\n\titers: 1200, epoch: 2 | loss: 0.1792366\n\tspeed: 0.0360s/iter; left time: 7828.6555s\n\titers: 1300, epoch: 2 | loss: 0.1825488\n\tspeed: 0.0356s/iter; left time: 7729.4172s\n\titers: 1400, epoch: 2 | loss: 0.1742697\n\tspeed: 0.0355s/iter; left time: 7704.7708s\n\titers: 1500, epoch: 2 | loss: 0.1685516\n\tspeed: 0.0365s/iter; left time: 7937.9129s\n\titers: 1600, epoch: 2 | loss: 0.1820384\n\tspeed: 0.0351s/iter; left time: 7613.7399s\n\titers: 1700, epoch: 2 | loss: 0.1743920\n\tspeed: 0.0353s/iter; left time: 7661.2211s\n\titers: 1800, epoch: 2 | loss: 0.1607082\n\tspeed: 0.0360s/iter; left time: 7805.0758s\n\titers: 1900, epoch: 2 | loss: 0.1588827\n\tspeed: 0.0351s/iter; left time: 7608.1763s\n\titers: 2000, epoch: 2 | loss: 0.1844743\n\tspeed: 0.0352s/iter; left time: 7626.5861s\n\titers: 2100, epoch: 2 | loss: 0.1835326\n\tspeed: 0.0355s/iter; left time: 7694.4272s\n\titers: 2200, epoch: 2 | loss: 0.1635159\n\tspeed: 0.0347s/iter; left time: 7518.8798s\n[TRAIN MEMORY] Max memory allocated in epoch 2: 203.15 MB\nEpoch: 2 cost time: 78.74126195907593\nEpoch: 2, Steps: 2209 | Train Loss: 0.1770385 Vali Loss: 0.0414898 Test Loss: 0.2162723\nValidation loss decreased (0.046027 --> 0.041490).  Saving model ...\nUpdating learning rate to 3.1784005853428764e-05\n\titers: 100, epoch: 3 | loss: 0.1701164\n\tspeed: 0.3536s/iter; left time: 76517.1202s\n\titers: 200, epoch: 3 | loss: 0.1984788\n\tspeed: 0.0352s/iter; left time: 7618.9457s\n\titers: 300, epoch: 3 | loss: 0.2031756\n\tspeed: 0.0363s/iter; left time: 7850.2151s\n\titers: 400, epoch: 3 | loss: 0.1694027\n\tspeed: 0.0348s/iter; left time: 7523.5193s\n\titers: 500, epoch: 3 | loss: 0.1653732\n\tspeed: 0.0346s/iter; left time: 7471.1771s\n\titers: 600, epoch: 3 | loss: 0.1905866\n\tspeed: 0.0349s/iter; left time: 7536.1697s\n\titers: 700, epoch: 3 | loss: 0.1524705\n\tspeed: 0.0347s/iter; left time: 7488.8565s\n\titers: 800, epoch: 3 | loss: 0.1642497\n\tspeed: 0.0347s/iter; left time: 7482.9609s\n\titers: 900, epoch: 3 | loss: 0.1855947\n\tspeed: 0.0355s/iter; left time: 7650.8439s\n\titers: 1000, epoch: 3 | loss: 0.1909509\n\tspeed: 0.0350s/iter; left time: 7546.6752s\n\titers: 1100, epoch: 3 | loss: 0.1909409\n\tspeed: 0.0354s/iter; left time: 7625.5944s\n\titers: 1200, epoch: 3 | loss: 0.1865851\n\tspeed: 0.0356s/iter; left time: 7665.4318s\n\titers: 1300, epoch: 3 | loss: 0.1843413\n\tspeed: 0.0352s/iter; left time: 7566.5706s\n\titers: 1400, epoch: 3 | loss: 0.1626918\n\tspeed: 0.0349s/iter; left time: 7501.5287s\n\titers: 1500, epoch: 3 | loss: 0.1830623\n\tspeed: 0.0355s/iter; left time: 7629.6832s\n\titers: 1600, epoch: 3 | loss: 0.1603836\n\tspeed: 0.0365s/iter; left time: 7836.3503s\n\titers: 1700, epoch: 3 | loss: 0.1882346\n\tspeed: 0.0357s/iter; left time: 7673.3203s\n\titers: 1800, epoch: 3 | loss: 0.1852133\n\tspeed: 0.0353s/iter; left time: 7585.3794s\n\titers: 1900, epoch: 3 | loss: 0.2117420\n\tspeed: 0.0348s/iter; left time: 7457.3638s\n\titers: 2000, epoch: 3 | loss: 0.1541167\n\tspeed: 0.0358s/iter; left time: 7679.1514s\n\titers: 2100, epoch: 3 | loss: 0.1695745\n\tspeed: 0.0350s/iter; left time: 7505.7281s\n\titers: 2200, epoch: 3 | loss: 0.1711652\n\tspeed: 0.0346s/iter; left time: 7407.2749s\n[TRAIN MEMORY] Max memory allocated in epoch 3: 203.15 MB\nEpoch: 3 cost time: 78.38233804702759\nEpoch: 3, Steps: 2209 | Train Loss: 0.1735804 Vali Loss: 0.0409006 Test Loss: 0.2144555\nValidation loss decreased (0.041490 --> 0.040901).  Saving model ...\nUpdating learning rate to 6.463398202002396e-05\n\titers: 100, epoch: 4 | loss: 0.1786573\n\tspeed: 0.3542s/iter; left time: 75870.0625s\n\titers: 200, epoch: 4 | loss: 0.1772728\n\tspeed: 0.0351s/iter; left time: 7512.5540s\n\titers: 300, epoch: 4 | loss: 0.1810549\n\tspeed: 0.0351s/iter; left time: 7508.3177s\n\titers: 400, epoch: 4 | loss: 0.1583730\n\tspeed: 0.0348s/iter; left time: 7438.1714s\n\titers: 500, epoch: 4 | loss: 0.1792061\n\tspeed: 0.0342s/iter; left time: 7306.3988s\n\titers: 600, epoch: 4 | loss: 0.1550765\n\tspeed: 0.0356s/iter; left time: 7614.1185s\n\titers: 700, epoch: 4 | loss: 0.1637859\n\tspeed: 0.0350s/iter; left time: 7480.7416s\n\titers: 800, epoch: 4 | loss: 0.1944980\n\tspeed: 0.0354s/iter; left time: 7552.0604s\n\titers: 900, epoch: 4 | loss: 0.1625533\n\tspeed: 0.0356s/iter; left time: 7596.8524s\n\titers: 1000, epoch: 4 | loss: 0.1776511\n\tspeed: 0.0351s/iter; left time: 7489.3146s\n\titers: 1100, epoch: 4 | loss: 0.1572523\n\tspeed: 0.0349s/iter; left time: 7446.7117s\n\titers: 1200, epoch: 4 | loss: 0.1561659\n\tspeed: 0.0359s/iter; left time: 7651.3427s\n\titers: 1300, epoch: 4 | loss: 0.1847069\n\tspeed: 0.0356s/iter; left time: 7574.4598s\n\titers: 1400, epoch: 4 | loss: 0.1610918\n\tspeed: 0.0357s/iter; left time: 7589.0714s\n\titers: 1500, epoch: 4 | loss: 0.1879075\n\tspeed: 0.0357s/iter; left time: 7590.3561s\n\titers: 1600, epoch: 4 | loss: 0.1869283\n\tspeed: 0.0359s/iter; left time: 7626.0180s\n\titers: 1700, epoch: 4 | loss: 0.1913047\n\tspeed: 0.0351s/iter; left time: 7455.1784s\n\titers: 1800, epoch: 4 | loss: 0.1478698\n\tspeed: 0.0359s/iter; left time: 7629.0935s\n\titers: 1900, epoch: 4 | loss: 0.1585545\n\tspeed: 0.0362s/iter; left time: 7677.6219s\n\titers: 2000, epoch: 4 | loss: 0.1833599\n\tspeed: 0.0354s/iter; left time: 7521.0996s\n\titers: 2100, epoch: 4 | loss: 0.2010946\n\tspeed: 0.0356s/iter; left time: 7557.9738s\n\titers: 2200, epoch: 4 | loss: 0.1578189\n\tspeed: 0.0350s/iter; left time: 7428.2737s\n[TRAIN MEMORY] Max memory allocated in epoch 4: 203.15 MB\nEpoch: 4 cost time: 78.75036430358887\nEpoch: 4, Steps: 2209 | Train Loss: 0.1717270 Vali Loss: 0.0406610 Test Loss: 0.2091950\nValidation loss decreased (0.040901 --> 0.040661).  Saving model ...\nUpdating learning rate to 0.00011778990607322068\n\titers: 100, epoch: 5 | loss: 0.1436793\n\tspeed: 0.3548s/iter; left time: 75195.6595s\n\titers: 200, epoch: 5 | loss: 0.1656464\n\tspeed: 0.0347s/iter; left time: 7356.1743s\n\titers: 300, epoch: 5 | loss: 0.2008631\n\tspeed: 0.0349s/iter; left time: 7384.8246s\n\titers: 400, epoch: 5 | loss: 0.1897290\n\tspeed: 0.0355s/iter; left time: 7524.4209s\n\titers: 500, epoch: 5 | loss: 0.1829821\n\tspeed: 0.0352s/iter; left time: 7437.0512s\n\titers: 600, epoch: 5 | loss: 0.1812416\n\tspeed: 0.0356s/iter; left time: 7522.2324s\n\titers: 700, epoch: 5 | loss: 0.1828963\n\tspeed: 0.0351s/iter; left time: 7423.1395s\n\titers: 800, epoch: 5 | loss: 0.1704563\n\tspeed: 0.0348s/iter; left time: 7357.6105s\n\titers: 900, epoch: 5 | loss: 0.1934501\n\tspeed: 0.0356s/iter; left time: 7507.9122s\n\titers: 1000, epoch: 5 | loss: 0.1920570\n\tspeed: 0.0352s/iter; left time: 7427.5611s\n\titers: 1100, epoch: 5 | loss: 0.1737924\n\tspeed: 0.0351s/iter; left time: 7414.2767s\n\titers: 1200, epoch: 5 | loss: 0.1871108\n\tspeed: 0.0359s/iter; left time: 7574.0305s\n\titers: 1300, epoch: 5 | loss: 0.1778345\n\tspeed: 0.0353s/iter; left time: 7438.7678s\n\titers: 1400, epoch: 5 | loss: 0.1848933\n\tspeed: 0.0357s/iter; left time: 7514.9799s\n\titers: 1500, epoch: 5 | loss: 0.1700913\n\tspeed: 0.0358s/iter; left time: 7546.0168s\n\titers: 1600, epoch: 5 | loss: 0.1512210\n\tspeed: 0.0350s/iter; left time: 7357.2714s\n\titers: 1700, epoch: 5 | loss: 0.1548009\n\tspeed: 0.0354s/iter; left time: 7436.9235s\n\titers: 1800, epoch: 5 | loss: 0.1786379\n\tspeed: 0.0350s/iter; left time: 7361.9916s\n\titers: 1900, epoch: 5 | loss: 0.1984498\n\tspeed: 0.0352s/iter; left time: 7407.6345s\n\titers: 2000, epoch: 5 | loss: 0.1781362\n\tspeed: 0.0348s/iter; left time: 7301.7268s\n\titers: 2100, epoch: 5 | loss: 0.1565959\n\tspeed: 0.0362s/iter; left time: 7607.9043s\n\titers: 2200, epoch: 5 | loss: 0.1760040\n\tspeed: 0.0350s/iter; left time: 7339.2897s\n[TRAIN MEMORY] Max memory allocated in epoch 5: 203.15 MB\nEpoch: 5 cost time: 78.45298457145691\nEpoch: 5, Steps: 2209 | Train Loss: 0.1702928 Vali Loss: 0.0406197 Test Loss: 0.2062216\nValidation loss decreased (0.040661 --> 0.040620).  Saving model ...\nUpdating learning rate to 0.0002018420838225947\n\titers: 100, epoch: 6 | loss: 0.1753158\n\tspeed: 0.3580s/iter; left time: 75082.3659s\n\titers: 200, epoch: 6 | loss: 0.1675243\n\tspeed: 0.0346s/iter; left time: 7248.8457s\n\titers: 300, epoch: 6 | loss: 0.1755595\n\tspeed: 0.0346s/iter; left time: 7253.8732s\n\titers: 400, epoch: 6 | loss: 0.1598158\n\tspeed: 0.0355s/iter; left time: 7433.7573s\n\titers: 500, epoch: 6 | loss: 0.1744267\n\tspeed: 0.0352s/iter; left time: 7373.1885s\n\titers: 600, epoch: 6 | loss: 0.1724633\n\tspeed: 0.0354s/iter; left time: 7410.3055s\n\titers: 700, epoch: 6 | loss: 0.1641971\n\tspeed: 0.0346s/iter; left time: 7238.5869s\n\titers: 800, epoch: 6 | loss: 0.1363764\n\tspeed: 0.0351s/iter; left time: 7335.1337s\n\titers: 900, epoch: 6 | loss: 0.1711111\n\tspeed: 0.0349s/iter; left time: 7296.3890s\n\titers: 1000, epoch: 6 | loss: 0.1709605\n\tspeed: 0.0354s/iter; left time: 7399.5255s\n\titers: 1100, epoch: 6 | loss: 0.1570213\n\tspeed: 0.0352s/iter; left time: 7350.5410s\n\titers: 1200, epoch: 6 | loss: 0.1696052\n\tspeed: 0.0353s/iter; left time: 7372.0070s\n\titers: 1300, epoch: 6 | loss: 0.1713275\n\tspeed: 0.0357s/iter; left time: 7446.5002s\n\titers: 1400, epoch: 6 | loss: 0.1638295\n\tspeed: 0.0355s/iter; left time: 7393.1748s\n\titers: 1500, epoch: 6 | loss: 0.1802663\n\tspeed: 0.0359s/iter; left time: 7477.7165s\n\titers: 1600, epoch: 6 | loss: 0.1714241\n\tspeed: 0.0352s/iter; left time: 7337.7531s\n\titers: 1700, epoch: 6 | loss: 0.1792775\n\tspeed: 0.0347s/iter; left time: 7223.7597s\n\titers: 1800, epoch: 6 | loss: 0.1825906\n\tspeed: 0.0352s/iter; left time: 7325.8078s\n\titers: 1900, epoch: 6 | loss: 0.1648191\n\tspeed: 0.0353s/iter; left time: 7334.1583s\n\titers: 2000, epoch: 6 | loss: 0.1667002\n\tspeed: 0.0358s/iter; left time: 7434.3979s\n\titers: 2100, epoch: 6 | loss: 0.1835607\n\tspeed: 0.0351s/iter; left time: 7298.0780s\n\titers: 2200, epoch: 6 | loss: 0.1774422\n\tspeed: 0.0337s/iter; left time: 6998.0154s\n[TRAIN MEMORY] Max memory allocated in epoch 6: 203.15 MB\nEpoch: 6 cost time: 78.14548254013062\nEpoch: 6, Steps: 2209 | Train Loss: 0.1694565 Vali Loss: 0.0406922 Test Loss: 0.2043097\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 0.00033056887010780915\n\titers: 100, epoch: 7 | loss: 0.1826570\n\tspeed: 0.3517s/iter; left time: 72989.8335s\n\titers: 200, epoch: 7 | loss: 0.1638743\n\tspeed: 0.0351s/iter; left time: 7290.6316s\n\titers: 300, epoch: 7 | loss: 0.1558323\n\tspeed: 0.0353s/iter; left time: 7311.0212s\n\titers: 400, epoch: 7 | loss: 0.1537454\n\tspeed: 0.0356s/iter; left time: 7374.0332s\n\titers: 500, epoch: 7 | loss: 0.1805025\n\tspeed: 0.0346s/iter; left time: 7157.4364s\n\titers: 600, epoch: 7 | loss: 0.1646810\n\tspeed: 0.0356s/iter; left time: 7369.9490s\n\titers: 700, epoch: 7 | loss: 0.1779498\n\tspeed: 0.0358s/iter; left time: 7411.3180s\n\titers: 800, epoch: 7 | loss: 0.1596342\n\tspeed: 0.0355s/iter; left time: 7342.8001s\n\titers: 900, epoch: 7 | loss: 0.1784186\n\tspeed: 0.0355s/iter; left time: 7335.3374s\n\titers: 1000, epoch: 7 | loss: 0.1675580\n\tspeed: 0.0358s/iter; left time: 7404.6679s\n\titers: 1100, epoch: 7 | loss: 0.1409827\n\tspeed: 0.0358s/iter; left time: 7400.1229s\n\titers: 1200, epoch: 7 | loss: 0.1878057\n\tspeed: 0.0362s/iter; left time: 7465.8760s\n\titers: 1300, epoch: 7 | loss: 0.1923392\n\tspeed: 0.0348s/iter; left time: 7172.4959s\n\titers: 1400, epoch: 7 | loss: 0.1796339\n\tspeed: 0.0345s/iter; left time: 7124.3388s\n\titers: 1500, epoch: 7 | loss: 0.1636339\n\tspeed: 0.0352s/iter; left time: 7249.9181s\n\titers: 1600, epoch: 7 | loss: 0.1875395\n\tspeed: 0.0355s/iter; left time: 7306.5843s\n\titers: 1700, epoch: 7 | loss: 0.1574652\n\tspeed: 0.0347s/iter; left time: 7137.7353s\n\titers: 1800, epoch: 7 | loss: 0.1497916\n\tspeed: 0.0348s/iter; left time: 7170.1296s\n\titers: 1900, epoch: 7 | loss: 0.1635046\n\tspeed: 0.0350s/iter; left time: 7204.0613s\n\titers: 2000, epoch: 7 | loss: 0.1819440\n\tspeed: 0.0354s/iter; left time: 7286.6213s\n\titers: 2100, epoch: 7 | loss: 0.1520768\n\tspeed: 0.0353s/iter; left time: 7246.1999s\n\titers: 2200, epoch: 7 | loss: 0.1640172\n\tspeed: 0.0340s/iter; left time: 6980.7256s\n[TRAIN MEMORY] Max memory allocated in epoch 7: 203.15 MB\nEpoch: 7 cost time: 78.38274097442627\nEpoch: 7, Steps: 2209 | Train Loss: 0.1693144 Vali Loss: 0.0408909 Test Loss: 0.2057835\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 0.0005188634406732307\n\titers: 100, epoch: 8 | loss: 0.1767499\n\tspeed: 0.3445s/iter; left time: 70737.8817s\n\titers: 200, epoch: 8 | loss: 0.1500901\n\tspeed: 0.0347s/iter; left time: 7123.4444s\n\titers: 300, epoch: 8 | loss: 0.1684887\n\tspeed: 0.0357s/iter; left time: 7317.0522s\n\titers: 400, epoch: 8 | loss: 0.1864567\n\tspeed: 0.0352s/iter; left time: 7219.9505s\n\titers: 500, epoch: 8 | loss: 0.1844003\n\tspeed: 0.0353s/iter; left time: 7243.8979s\n\titers: 600, epoch: 8 | loss: 0.1658984\n\tspeed: 0.0349s/iter; left time: 7158.9438s\n\titers: 700, epoch: 8 | loss: 0.1903790\n\tspeed: 0.0358s/iter; left time: 7324.0052s\n\titers: 800, epoch: 8 | loss: 0.1688828\n\tspeed: 0.0353s/iter; left time: 7230.6548s\n\titers: 900, epoch: 8 | loss: 0.1772814\n\tspeed: 0.0351s/iter; left time: 7175.8716s\n\titers: 1000, epoch: 8 | loss: 0.1652818\n\tspeed: 0.0350s/iter; left time: 7164.5188s\n\titers: 1100, epoch: 8 | loss: 0.1623560\n\tspeed: 0.0358s/iter; left time: 7307.4635s\n\titers: 1200, epoch: 8 | loss: 0.1519513\n\tspeed: 0.0355s/iter; left time: 7252.5875s\n\titers: 1300, epoch: 8 | loss: 0.1666423\n\tspeed: 0.0354s/iter; left time: 7235.6872s\n\titers: 1400, epoch: 8 | loss: 0.1648624\n\tspeed: 0.0345s/iter; left time: 7047.2186s\n\titers: 1500, epoch: 8 | loss: 0.1523701\n\tspeed: 0.0348s/iter; left time: 7101.0086s\n\titers: 1600, epoch: 8 | loss: 0.1821781\n\tspeed: 0.0361s/iter; left time: 7359.8584s\n\titers: 1700, epoch: 8 | loss: 0.1537632\n\tspeed: 0.0351s/iter; left time: 7143.1214s\n\titers: 1800, epoch: 8 | loss: 0.1678770\n\tspeed: 0.0357s/iter; left time: 7277.4548s\n\titers: 1900, epoch: 8 | loss: 0.1700716\n\tspeed: 0.0362s/iter; left time: 7365.3574s\n\titers: 2000, epoch: 8 | loss: 0.1764325\n\tspeed: 0.0355s/iter; left time: 7218.5470s\n\titers: 2100, epoch: 8 | loss: 0.1611587\n\tspeed: 0.0362s/iter; left time: 7360.5857s\n\titers: 2200, epoch: 8 | loss: 0.1823239\n\tspeed: 0.0346s/iter; left time: 7026.9272s\n[TRAIN MEMORY] Max memory allocated in epoch 8: 203.15 MB\nEpoch: 8 cost time: 78.57128286361694\nEpoch: 8, Steps: 2209 | Train Loss: 0.1696917 Vali Loss: 0.0414595 Test Loss: 0.2079694\nEarlyStopping counter: 3 out of 5\nUpdating learning rate to 0.0007769688585092725\n\titers: 100, epoch: 9 | loss: 0.1703838\n\tspeed: 0.3557s/iter; left time: 72252.9338s\n\titers: 200, epoch: 9 | loss: 0.1611914\n\tspeed: 0.0354s/iter; left time: 7195.4100s\n\titers: 300, epoch: 9 | loss: 0.1799820\n\tspeed: 0.0355s/iter; left time: 7199.8389s\n\titers: 400, epoch: 9 | loss: 0.1478735\n\tspeed: 0.0351s/iter; left time: 7121.3729s\n\titers: 500, epoch: 9 | loss: 0.1539128\n\tspeed: 0.0343s/iter; left time: 6957.6311s\n\titers: 600, epoch: 9 | loss: 0.1925600\n\tspeed: 0.0355s/iter; left time: 7186.6287s\n\titers: 700, epoch: 9 | loss: 0.1778295\n\tspeed: 0.0361s/iter; left time: 7308.2266s\n\titers: 800, epoch: 9 | loss: 0.1792084\n\tspeed: 0.0359s/iter; left time: 7269.0784s\n\titers: 900, epoch: 9 | loss: 0.1582829\n\tspeed: 0.0359s/iter; left time: 7268.6606s\n\titers: 1000, epoch: 9 | loss: 0.1779808\n\tspeed: 0.0344s/iter; left time: 6957.6464s\n\titers: 1100, epoch: 9 | loss: 0.1918601\n\tspeed: 0.0354s/iter; left time: 7164.5981s\n\titers: 1200, epoch: 9 | loss: 0.1656801\n\tspeed: 0.0349s/iter; left time: 7051.1623s\n\titers: 1300, epoch: 9 | loss: 0.1787969\n\tspeed: 0.0356s/iter; left time: 7192.7621s\n\titers: 1400, epoch: 9 | loss: 0.1960109\n\tspeed: 0.0354s/iter; left time: 7138.1301s\n\titers: 1500, epoch: 9 | loss: 0.1788686\n\tspeed: 0.0357s/iter; left time: 7207.1095s\n\titers: 1600, epoch: 9 | loss: 0.1858006\n\tspeed: 0.0356s/iter; left time: 7179.2883s\n\titers: 1700, epoch: 9 | loss: 0.1658464\n\tspeed: 0.0358s/iter; left time: 7218.8841s\n\titers: 1800, epoch: 9 | loss: 0.1752196\n\tspeed: 0.0352s/iter; left time: 7084.3669s\n\titers: 1900, epoch: 9 | loss: 0.1895705\n\tspeed: 0.0353s/iter; left time: 7104.7133s\n\titers: 2000, epoch: 9 | loss: 0.1758802\n\tspeed: 0.0348s/iter; left time: 6993.7573s\n\titers: 2100, epoch: 9 | loss: 0.1683653\n\tspeed: 0.0355s/iter; left time: 7132.6021s\n\titers: 2200, epoch: 9 | loss: 0.1503230\n\tspeed: 0.0349s/iter; left time: 7014.1397s\n[TRAIN MEMORY] Max memory allocated in epoch 9: 203.15 MB\nEpoch: 9 cost time: 78.60953712463379\nEpoch: 9, Steps: 2209 | Train Loss: 0.1704210 Vali Loss: 0.0414780 Test Loss: 0.2092535\nEarlyStopping counter: 4 out of 5\nUpdating learning rate to 0.001101251887698682\n\titers: 100, epoch: 10 | loss: 0.1692077\n\tspeed: 0.3567s/iter; left time: 71659.2777s\n\titers: 200, epoch: 10 | loss: 0.1906224\n\tspeed: 0.0358s/iter; left time: 7191.6201s\n\titers: 300, epoch: 10 | loss: 0.1782757\n\tspeed: 0.0362s/iter; left time: 7265.3268s\n\titers: 400, epoch: 10 | loss: 0.1743965\n\tspeed: 0.0354s/iter; left time: 7107.7838s\n\titers: 500, epoch: 10 | loss: 0.1778516\n\tspeed: 0.0348s/iter; left time: 6978.4101s\n\titers: 600, epoch: 10 | loss: 0.1842511\n\tspeed: 0.0357s/iter; left time: 7151.5224s\n\titers: 700, epoch: 10 | loss: 0.1841026\n\tspeed: 0.0360s/iter; left time: 7207.6412s\n\titers: 800, epoch: 10 | loss: 0.1839803\n\tspeed: 0.0353s/iter; left time: 7059.3367s\n\titers: 900, epoch: 10 | loss: 0.1778469\n\tspeed: 0.0359s/iter; left time: 7179.8415s\n\titers: 1000, epoch: 10 | loss: 0.1754941\n\tspeed: 0.0352s/iter; left time: 7041.3091s\n\titers: 1100, epoch: 10 | loss: 0.1681623\n\tspeed: 0.0348s/iter; left time: 6960.8567s\n\titers: 1200, epoch: 10 | loss: 0.1574707\n\tspeed: 0.0349s/iter; left time: 6979.7265s\n\titers: 1300, epoch: 10 | loss: 0.1668128\n\tspeed: 0.0350s/iter; left time: 6989.6200s\n\titers: 1400, epoch: 10 | loss: 0.1952537\n\tspeed: 0.0356s/iter; left time: 7111.4530s\n\titers: 1500, epoch: 10 | loss: 0.1526664\n\tspeed: 0.0353s/iter; left time: 7048.3371s\n\titers: 1600, epoch: 10 | loss: 0.1599519\n\tspeed: 0.0356s/iter; left time: 7098.1647s\n\titers: 1700, epoch: 10 | loss: 0.1920680\n\tspeed: 0.0348s/iter; left time: 6928.6138s\n\titers: 1800, epoch: 10 | loss: 0.1725649\n\tspeed: 0.0356s/iter; left time: 7091.6529s\n\titers: 1900, epoch: 10 | loss: 0.1830916\n\tspeed: 0.0356s/iter; left time: 7081.3210s\n\titers: 2000, epoch: 10 | loss: 0.1710106\n\tspeed: 0.0350s/iter; left time: 6957.6819s\n\titers: 2100, epoch: 10 | loss: 0.1923004\n\tspeed: 0.0356s/iter; left time: 7076.1647s\n\titers: 2200, epoch: 10 | loss: 0.1685379\n\tspeed: 0.0343s/iter; left time: 6827.6343s\n[TRAIN MEMORY] Max memory allocated in epoch 10: 203.15 MB\nEpoch: 10 cost time: 78.64887952804565\nEpoch: 10, Steps: 2209 | Train Loss: 0.1707962 Vali Loss: 0.0416572 Test Loss: 0.2218181\nEarlyStopping counter: 5 out of 5\nEarly stopping\nAverage epoch time: 78.55 seconds\n>>>>>>>testing : transformer_test_xPatch_Solar_ftM_sl720_ll48_pl720_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 9793\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 11.57 MB\n[PARAMS] Total parameters: 3,031,865\nmse:0.20622168481349945, mae:0.21954362094402313\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data custom --learning_rate 0.00001 --data_path exchange_rate.csv --seq_len 96 --pred_len 96 --patience 5 --enc_in 8 --dropout 0 --batch_size 16 --lradj 'sigmoid' --period_len 1 --d_model 256 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T07:09:33.775965Z","iopub.execute_input":"2025-10-22T07:09:33.776620Z","iopub.status.idle":"2025-10-22T07:14:10.958279Z","shell.execute_reply.started":"2025-10-22T07:09:33.776590Z","shell.execute_reply":"2025-10-22T07:14:10.957307Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=1, d_model=256, data='custom', root_path='./dataset', data_path='exchange_rate.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=96, enc_in=8, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=16, patience=5, learning_rate=1e-05, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.0, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_custom_ftM_sl96_ll48_pl96_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 5120\nval 665\ntest 1422\n\titers: 100, epoch: 1 | loss: 0.2400656\n\tspeed: 0.0339s/iter; left time: 1079.9591s\n\titers: 200, epoch: 1 | loss: 0.3135854\n\tspeed: 0.0123s/iter; left time: 390.6562s\n\titers: 300, epoch: 1 | loss: 0.2533169\n\tspeed: 0.0126s/iter; left time: 398.7267s\n[TRAIN MEMORY] Max memory allocated in epoch 1: 22.23 MB\nEpoch: 1 cost time: 5.072427034378052\nEpoch: 1, Steps: 320 | Train Loss: 0.2604088 Vali Loss: 0.0784933 Test Loss: 0.1252381\nValidation loss decreased (inf --> 0.078493).  Saving model ...\nUpdating learning rate to 3.95335547559802e-08\n\titers: 100, epoch: 2 | loss: 0.2120472\n\tspeed: 0.0324s/iter; left time: 1022.7101s\n\titers: 200, epoch: 2 | loss: 0.2096897\n\tspeed: 0.0124s/iter; left time: 389.3753s\n\titers: 300, epoch: 2 | loss: 0.2305526\n\tspeed: 0.0121s/iter; left time: 378.3050s\n[TRAIN MEMORY] Max memory allocated in epoch 2: 22.23 MB\nEpoch: 2 cost time: 4.2156970500946045\nEpoch: 2, Steps: 320 | Train Loss: 0.2488576 Vali Loss: 0.0784868 Test Loss: 0.1251630\nValidation loss decreased (0.078493 --> 0.078487).  Saving model ...\nUpdating learning rate to 1.0594668617809589e-07\n\titers: 100, epoch: 3 | loss: 0.2248310\n\tspeed: 0.0327s/iter; left time: 1020.7736s\n\titers: 200, epoch: 3 | loss: 0.1959696\n\tspeed: 0.0122s/iter; left time: 379.1181s\n\titers: 300, epoch: 3 | loss: 0.2355249\n\tspeed: 0.0126s/iter; left time: 391.9727s\n[TRAIN MEMORY] Max memory allocated in epoch 3: 22.23 MB\nEpoch: 3 cost time: 4.221508502960205\nEpoch: 3, Steps: 320 | Train Loss: 0.2486967 Vali Loss: 0.0785307 Test Loss: 0.1249677\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 2.1544660673341325e-07\n\titers: 100, epoch: 4 | loss: 0.2666576\n\tspeed: 0.0321s/iter; left time: 992.7010s\n\titers: 200, epoch: 4 | loss: 0.2851384\n\tspeed: 0.0116s/iter; left time: 358.0198s\n\titers: 300, epoch: 4 | loss: 0.2640055\n\tspeed: 0.0118s/iter; left time: 363.6673s\n[TRAIN MEMORY] Max memory allocated in epoch 4: 22.23 MB\nEpoch: 4 cost time: 4.052609205245972\nEpoch: 4, Steps: 320 | Train Loss: 0.2483457 Vali Loss: 0.0783526 Test Loss: 0.1245742\nValidation loss decreased (0.078487 --> 0.078353).  Saving model ...\nUpdating learning rate to 3.9263302024406893e-07\n\titers: 100, epoch: 5 | loss: 0.2075848\n\tspeed: 0.0329s/iter; left time: 1006.3249s\n\titers: 200, epoch: 5 | loss: 0.2330694\n\tspeed: 0.0123s/iter; left time: 375.6401s\n\titers: 300, epoch: 5 | loss: 0.2312746\n\tspeed: 0.0122s/iter; left time: 370.2666s\n[TRAIN MEMORY] Max memory allocated in epoch 5: 22.23 MB\nEpoch: 5 cost time: 4.219703197479248\nEpoch: 5, Steps: 320 | Train Loss: 0.2476887 Vali Loss: 0.0780650 Test Loss: 0.1238653\nValidation loss decreased (0.078353 --> 0.078065).  Saving model ...\nUpdating learning rate to 6.728069460753158e-07\n\titers: 100, epoch: 6 | loss: 0.2165248\n\tspeed: 0.0321s/iter; left time: 973.0664s\n\titers: 200, epoch: 6 | loss: 0.2411288\n\tspeed: 0.0125s/iter; left time: 377.5123s\n\titers: 300, epoch: 6 | loss: 0.2330628\n\tspeed: 0.0120s/iter; left time: 361.0204s\n[TRAIN MEMORY] Max memory allocated in epoch 6: 22.23 MB\nEpoch: 6 cost time: 4.179319143295288\nEpoch: 6, Steps: 320 | Train Loss: 0.2465471 Vali Loss: 0.0777582 Test Loss: 0.1226562\nValidation loss decreased (0.078065 --> 0.077758).  Saving model ...\nUpdating learning rate to 1.1018962336926974e-06\n\titers: 100, epoch: 7 | loss: 0.1948337\n\tspeed: 0.0325s/iter; left time: 972.9951s\n\titers: 200, epoch: 7 | loss: 0.2995792\n\tspeed: 0.0121s/iter; left time: 362.4107s\n\titers: 300, epoch: 7 | loss: 0.2356249\n\tspeed: 0.0120s/iter; left time: 357.6769s\n[TRAIN MEMORY] Max memory allocated in epoch 7: 22.23 MB\nEpoch: 7 cost time: 4.169938087463379\nEpoch: 7, Steps: 320 | Train Loss: 0.2446800 Vali Loss: 0.0768799 Test Loss: 0.1207104\nValidation loss decreased (0.077758 --> 0.076880).  Saving model ...\nUpdating learning rate to 1.7295448022441024e-06\n\titers: 100, epoch: 8 | loss: 0.2016682\n\tspeed: 0.0329s/iter; left time: 977.2078s\n\titers: 200, epoch: 8 | loss: 0.1913429\n\tspeed: 0.0122s/iter; left time: 360.3287s\n\titers: 300, epoch: 8 | loss: 0.2093398\n\tspeed: 0.0121s/iter; left time: 356.0119s\n[TRAIN MEMORY] Max memory allocated in epoch 8: 22.23 MB\nEpoch: 8 cost time: 4.193591117858887\nEpoch: 8, Steps: 320 | Train Loss: 0.2417677 Vali Loss: 0.0759032 Test Loss: 0.1177046\nValidation loss decreased (0.076880 --> 0.075903).  Saving model ...\nUpdating learning rate to 2.589896195030908e-06\n\titers: 100, epoch: 9 | loss: 0.2571375\n\tspeed: 0.0329s/iter; left time: 966.6872s\n\titers: 200, epoch: 9 | loss: 0.2127256\n\tspeed: 0.0125s/iter; left time: 366.6816s\n\titers: 300, epoch: 9 | loss: 0.2340275\n\tspeed: 0.0120s/iter; left time: 349.9606s\n[TRAIN MEMORY] Max memory allocated in epoch 9: 22.23 MB\nEpoch: 9 cost time: 4.244530916213989\nEpoch: 9, Steps: 320 | Train Loss: 0.2374388 Vali Loss: 0.0742474 Test Loss: 0.1133177\nValidation loss decreased (0.075903 --> 0.074247).  Saving model ...\nUpdating learning rate to 3.670839625662274e-06\n\titers: 100, epoch: 10 | loss: 0.2087904\n\tspeed: 0.0324s/iter; left time: 940.7513s\n\titers: 200, epoch: 10 | loss: 0.2451726\n\tspeed: 0.0125s/iter; left time: 362.2344s\n\titers: 300, epoch: 10 | loss: 0.2316507\n\tspeed: 0.0122s/iter; left time: 351.3421s\n[TRAIN MEMORY] Max memory allocated in epoch 10: 22.23 MB\nEpoch: 10 cost time: 4.203873157501221\nEpoch: 10, Steps: 320 | Train Loss: 0.2314486 Vali Loss: 0.0722096 Test Loss: 0.1074634\nValidation loss decreased (0.074247 --> 0.072210).  Saving model ...\nUpdating learning rate to 4.890130573694069e-06\n\titers: 100, epoch: 11 | loss: 0.2488533\n\tspeed: 0.0326s/iter; left time: 936.5647s\n\titers: 200, epoch: 11 | loss: 0.2450445\n\tspeed: 0.0121s/iter; left time: 347.0109s\n\titers: 300, epoch: 11 | loss: 0.2263852\n\tspeed: 0.0123s/iter; left time: 350.6904s\n[TRAIN MEMORY] Max memory allocated in epoch 11: 22.23 MB\nEpoch: 11 cost time: 4.136432409286499\nEpoch: 11, Steps: 320 | Train Loss: 0.2240874 Vali Loss: 0.0698931 Test Loss: 0.1008271\nValidation loss decreased (0.072210 --> 0.069893).  Saving model ...\nUpdating learning rate to 6.1091557871793225e-06\n\titers: 100, epoch: 12 | loss: 0.2271684\n\tspeed: 0.0331s/iter; left time: 938.2437s\n\titers: 200, epoch: 12 | loss: 0.1718021\n\tspeed: 0.0122s/iter; left time: 343.7416s\n\titers: 300, epoch: 12 | loss: 0.2031107\n\tspeed: 0.0129s/iter; left time: 363.3898s\n[TRAIN MEMORY] Max memory allocated in epoch 12: 22.23 MB\nEpoch: 12 cost time: 4.256531238555908\nEpoch: 12, Steps: 320 | Train Loss: 0.2164944 Vali Loss: 0.0674448 Test Loss: 0.0948327\nValidation loss decreased (0.069893 --> 0.067445).  Saving model ...\nUpdating learning rate to 7.189301436457307e-06\n\titers: 100, epoch: 13 | loss: 0.2462030\n\tspeed: 0.0328s/iter; left time: 919.5438s\n\titers: 200, epoch: 13 | loss: 0.1893737\n\tspeed: 0.0119s/iter; left time: 333.9241s\n\titers: 300, epoch: 13 | loss: 0.2751143\n\tspeed: 0.0116s/iter; left time: 324.1762s\n[TRAIN MEMORY] Max memory allocated in epoch 13: 22.23 MB\nEpoch: 13 cost time: 4.148978233337402\nEpoch: 13, Steps: 320 | Train Loss: 0.2100336 Vali Loss: 0.0658296 Test Loss: 0.0904157\nValidation loss decreased (0.067445 --> 0.065830).  Saving model ...\nUpdating learning rate to 8.04832126729532e-06\n\titers: 100, epoch: 14 | loss: 0.2068374\n\tspeed: 0.0328s/iter; left time: 909.7930s\n\titers: 200, epoch: 14 | loss: 0.1936784\n\tspeed: 0.0122s/iter; left time: 337.0213s\n\titers: 300, epoch: 14 | loss: 0.1680154\n\tspeed: 0.0118s/iter; left time: 325.9455s\n[TRAIN MEMORY] Max memory allocated in epoch 14: 22.23 MB\nEpoch: 14 cost time: 4.1394219398498535\nEpoch: 14, Steps: 320 | Train Loss: 0.2053371 Vali Loss: 0.0644884 Test Loss: 0.0874669\nValidation loss decreased (0.065830 --> 0.064488).  Saving model ...\nUpdating learning rate to 8.674101601502177e-06\n\titers: 100, epoch: 15 | loss: 0.2149082\n\tspeed: 0.0330s/iter; left time: 905.0266s\n\titers: 200, epoch: 15 | loss: 0.2150570\n\tspeed: 0.0123s/iter; left time: 336.4306s\n\titers: 300, epoch: 15 | loss: 0.1912088\n\tspeed: 0.0124s/iter; left time: 336.7728s\n[TRAIN MEMORY] Max memory allocated in epoch 15: 22.23 MB\nEpoch: 15 cost time: 4.253148317337036\nEpoch: 15, Steps: 320 | Train Loss: 0.2021813 Vali Loss: 0.0636906 Test Loss: 0.0858182\nValidation loss decreased (0.064488 --> 0.063691).  Saving model ...\nUpdating learning rate to 9.100781929355111e-06\n\titers: 100, epoch: 16 | loss: 0.1875132\n\tspeed: 0.0329s/iter; left time: 891.1342s\n\titers: 200, epoch: 16 | loss: 0.1664220\n\tspeed: 0.0123s/iter; left time: 330.8852s\n\titers: 300, epoch: 16 | loss: 0.2071365\n\tspeed: 0.0115s/iter; left time: 310.6702s\n[TRAIN MEMORY] Max memory allocated in epoch 16: 22.23 MB\nEpoch: 16 cost time: 4.136481285095215\nEpoch: 16, Steps: 320 | Train Loss: 0.2000532 Vali Loss: 0.0632179 Test Loss: 0.0848071\nValidation loss decreased (0.063691 --> 0.063218).  Saving model ...\nUpdating learning rate to 9.378000951291604e-06\n\titers: 100, epoch: 17 | loss: 0.2032597\n\tspeed: 0.0329s/iter; left time: 881.3500s\n\titers: 200, epoch: 17 | loss: 0.2073145\n\tspeed: 0.0118s/iter; left time: 315.0915s\n\titers: 300, epoch: 17 | loss: 0.2109853\n\tspeed: 0.0126s/iter; left time: 335.5220s\n[TRAIN MEMORY] Max memory allocated in epoch 17: 22.23 MB\nEpoch: 17 cost time: 4.262439012527466\nEpoch: 17, Steps: 320 | Train Loss: 0.1985159 Vali Loss: 0.0629473 Test Loss: 0.0841863\nValidation loss decreased (0.063218 --> 0.062947).  Saving model ...\nUpdating learning rate to 9.551680126702348e-06\n\titers: 100, epoch: 18 | loss: 0.2042689\n\tspeed: 0.0332s/iter; left time: 877.5567s\n\titers: 200, epoch: 18 | loss: 0.1642317\n\tspeed: 0.0122s/iter; left time: 321.0804s\n\titers: 300, epoch: 18 | loss: 0.1753148\n\tspeed: 0.0117s/iter; left time: 306.2836s\n[TRAIN MEMORY] Max memory allocated in epoch 18: 22.23 MB\nEpoch: 18 cost time: 4.084277868270874\nEpoch: 18, Steps: 320 | Train Loss: 0.1973693 Vali Loss: 0.0626338 Test Loss: 0.0836323\nValidation loss decreased (0.062947 --> 0.062634).  Saving model ...\nUpdating learning rate to 9.657112906664675e-06\n\titers: 100, epoch: 19 | loss: 0.1872353\n\tspeed: 0.0323s/iter; left time: 844.0893s\n\titers: 200, epoch: 19 | loss: 0.1880965\n\tspeed: 0.0119s/iter; left time: 309.2397s\n\titers: 300, epoch: 19 | loss: 0.1853663\n\tspeed: 0.0124s/iter; left time: 321.1207s\n[TRAIN MEMORY] Max memory allocated in epoch 19: 22.23 MB\nEpoch: 19 cost time: 4.168868541717529\nEpoch: 19, Steps: 320 | Train Loss: 0.1964677 Vali Loss: 0.0626144 Test Loss: 0.0832208\nValidation loss decreased (0.062634 --> 0.062614).  Saving model ...\nUpdating learning rate to 9.718890240536791e-06\n\titers: 100, epoch: 20 | loss: 0.1986576\n\tspeed: 0.0322s/iter; left time: 830.7609s\n\titers: 200, epoch: 20 | loss: 0.2157462\n\tspeed: 0.0122s/iter; left time: 313.2065s\n\titers: 300, epoch: 20 | loss: 0.1991122\n\tspeed: 0.0120s/iter; left time: 306.5087s\n[TRAIN MEMORY] Max memory allocated in epoch 20: 22.23 MB\nEpoch: 20 cost time: 4.122021913528442\nEpoch: 20, Steps: 320 | Train Loss: 0.1957614 Vali Loss: 0.0623448 Test Loss: 0.0830258\nValidation loss decreased (0.062614 --> 0.062345).  Saving model ...\nUpdating learning rate to 9.753209391136237e-06\n\titers: 100, epoch: 21 | loss: 0.1640799\n\tspeed: 0.0323s/iter; left time: 822.5562s\n\titers: 200, epoch: 21 | loss: 0.1792572\n\tspeed: 0.0126s/iter; left time: 318.7940s\n\titers: 300, epoch: 21 | loss: 0.2122999\n\tspeed: 0.0125s/iter; left time: 316.0917s\n[TRAIN MEMORY] Max memory allocated in epoch 21: 22.23 MB\nEpoch: 21 cost time: 4.225927829742432\nEpoch: 21, Steps: 320 | Train Loss: 0.1951464 Vali Loss: 0.0622156 Test Loss: 0.0826288\nValidation loss decreased (0.062345 --> 0.062216).  Saving model ...\nUpdating learning rate to 9.77038900347065e-06\n\titers: 100, epoch: 22 | loss: 0.1679686\n\tspeed: 0.0327s/iter; left time: 822.8824s\n\titers: 200, epoch: 22 | loss: 0.1765202\n\tspeed: 0.0120s/iter; left time: 301.1078s\n\titers: 300, epoch: 22 | loss: 0.1848223\n\tspeed: 0.0123s/iter; left time: 306.4154s\n[TRAIN MEMORY] Max memory allocated in epoch 22: 22.23 MB\nEpoch: 22 cost time: 4.183542966842651\nEpoch: 22, Steps: 320 | Train Loss: 0.1946526 Vali Loss: 0.0623139 Test Loss: 0.0824215\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 9.776870711092878e-06\n\titers: 100, epoch: 23 | loss: 0.1924790\n\tspeed: 0.0331s/iter; left time: 823.6287s\n\titers: 200, epoch: 23 | loss: 0.1850624\n\tspeed: 0.0117s/iter; left time: 289.1134s\n\titers: 300, epoch: 23 | loss: 0.1671426\n\tspeed: 0.0118s/iter; left time: 291.6369s\n[TRAIN MEMORY] Max memory allocated in epoch 23: 22.23 MB\nEpoch: 23 cost time: 4.156274318695068\nEpoch: 23, Steps: 320 | Train Loss: 0.1942079 Vali Loss: 0.0619415 Test Loss: 0.0824671\nValidation loss decreased (0.062216 --> 0.061942).  Saving model ...\nUpdating learning rate to 9.776624732245827e-06\n\titers: 100, epoch: 24 | loss: 0.1581147\n\tspeed: 0.0321s/iter; left time: 787.0753s\n\titers: 200, epoch: 24 | loss: 0.2172965\n\tspeed: 0.0120s/iter; left time: 292.7563s\n\titers: 300, epoch: 24 | loss: 0.1525976\n\tspeed: 0.0124s/iter; left time: 302.8433s\n[TRAIN MEMORY] Max memory allocated in epoch 24: 22.23 MB\nEpoch: 24 cost time: 4.149129390716553\nEpoch: 24, Steps: 320 | Train Loss: 0.1938156 Vali Loss: 0.0620055 Test Loss: 0.0824502\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 9.77207677869469e-06\n\titers: 100, epoch: 25 | loss: 0.2188058\n\tspeed: 0.0325s/iter; left time: 787.5873s\n\titers: 200, epoch: 25 | loss: 0.1675917\n\tspeed: 0.0119s/iter; left time: 285.8625s\n\titers: 300, epoch: 25 | loss: 0.1886360\n\tspeed: 0.0119s/iter; left time: 286.7592s\n[TRAIN MEMORY] Max memory allocated in epoch 25: 22.23 MB\nEpoch: 25 cost time: 4.120564699172974\nEpoch: 25, Steps: 320 | Train Loss: 0.1934716 Vali Loss: 0.0616049 Test Loss: 0.0824000\nValidation loss decreased (0.061942 --> 0.061605).  Saving model ...\nUpdating learning rate to 9.764698514530509e-06\n\titers: 100, epoch: 26 | loss: 0.1840900\n\tspeed: 0.0328s/iter; left time: 784.6790s\n\titers: 200, epoch: 26 | loss: 0.1969965\n\tspeed: 0.0124s/iter; left time: 296.1032s\n\titers: 300, epoch: 26 | loss: 0.2082765\n\tspeed: 0.0122s/iter; left time: 289.1685s\n[TRAIN MEMORY] Max memory allocated in epoch 26: 22.23 MB\nEpoch: 26 cost time: 4.2153425216674805\nEpoch: 26, Steps: 320 | Train Loss: 0.1931474 Vali Loss: 0.0618317 Test Loss: 0.0823583\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 9.755376284518646e-06\n\titers: 100, epoch: 27 | loss: 0.1715222\n\tspeed: 0.0323s/iter; left time: 761.0163s\n\titers: 200, epoch: 27 | loss: 0.1808731\n\tspeed: 0.0116s/iter; left time: 271.8498s\n\titers: 300, epoch: 27 | loss: 0.1535603\n\tspeed: 0.0125s/iter; left time: 291.1554s\n[TRAIN MEMORY] Max memory allocated in epoch 27: 22.23 MB\nEpoch: 27 cost time: 4.145310163497925\nEpoch: 27, Steps: 320 | Train Loss: 0.1928716 Vali Loss: 0.0615511 Test Loss: 0.0823025\nValidation loss decreased (0.061605 --> 0.061551).  Saving model ...\nUpdating learning rate to 9.744638697950733e-06\n\titers: 100, epoch: 28 | loss: 0.2089743\n\tspeed: 0.0326s/iter; left time: 757.9944s\n\titers: 200, epoch: 28 | loss: 0.1812232\n\tspeed: 0.0122s/iter; left time: 282.7577s\n\titers: 300, epoch: 28 | loss: 0.2159879\n\tspeed: 0.0115s/iter; left time: 265.3106s\n[TRAIN MEMORY] Max memory allocated in epoch 28: 22.23 MB\nEpoch: 28 cost time: 4.098974704742432\nEpoch: 28, Steps: 320 | Train Loss: 0.1926260 Vali Loss: 0.0614767 Test Loss: 0.0822527\nValidation loss decreased (0.061551 --> 0.061477).  Saving model ...\nUpdating learning rate to 9.73279611847148e-06\n\titers: 100, epoch: 29 | loss: 0.1858174\n\tspeed: 0.0327s/iter; left time: 750.8933s\n\titers: 200, epoch: 29 | loss: 0.1688402\n\tspeed: 0.0120s/iter; left time: 274.5694s\n\titers: 300, epoch: 29 | loss: 0.1669788\n\tspeed: 0.0120s/iter; left time: 273.0204s\n[TRAIN MEMORY] Max memory allocated in epoch 29: 22.23 MB\nEpoch: 29 cost time: 4.1807026863098145\nEpoch: 29, Steps: 320 | Train Loss: 0.1923961 Vali Loss: 0.0617477 Test Loss: 0.0820064\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 9.720025799884166e-06\n\titers: 100, epoch: 30 | loss: 0.2031423\n\tspeed: 0.0330s/iter; left time: 745.3854s\n\titers: 200, epoch: 30 | loss: 0.2187460\n\tspeed: 0.0124s/iter; left time: 279.9855s\n\titers: 300, epoch: 30 | loss: 0.1549743\n\tspeed: 0.0123s/iter; left time: 276.6206s\n[TRAIN MEMORY] Max memory allocated in epoch 30: 22.23 MB\nEpoch: 30 cost time: 4.250800132751465\nEpoch: 30, Steps: 320 | Train Loss: 0.1921703 Vali Loss: 0.0614601 Test Loss: 0.0821441\nValidation loss decreased (0.061477 --> 0.061460).  Saving model ...\nUpdating learning rate to 9.706423713799412e-06\n\titers: 100, epoch: 31 | loss: 0.1883001\n\tspeed: 0.0329s/iter; left time: 734.7053s\n\titers: 200, epoch: 31 | loss: 0.1884174\n\tspeed: 0.0126s/iter; left time: 279.2425s\n\titers: 300, epoch: 31 | loss: 0.1782973\n\tspeed: 0.0118s/iter; left time: 261.8619s\n[TRAIN MEMORY] Max memory allocated in epoch 31: 22.23 MB\nEpoch: 31 cost time: 4.235090732574463\nEpoch: 31, Steps: 320 | Train Loss: 0.1919645 Vali Loss: 0.0615564 Test Loss: 0.0821617\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 9.692036049517375e-06\n\titers: 100, epoch: 32 | loss: 0.1815810\n\tspeed: 0.0329s/iter; left time: 723.7862s\n\titers: 200, epoch: 32 | loss: 0.2206088\n\tspeed: 0.0120s/iter; left time: 263.1527s\n\titers: 300, epoch: 32 | loss: 0.1864584\n\tspeed: 0.0123s/iter; left time: 266.9191s\n[TRAIN MEMORY] Max memory allocated in epoch 32: 22.23 MB\nEpoch: 32 cost time: 4.182769298553467\nEpoch: 32, Steps: 320 | Train Loss: 0.1917758 Vali Loss: 0.0614731 Test Loss: 0.0821215\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 9.676878338797016e-06\n\titers: 100, epoch: 33 | loss: 0.2037938\n\tspeed: 0.0324s/iter; left time: 701.1212s\n\titers: 200, epoch: 33 | loss: 0.1693352\n\tspeed: 0.0123s/iter; left time: 265.9273s\n\titers: 300, epoch: 33 | loss: 0.1478365\n\tspeed: 0.0128s/iter; left time: 274.0824s\n[TRAIN MEMORY] Max memory allocated in epoch 33: 22.23 MB\nEpoch: 33 cost time: 4.206332683563232\nEpoch: 33, Steps: 320 | Train Loss: 0.1915877 Vali Loss: 0.0614640 Test Loss: 0.0819241\nEarlyStopping counter: 3 out of 5\nUpdating learning rate to 9.66094705849841e-06\n\titers: 100, epoch: 34 | loss: 0.2585221\n\tspeed: 0.0318s/iter; left time: 679.1255s\n\titers: 200, epoch: 34 | loss: 0.1764518\n\tspeed: 0.0120s/iter; left time: 254.8959s\n\titers: 300, epoch: 34 | loss: 0.1673694\n\tspeed: 0.0118s/iter; left time: 249.3894s\n[TRAIN MEMORY] Max memory allocated in epoch 34: 22.23 MB\nEpoch: 34 cost time: 4.063595771789551\nEpoch: 34, Steps: 320 | Train Loss: 0.1914348 Vali Loss: 0.0613015 Test Loss: 0.0819676\nValidation loss decreased (0.061460 --> 0.061301).  Saving model ...\nUpdating learning rate to 9.644226665527616e-06\n\titers: 100, epoch: 35 | loss: 0.2140338\n\tspeed: 0.0319s/iter; left time: 671.5659s\n\titers: 200, epoch: 35 | loss: 0.1834912\n\tspeed: 0.0122s/iter; left time: 256.2592s\n\titers: 300, epoch: 35 | loss: 0.1963351\n\tspeed: 0.0123s/iter; left time: 255.2247s\n[TRAIN MEMORY] Max memory allocated in epoch 35: 22.23 MB\nEpoch: 35 cost time: 4.16063380241394\nEpoch: 35, Steps: 320 | Train Loss: 0.1912701 Vali Loss: 0.0614570 Test Loss: 0.0821139\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 9.626693860165864e-06\n\titers: 100, epoch: 36 | loss: 0.2089837\n\tspeed: 0.0345s/iter; left time: 714.0490s\n\titers: 200, epoch: 36 | loss: 0.2141679\n\tspeed: 0.0127s/iter; left time: 261.7588s\n\titers: 300, epoch: 36 | loss: 0.1680116\n\tspeed: 0.0126s/iter; left time: 257.8584s\n[TRAIN MEMORY] Max memory allocated in epoch 36: 22.23 MB\nEpoch: 36 cost time: 4.339166641235352\nEpoch: 36, Steps: 320 | Train Loss: 0.1911227 Vali Loss: 0.0613945 Test Loss: 0.0819088\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 9.608320168789379e-06\n\titers: 100, epoch: 37 | loss: 0.2052526\n\tspeed: 0.0332s/iter; left time: 675.9010s\n\titers: 200, epoch: 37 | loss: 0.1810484\n\tspeed: 0.0117s/iter; left time: 236.7728s\n\titers: 300, epoch: 37 | loss: 0.1978519\n\tspeed: 0.0122s/iter; left time: 245.4172s\n[TRAIN MEMORY] Max memory allocated in epoch 37: 22.23 MB\nEpoch: 37 cost time: 4.156649589538574\nEpoch: 37, Steps: 320 | Train Loss: 0.1909968 Vali Loss: 0.0611264 Test Loss: 0.0820849\nValidation loss decreased (0.061301 --> 0.061126).  Saving model ...\nUpdating learning rate to 9.589073508423282e-06\n\titers: 100, epoch: 38 | loss: 0.1892700\n\tspeed: 0.0336s/iter; left time: 673.0561s\n\titers: 200, epoch: 38 | loss: 0.1732202\n\tspeed: 0.0132s/iter; left time: 263.0263s\n\titers: 300, epoch: 38 | loss: 0.1780033\n\tspeed: 0.0127s/iter; left time: 253.0301s\n[TRAIN MEMORY] Max memory allocated in epoch 38: 22.23 MB\nEpoch: 38 cost time: 4.37198281288147\nEpoch: 38, Steps: 320 | Train Loss: 0.1908563 Vali Loss: 0.0614081 Test Loss: 0.0819659\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 9.568919135308863e-06\n\titers: 100, epoch: 39 | loss: 0.1795709\n\tspeed: 0.0347s/iter; left time: 685.8382s\n\titers: 200, epoch: 39 | loss: 0.2029381\n\tspeed: 0.0127s/iter; left time: 250.3315s\n\titers: 300, epoch: 39 | loss: 0.2136272\n\tspeed: 0.0135s/iter; left time: 263.2647s\n[TRAIN MEMORY] Max memory allocated in epoch 39: 22.23 MB\nEpoch: 39 cost time: 4.298954963684082\nEpoch: 39, Steps: 320 | Train Loss: 0.1907459 Vali Loss: 0.0610871 Test Loss: 0.0823311\nValidation loss decreased (0.061126 --> 0.061087).  Saving model ...\nUpdating learning rate to 9.547820221693044e-06\n\titers: 100, epoch: 40 | loss: 0.2382833\n\tspeed: 0.0325s/iter; left time: 630.3665s\n\titers: 200, epoch: 40 | loss: 0.2217630\n\tspeed: 0.0122s/iter; left time: 235.4367s\n\titers: 300, epoch: 40 | loss: 0.1793077\n\tspeed: 0.0121s/iter; left time: 233.0453s\n[TRAIN MEMORY] Max memory allocated in epoch 40: 22.23 MB\nEpoch: 40 cost time: 4.178788661956787\nEpoch: 40, Steps: 320 | Train Loss: 0.1906191 Vali Loss: 0.0610919 Test Loss: 0.0820728\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 9.525738209202062e-06\n\titers: 100, epoch: 41 | loss: 0.1996918\n\tspeed: 0.0334s/iter; left time: 637.6200s\n\titers: 200, epoch: 41 | loss: 0.2093272\n\tspeed: 0.0127s/iter; left time: 240.8055s\n\titers: 300, epoch: 41 | loss: 0.2202788\n\tspeed: 0.0117s/iter; left time: 220.3020s\n[TRAIN MEMORY] Max memory allocated in epoch 41: 22.23 MB\nEpoch: 41 cost time: 4.243819713592529\nEpoch: 41, Steps: 320 | Train Loss: 0.1905038 Vali Loss: 0.0612847 Test Loss: 0.0820296\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 9.502633029023414e-06\n\titers: 100, epoch: 42 | loss: 0.2170974\n\tspeed: 0.0335s/iter; left time: 628.7729s\n\titers: 200, epoch: 42 | loss: 0.1509082\n\tspeed: 0.0127s/iter; left time: 237.9425s\n\titers: 300, epoch: 42 | loss: 0.2085900\n\tspeed: 0.0124s/iter; left time: 230.6460s\n[TRAIN MEMORY] Max memory allocated in epoch 42: 22.23 MB\nEpoch: 42 cost time: 4.317396402359009\nEpoch: 42, Steps: 320 | Train Loss: 0.1903871 Vali Loss: 0.0610300 Test Loss: 0.0822631\nValidation loss decreased (0.061087 --> 0.061030).  Saving model ...\nUpdating learning rate to 9.478463243864202e-06\n\titers: 100, epoch: 43 | loss: 0.2073073\n\tspeed: 0.0330s/iter; left time: 608.6715s\n\titers: 200, epoch: 43 | loss: 0.1810619\n\tspeed: 0.0124s/iter; left time: 227.4918s\n\titers: 300, epoch: 43 | loss: 0.1761302\n\tspeed: 0.0125s/iter; left time: 228.9177s\n[TRAIN MEMORY] Max memory allocated in epoch 43: 22.23 MB\nEpoch: 43 cost time: 4.281440258026123\nEpoch: 43, Steps: 320 | Train Loss: 0.1903004 Vali Loss: 0.0610837 Test Loss: 0.0822069\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 9.453186145280302e-06\n\titers: 100, epoch: 44 | loss: 0.1650841\n\tspeed: 0.0332s/iter; left time: 603.1040s\n\titers: 200, epoch: 44 | loss: 0.2049612\n\tspeed: 0.0117s/iter; left time: 211.8839s\n\titers: 300, epoch: 44 | loss: 0.1768214\n\tspeed: 0.0124s/iter; left time: 223.1197s\n[TRAIN MEMORY] Max memory allocated in epoch 44: 22.23 MB\nEpoch: 44 cost time: 4.163240194320679\nEpoch: 44, Steps: 320 | Train Loss: 0.1901989 Vali Loss: 0.0613007 Test Loss: 0.0820900\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 9.426757827017559e-06\n\titers: 100, epoch: 45 | loss: 0.2034431\n\tspeed: 0.0330s/iter; left time: 587.3666s\n\titers: 200, epoch: 45 | loss: 0.1619138\n\tspeed: 0.0118s/iter; left time: 208.8007s\n\titers: 300, epoch: 45 | loss: 0.1707798\n\tspeed: 0.0123s/iter; left time: 216.7063s\n[TRAIN MEMORY] Max memory allocated in epoch 45: 22.23 MB\nEpoch: 45 cost time: 4.1766676902771\nEpoch: 45, Steps: 320 | Train Loss: 0.1901185 Vali Loss: 0.0611538 Test Loss: 0.0820767\nEarlyStopping counter: 3 out of 5\nUpdating learning rate to 9.399133247160014e-06\n\titers: 100, epoch: 46 | loss: 0.1749387\n\tspeed: 0.0334s/iter; left time: 584.8940s\n\titers: 200, epoch: 46 | loss: 0.1877452\n\tspeed: 0.0118s/iter; left time: 205.2123s\n\titers: 300, epoch: 46 | loss: 0.1600509\n\tspeed: 0.0123s/iter; left time: 212.6208s\n[TRAIN MEMORY] Max memory allocated in epoch 46: 22.23 MB\nEpoch: 46 cost time: 4.13097620010376\nEpoch: 46, Steps: 320 | Train Loss: 0.1900032 Vali Loss: 0.0612836 Test Loss: 0.0820815\nEarlyStopping counter: 4 out of 5\nUpdating learning rate to 9.370266287130242e-06\n\titers: 100, epoch: 47 | loss: 0.1718037\n\tspeed: 0.0332s/iter; left time: 569.8116s\n\titers: 200, epoch: 47 | loss: 0.2038889\n\tspeed: 0.0128s/iter; left time: 218.4450s\n\titers: 300, epoch: 47 | loss: 0.1655931\n\tspeed: 0.0124s/iter; left time: 210.4209s\n[TRAIN MEMORY] Max memory allocated in epoch 47: 22.23 MB\nEpoch: 47 cost time: 4.3007776737213135\nEpoch: 47, Steps: 320 | Train Loss: 0.1899182 Vali Loss: 0.0611097 Test Loss: 0.0821034\nEarlyStopping counter: 5 out of 5\nEarly stopping\nAverage epoch time: 4.21 seconds\n>>>>>>>testing : transformer_test_xPatch_custom_ftM_sl96_ll48_pl96_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1422\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 0.94 MB\n[PARAMS] Total parameters: 244,888\nmse:0.08226310461759567, mae:0.19982969760894775\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data custom --learning_rate 0.000005 --data_path exchange_rate.csv --seq_len 96 --pred_len 192 --patience 5 --enc_in 8 --dropout 0 --batch_size 32 --lradj 'sigmoid' --period_len 1 --d_model 256 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T07:38:41.517807Z","iopub.execute_input":"2025-10-22T07:38:41.518155Z","iopub.status.idle":"2025-10-22T07:42:16.031071Z","shell.execute_reply.started":"2025-10-22T07:38:41.518118Z","shell.execute_reply":"2025-10-22T07:42:16.030323Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=1, d_model=256, data='custom', root_path='./dataset', data_path='exchange_rate.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=192, enc_in=8, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=32, patience=5, learning_rate=5e-06, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.0, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_custom_ftM_sl96_ll48_pl192_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 5024\nval 569\ntest 1326\n\titers: 100, epoch: 1 | loss: 0.3417253\n\tspeed: 0.0347s/iter; left time: 541.0279s\n[TRAIN MEMORY] Max memory allocated in epoch 1: 27.30 MB\nEpoch: 1 cost time: 3.155609607696533\nEpoch: 1, Steps: 157 | Train Loss: 0.3246670 Vali Loss: 0.0969846 Test Loss: 0.2288066\nValidation loss decreased (inf --> 0.096985).  Saving model ...\nUpdating learning rate to 1.97667773779901e-08\n\titers: 100, epoch: 2 | loss: 0.3440527\n\tspeed: 0.0341s/iter; left time: 526.3793s\n[TRAIN MEMORY] Max memory allocated in epoch 2: 27.36 MB\nEpoch: 2 cost time: 2.3600666522979736\nEpoch: 2, Steps: 157 | Train Loss: 0.3202348 Vali Loss: 0.0965290 Test Loss: 0.2287691\nValidation loss decreased (0.096985 --> 0.096529).  Saving model ...\nUpdating learning rate to 5.2973343089047946e-08\n\titers: 100, epoch: 3 | loss: 0.3081495\n\tspeed: 0.0340s/iter; left time: 519.3163s\n[TRAIN MEMORY] Max memory allocated in epoch 3: 27.36 MB\nEpoch: 3 cost time: 2.4067981243133545\nEpoch: 3, Steps: 157 | Train Loss: 0.3201716 Vali Loss: 0.0966515 Test Loss: 0.2286696\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 1.0772330336670663e-07\n\titers: 100, epoch: 4 | loss: 0.3166808\n\tspeed: 0.0345s/iter; left time: 521.8227s\n[TRAIN MEMORY] Max memory allocated in epoch 4: 27.36 MB\nEpoch: 4 cost time: 2.4029388427734375\nEpoch: 4, Steps: 157 | Train Loss: 0.3200316 Vali Loss: 0.0964824 Test Loss: 0.2284655\nValidation loss decreased (0.096529 --> 0.096482).  Saving model ...\nUpdating learning rate to 1.9631651012203446e-07\n\titers: 100, epoch: 5 | loss: 0.3128043\n\tspeed: 0.0348s/iter; left time: 521.0500s\n[TRAIN MEMORY] Max memory allocated in epoch 5: 27.36 MB\nEpoch: 5 cost time: 2.4228222370147705\nEpoch: 5, Steps: 157 | Train Loss: 0.3197671 Vali Loss: 0.0965286 Test Loss: 0.2280926\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 3.364034730376579e-07\n\titers: 100, epoch: 6 | loss: 0.3333766\n\tspeed: 0.0347s/iter; left time: 514.3366s\n[TRAIN MEMORY] Max memory allocated in epoch 6: 27.36 MB\nEpoch: 6 cost time: 2.3889975547790527\nEpoch: 6, Steps: 157 | Train Loss: 0.3193047 Vali Loss: 0.0965849 Test Loss: 0.2274615\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 5.509481168463487e-07\n\titers: 100, epoch: 7 | loss: 0.3185758\n\tspeed: 0.0341s/iter; left time: 499.6542s\n[TRAIN MEMORY] Max memory allocated in epoch 7: 27.36 MB\nEpoch: 7 cost time: 2.3953909873962402\nEpoch: 7, Steps: 157 | Train Loss: 0.3185332 Vali Loss: 0.0961674 Test Loss: 0.2264148\nValidation loss decreased (0.096482 --> 0.096167).  Saving model ...\nUpdating learning rate to 8.647724011220512e-07\n\titers: 100, epoch: 8 | loss: 0.3408919\n\tspeed: 0.0347s/iter; left time: 503.5332s\n[TRAIN MEMORY] Max memory allocated in epoch 8: 27.36 MB\nEpoch: 8 cost time: 2.372490167617798\nEpoch: 8, Steps: 157 | Train Loss: 0.3172990 Vali Loss: 0.0958347 Test Loss: 0.2247719\nValidation loss decreased (0.096167 --> 0.095835).  Saving model ...\nUpdating learning rate to 1.294948097515454e-06\n\titers: 100, epoch: 9 | loss: 0.3228720\n\tspeed: 0.0347s/iter; left time: 497.2867s\n[TRAIN MEMORY] Max memory allocated in epoch 9: 27.36 MB\nEpoch: 9 cost time: 2.4253711700439453\nEpoch: 9, Steps: 157 | Train Loss: 0.3153917 Vali Loss: 0.0948099 Test Loss: 0.2222917\nValidation loss decreased (0.095835 --> 0.094810).  Saving model ...\nUpdating learning rate to 1.835419812831137e-06\n\titers: 100, epoch: 10 | loss: 0.3554509\n\tspeed: 0.0337s/iter; left time: 478.6275s\n[TRAIN MEMORY] Max memory allocated in epoch 10: 27.36 MB\nEpoch: 10 cost time: 2.3124966621398926\nEpoch: 10, Steps: 157 | Train Loss: 0.3125523 Vali Loss: 0.0934259 Test Loss: 0.2185986\nValidation loss decreased (0.094810 --> 0.093426).  Saving model ...\nUpdating learning rate to 2.4450652868470344e-06\n\titers: 100, epoch: 11 | loss: 0.3369938\n\tspeed: 0.0340s/iter; left time: 476.5669s\n[TRAIN MEMORY] Max memory allocated in epoch 11: 27.36 MB\nEpoch: 11 cost time: 2.352915048599243\nEpoch: 11, Steps: 157 | Train Loss: 0.3085386 Vali Loss: 0.0918500 Test Loss: 0.2134410\nValidation loss decreased (0.093426 --> 0.091850).  Saving model ...\nUpdating learning rate to 3.0545778935896613e-06\n\titers: 100, epoch: 12 | loss: 0.3415439\n\tspeed: 0.0349s/iter; left time: 484.8019s\n[TRAIN MEMORY] Max memory allocated in epoch 12: 27.36 MB\nEpoch: 12 cost time: 2.428140640258789\nEpoch: 12, Steps: 157 | Train Loss: 0.3032983 Vali Loss: 0.0901057 Test Loss: 0.2070664\nValidation loss decreased (0.091850 --> 0.090106).  Saving model ...\nUpdating learning rate to 3.5946507182286534e-06\n\titers: 100, epoch: 13 | loss: 0.2600537\n\tspeed: 0.0345s/iter; left time: 473.0044s\n[TRAIN MEMORY] Max memory allocated in epoch 13: 27.36 MB\nEpoch: 13 cost time: 2.413738965988159\nEpoch: 13, Steps: 157 | Train Loss: 0.2972218 Vali Loss: 0.0881786 Test Loss: 0.1997824\nValidation loss decreased (0.090106 --> 0.088179).  Saving model ...\nUpdating learning rate to 4.02416063364766e-06\n\titers: 100, epoch: 14 | loss: 0.2731684\n\tspeed: 0.0350s/iter; left time: 475.0419s\n[TRAIN MEMORY] Max memory allocated in epoch 14: 27.36 MB\nEpoch: 14 cost time: 2.449002504348755\nEpoch: 14, Steps: 157 | Train Loss: 0.2912418 Vali Loss: 0.0858929 Test Loss: 0.1931251\nValidation loss decreased (0.088179 --> 0.085893).  Saving model ...\nUpdating learning rate to 4.3370508007510885e-06\n\titers: 100, epoch: 15 | loss: 0.2845025\n\tspeed: 0.0349s/iter; left time: 467.3719s\n[TRAIN MEMORY] Max memory allocated in epoch 15: 27.36 MB\nEpoch: 15 cost time: 2.4270377159118652\nEpoch: 15, Steps: 157 | Train Loss: 0.2861154 Vali Loss: 0.0842721 Test Loss: 0.1877697\nValidation loss decreased (0.085893 --> 0.084272).  Saving model ...\nUpdating learning rate to 4.5503909646775556e-06\n\titers: 100, epoch: 16 | loss: 0.2452351\n\tspeed: 0.0341s/iter; left time: 451.5516s\n[TRAIN MEMORY] Max memory allocated in epoch 16: 27.36 MB\nEpoch: 16 cost time: 2.4018208980560303\nEpoch: 16, Steps: 157 | Train Loss: 0.2820730 Vali Loss: 0.0822985 Test Loss: 0.1838387\nValidation loss decreased (0.084272 --> 0.082298).  Saving model ...\nUpdating learning rate to 4.689000475645802e-06\n\titers: 100, epoch: 17 | loss: 0.2702791\n\tspeed: 0.0343s/iter; left time: 448.9744s\n[TRAIN MEMORY] Max memory allocated in epoch 17: 27.36 MB\nEpoch: 17 cost time: 2.3919565677642822\nEpoch: 17, Steps: 157 | Train Loss: 0.2790434 Vali Loss: 0.0818161 Test Loss: 0.1810531\nValidation loss decreased (0.082298 --> 0.081816).  Saving model ...\nUpdating learning rate to 4.775840063351174e-06\n\titers: 100, epoch: 18 | loss: 0.2773658\n\tspeed: 0.0350s/iter; left time: 453.1088s\n[TRAIN MEMORY] Max memory allocated in epoch 18: 27.36 MB\nEpoch: 18 cost time: 2.4471395015716553\nEpoch: 18, Steps: 157 | Train Loss: 0.2767923 Vali Loss: 0.0811175 Test Loss: 0.1791282\nValidation loss decreased (0.081816 --> 0.081117).  Saving model ...\nUpdating learning rate to 4.828556453332338e-06\n\titers: 100, epoch: 19 | loss: 0.2863592\n\tspeed: 0.0349s/iter; left time: 445.8576s\n[TRAIN MEMORY] Max memory allocated in epoch 19: 27.36 MB\nEpoch: 19 cost time: 2.363239049911499\nEpoch: 19, Steps: 157 | Train Loss: 0.2750864 Vali Loss: 0.0799350 Test Loss: 0.1780240\nValidation loss decreased (0.081117 --> 0.079935).  Saving model ...\nUpdating learning rate to 4.859445120268396e-06\n\titers: 100, epoch: 20 | loss: 0.2388278\n\tspeed: 0.0339s/iter; left time: 428.1668s\n[TRAIN MEMORY] Max memory allocated in epoch 20: 27.36 MB\nEpoch: 20 cost time: 2.363262414932251\nEpoch: 20, Steps: 157 | Train Loss: 0.2737792 Vali Loss: 0.0798545 Test Loss: 0.1772503\nValidation loss decreased (0.079935 --> 0.079854).  Saving model ...\nUpdating learning rate to 4.876604695568118e-06\n\titers: 100, epoch: 21 | loss: 0.2496101\n\tspeed: 0.0348s/iter; left time: 433.7161s\n[TRAIN MEMORY] Max memory allocated in epoch 21: 27.36 MB\nEpoch: 21 cost time: 2.38505220413208\nEpoch: 21, Steps: 157 | Train Loss: 0.2727546 Vali Loss: 0.0798250 Test Loss: 0.1765363\nValidation loss decreased (0.079854 --> 0.079825).  Saving model ...\nUpdating learning rate to 4.885194501735325e-06\n\titers: 100, epoch: 22 | loss: 0.2660763\n\tspeed: 0.0342s/iter; left time: 420.3454s\n[TRAIN MEMORY] Max memory allocated in epoch 22: 27.36 MB\nEpoch: 22 cost time: 2.338709831237793\nEpoch: 22, Steps: 157 | Train Loss: 0.2719293 Vali Loss: 0.0793475 Test Loss: 0.1761488\nValidation loss decreased (0.079825 --> 0.079347).  Saving model ...\nUpdating learning rate to 4.888435355546439e-06\n\titers: 100, epoch: 23 | loss: 0.2584207\n\tspeed: 0.0340s/iter; left time: 412.8445s\n[TRAIN MEMORY] Max memory allocated in epoch 23: 27.36 MB\nEpoch: 23 cost time: 2.3591339588165283\nEpoch: 23, Steps: 157 | Train Loss: 0.2712286 Vali Loss: 0.0787875 Test Loss: 0.1759884\nValidation loss decreased (0.079347 --> 0.078788).  Saving model ...\nUpdating learning rate to 4.888312366122913e-06\n\titers: 100, epoch: 24 | loss: 0.2824387\n\tspeed: 0.0344s/iter; left time: 412.6991s\n[TRAIN MEMORY] Max memory allocated in epoch 24: 27.36 MB\nEpoch: 24 cost time: 2.357633113861084\nEpoch: 24, Steps: 157 | Train Loss: 0.2706613 Vali Loss: 0.0787300 Test Loss: 0.1755974\nValidation loss decreased (0.078788 --> 0.078730).  Saving model ...\nUpdating learning rate to 4.886038389347345e-06\n\titers: 100, epoch: 25 | loss: 0.2674321\n\tspeed: 0.0345s/iter; left time: 407.7218s\n[TRAIN MEMORY] Max memory allocated in epoch 25: 27.36 MB\nEpoch: 25 cost time: 2.3631937503814697\nEpoch: 25, Steps: 157 | Train Loss: 0.2701612 Vali Loss: 0.0788631 Test Loss: 0.1753718\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 4.882349257265254e-06\n\titers: 100, epoch: 26 | loss: 0.2462926\n\tspeed: 0.0340s/iter; left time: 396.4777s\n[TRAIN MEMORY] Max memory allocated in epoch 26: 27.36 MB\nEpoch: 26 cost time: 2.3833088874816895\nEpoch: 26, Steps: 157 | Train Loss: 0.2697324 Vali Loss: 0.0787374 Test Loss: 0.1754214\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 4.877688142259323e-06\n\titers: 100, epoch: 27 | loss: 0.2424157\n\tspeed: 0.0347s/iter; left time: 399.3898s\n[TRAIN MEMORY] Max memory allocated in epoch 27: 27.36 MB\nEpoch: 27 cost time: 2.3380229473114014\nEpoch: 27, Steps: 157 | Train Loss: 0.2693580 Vali Loss: 0.0784530 Test Loss: 0.1750378\nValidation loss decreased (0.078730 --> 0.078453).  Saving model ...\nUpdating learning rate to 4.872319348975366e-06\n\titers: 100, epoch: 28 | loss: 0.2325882\n\tspeed: 0.0347s/iter; left time: 394.3502s\n[TRAIN MEMORY] Max memory allocated in epoch 28: 27.36 MB\nEpoch: 28 cost time: 2.4180376529693604\nEpoch: 28, Steps: 157 | Train Loss: 0.2690262 Vali Loss: 0.0783040 Test Loss: 0.1751256\nValidation loss decreased (0.078453 --> 0.078304).  Saving model ...\nUpdating learning rate to 4.86639805923574e-06\n\titers: 100, epoch: 29 | loss: 0.2920626\n\tspeed: 0.0341s/iter; left time: 381.8842s\n[TRAIN MEMORY] Max memory allocated in epoch 29: 27.36 MB\nEpoch: 29 cost time: 2.3978185653686523\nEpoch: 29, Steps: 157 | Train Loss: 0.2687131 Vali Loss: 0.0782895 Test Loss: 0.1750436\nValidation loss decreased (0.078304 --> 0.078289).  Saving model ...\nUpdating learning rate to 4.860012899942083e-06\n\titers: 100, epoch: 30 | loss: 0.2633758\n\tspeed: 0.0344s/iter; left time: 380.4327s\n[TRAIN MEMORY] Max memory allocated in epoch 30: 27.36 MB\nEpoch: 30 cost time: 2.327031373977661\nEpoch: 30, Steps: 157 | Train Loss: 0.2684366 Vali Loss: 0.0782109 Test Loss: 0.1750519\nValidation loss decreased (0.078289 --> 0.078211).  Saving model ...\nUpdating learning rate to 4.853211856899706e-06\n\titers: 100, epoch: 31 | loss: 0.2875145\n\tspeed: 0.0344s/iter; left time: 374.8647s\n[TRAIN MEMORY] Max memory allocated in epoch 31: 27.36 MB\nEpoch: 31 cost time: 2.4488747119903564\nEpoch: 31, Steps: 157 | Train Loss: 0.2681771 Vali Loss: 0.0780992 Test Loss: 0.1749866\nValidation loss decreased (0.078211 --> 0.078099).  Saving model ...\nUpdating learning rate to 4.846018024758687e-06\n\titers: 100, epoch: 32 | loss: 0.2744699\n\tspeed: 0.0353s/iter; left time: 378.6696s\n[TRAIN MEMORY] Max memory allocated in epoch 32: 27.36 MB\nEpoch: 32 cost time: 2.441132068634033\nEpoch: 32, Steps: 157 | Train Loss: 0.2679475 Vali Loss: 0.0780767 Test Loss: 0.1750632\nValidation loss decreased (0.078099 --> 0.078077).  Saving model ...\nUpdating learning rate to 4.838439169398508e-06\n\titers: 100, epoch: 33 | loss: 0.2466401\n\tspeed: 0.0352s/iter; left time: 372.3712s\n[TRAIN MEMORY] Max memory allocated in epoch 33: 27.36 MB\nEpoch: 33 cost time: 2.439737319946289\nEpoch: 33, Steps: 157 | Train Loss: 0.2677424 Vali Loss: 0.0777832 Test Loss: 0.1749902\nValidation loss decreased (0.078077 --> 0.077783).  Saving model ...\nUpdating learning rate to 4.830473529249205e-06\n\titers: 100, epoch: 34 | loss: 0.2953156\n\tspeed: 0.0347s/iter; left time: 361.7509s\n[TRAIN MEMORY] Max memory allocated in epoch 34: 27.36 MB\nEpoch: 34 cost time: 2.406534433364868\nEpoch: 34, Steps: 157 | Train Loss: 0.2675322 Vali Loss: 0.0777402 Test Loss: 0.1748351\nValidation loss decreased (0.077783 --> 0.077740).  Saving model ...\nUpdating learning rate to 4.822113332763808e-06\n\titers: 100, epoch: 35 | loss: 0.2741373\n\tspeed: 0.0353s/iter; left time: 362.2760s\n[TRAIN MEMORY] Max memory allocated in epoch 35: 27.36 MB\nEpoch: 35 cost time: 2.4779436588287354\nEpoch: 35, Steps: 157 | Train Loss: 0.2673444 Vali Loss: 0.0779794 Test Loss: 0.1747779\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 4.813346930082932e-06\n\titers: 100, epoch: 36 | loss: 0.2841764\n\tspeed: 0.0346s/iter; left time: 349.3496s\n[TRAIN MEMORY] Max memory allocated in epoch 36: 27.36 MB\nEpoch: 36 cost time: 2.391324281692505\nEpoch: 36, Steps: 157 | Train Loss: 0.2671712 Vali Loss: 0.0780468 Test Loss: 0.1748614\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 4.8041600843946895e-06\n\titers: 100, epoch: 37 | loss: 0.2561905\n\tspeed: 0.0341s/iter; left time: 338.9019s\n[TRAIN MEMORY] Max memory allocated in epoch 37: 27.36 MB\nEpoch: 37 cost time: 2.404136896133423\nEpoch: 37, Steps: 157 | Train Loss: 0.2669990 Vali Loss: 0.0775910 Test Loss: 0.1747955\nValidation loss decreased (0.077740 --> 0.077591).  Saving model ...\nUpdating learning rate to 4.794536754211641e-06\n\titers: 100, epoch: 38 | loss: 0.3340164\n\tspeed: 0.0359s/iter; left time: 351.6457s\n[TRAIN MEMORY] Max memory allocated in epoch 38: 27.36 MB\nEpoch: 38 cost time: 2.477553606033325\nEpoch: 38, Steps: 157 | Train Loss: 0.2668497 Vali Loss: 0.0775293 Test Loss: 0.1746991\nValidation loss decreased (0.077591 --> 0.077529).  Saving model ...\nUpdating learning rate to 4.784459567654431e-06\n\titers: 100, epoch: 39 | loss: 0.2524397\n\tspeed: 0.0348s/iter; left time: 335.4108s\n[TRAIN MEMORY] Max memory allocated in epoch 39: 27.36 MB\nEpoch: 39 cost time: 2.4001681804656982\nEpoch: 39, Steps: 157 | Train Loss: 0.2667023 Vali Loss: 0.0776251 Test Loss: 0.1745087\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 4.773910110846522e-06\n\titers: 100, epoch: 40 | loss: 0.3233046\n\tspeed: 0.0340s/iter; left time: 322.4794s\n[TRAIN MEMORY] Max memory allocated in epoch 40: 27.36 MB\nEpoch: 40 cost time: 2.334644317626953\nEpoch: 40, Steps: 157 | Train Loss: 0.2665619 Vali Loss: 0.0775346 Test Loss: 0.1747089\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 4.762869104601031e-06\n\titers: 100, epoch: 41 | loss: 0.2249317\n\tspeed: 0.0349s/iter; left time: 325.6166s\n[TRAIN MEMORY] Max memory allocated in epoch 41: 27.36 MB\nEpoch: 41 cost time: 2.4850335121154785\nEpoch: 41, Steps: 157 | Train Loss: 0.2664194 Vali Loss: 0.0775807 Test Loss: 0.1747572\nEarlyStopping counter: 3 out of 5\nUpdating learning rate to 4.751316514511707e-06\n\titers: 100, epoch: 42 | loss: 0.2837346\n\tspeed: 0.0348s/iter; left time: 318.9625s\n[TRAIN MEMORY] Max memory allocated in epoch 42: 27.36 MB\nEpoch: 42 cost time: 2.3558359146118164\nEpoch: 42, Steps: 157 | Train Loss: 0.2662937 Vali Loss: 0.0774819 Test Loss: 0.1748387\nValidation loss decreased (0.077529 --> 0.077482).  Saving model ...\nUpdating learning rate to 4.739231621932101e-06\n\titers: 100, epoch: 43 | loss: 0.2884252\n\tspeed: 0.0344s/iter; left time: 309.7546s\n[TRAIN MEMORY] Max memory allocated in epoch 43: 27.36 MB\nEpoch: 43 cost time: 2.4017581939697266\nEpoch: 43, Steps: 157 | Train Loss: 0.2661637 Vali Loss: 0.0773272 Test Loss: 0.1747237\nValidation loss decreased (0.077482 --> 0.077327).  Saving model ...\nUpdating learning rate to 4.726593072640151e-06\n\titers: 100, epoch: 44 | loss: 0.3160983\n\tspeed: 0.0342s/iter; left time: 302.7803s\n[TRAIN MEMORY] Max memory allocated in epoch 44: 27.36 MB\nEpoch: 44 cost time: 2.331742286682129\nEpoch: 44, Steps: 157 | Train Loss: 0.2660503 Vali Loss: 0.0775150 Test Loss: 0.1748012\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 4.713378913508779e-06\n\titers: 100, epoch: 45 | loss: 0.2496421\n\tspeed: 0.0341s/iter; left time: 296.0262s\n[TRAIN MEMORY] Max memory allocated in epoch 45: 27.36 MB\nEpoch: 45 cost time: 2.3344345092773438\nEpoch: 45, Steps: 157 | Train Loss: 0.2659302 Vali Loss: 0.0776449 Test Loss: 0.1748547\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 4.699566623580007e-06\n\titers: 100, epoch: 46 | loss: 0.2736322\n\tspeed: 0.0342s/iter; left time: 292.0789s\n[TRAIN MEMORY] Max memory allocated in epoch 46: 27.36 MB\nEpoch: 46 cost time: 2.362994432449341\nEpoch: 46, Steps: 157 | Train Loss: 0.2658331 Vali Loss: 0.0775953 Test Loss: 0.1748140\nEarlyStopping counter: 3 out of 5\nUpdating learning rate to 4.685133143565121e-06\n\titers: 100, epoch: 47 | loss: 0.2749011\n\tspeed: 0.0347s/iter; left time: 290.7743s\n[TRAIN MEMORY] Max memory allocated in epoch 47: 27.36 MB\nEpoch: 47 cost time: 2.401310920715332\nEpoch: 47, Steps: 157 | Train Loss: 0.2657255 Vali Loss: 0.0775093 Test Loss: 0.1746064\nEarlyStopping counter: 4 out of 5\nUpdating learning rate to 4.6700549063566586e-06\n\titers: 100, epoch: 48 | loss: 0.2871390\n\tspeed: 0.0347s/iter; left time: 285.6377s\n[TRAIN MEMORY] Max memory allocated in epoch 48: 27.36 MB\nEpoch: 48 cost time: 2.384938955307007\nEpoch: 48, Steps: 157 | Train Loss: 0.2656313 Vali Loss: 0.0772726 Test Loss: 0.1745028\nValidation loss decreased (0.077327 --> 0.077273).  Saving model ...\nUpdating learning rate to 4.654307870269284e-06\n\titers: 100, epoch: 49 | loss: 0.2664123\n\tspeed: 0.0345s/iter; left time: 278.0731s\n[TRAIN MEMORY] Max memory allocated in epoch 49: 27.36 MB\nEpoch: 49 cost time: 2.3910634517669678\nEpoch: 49, Steps: 157 | Train Loss: 0.2655264 Vali Loss: 0.0772632 Test Loss: 0.1747082\nValidation loss decreased (0.077273 --> 0.077263).  Saving model ...\nUpdating learning rate to 4.637867556201073e-06\n\titers: 100, epoch: 50 | loss: 0.2475505\n\tspeed: 0.0346s/iter; left time: 273.3018s\n[TRAIN MEMORY] Max memory allocated in epoch 50: 27.36 MB\nEpoch: 50 cost time: 2.361795663833618\nEpoch: 50, Steps: 157 | Train Loss: 0.2654397 Vali Loss: 0.0773522 Test Loss: 0.1745872\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 4.620709089588014e-06\n\titers: 100, epoch: 51 | loss: 0.2319853\n\tspeed: 0.0340s/iter; left time: 263.4165s\n[TRAIN MEMORY] Max memory allocated in epoch 51: 27.36 MB\nEpoch: 51 cost time: 2.3739869594573975\nEpoch: 51, Steps: 157 | Train Loss: 0.2653483 Vali Loss: 0.0775460 Test Loss: 0.1747095\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 4.602807247829344e-06\n\titers: 100, epoch: 52 | loss: 0.2552806\n\tspeed: 0.0348s/iter; left time: 264.2842s\n[TRAIN MEMORY] Max memory allocated in epoch 52: 27.36 MB\nEpoch: 52 cost time: 2.4398300647735596\nEpoch: 52, Steps: 157 | Train Loss: 0.2652620 Vali Loss: 0.0772294 Test Loss: 0.1747830\nValidation loss decreased (0.077263 --> 0.077229).  Saving model ...\nUpdating learning rate to 4.584136513739108e-06\n\titers: 100, epoch: 53 | loss: 0.2573090\n\tspeed: 0.0344s/iter; left time: 256.1209s\n[TRAIN MEMORY] Max memory allocated in epoch 53: 27.36 MB\nEpoch: 53 cost time: 2.32277250289917\nEpoch: 53, Steps: 157 | Train Loss: 0.2651765 Vali Loss: 0.0772806 Test Loss: 0.1747833\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 4.564671135499116e-06\n\titers: 100, epoch: 54 | loss: 0.2673478\n\tspeed: 0.0345s/iter; left time: 251.2493s\n[TRAIN MEMORY] Max memory allocated in epoch 54: 27.36 MB\nEpoch: 54 cost time: 2.4121134281158447\nEpoch: 54, Steps: 157 | Train Loss: 0.2650924 Vali Loss: 0.0773020 Test Loss: 0.1748572\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 4.5443851935309855e-06\n\titers: 100, epoch: 55 | loss: 0.2886986\n\tspeed: 0.0340s/iter; left time: 242.1778s\n[TRAIN MEMORY] Max memory allocated in epoch 55: 27.36 MB\nEpoch: 55 cost time: 2.3440728187561035\nEpoch: 55, Steps: 157 | Train Loss: 0.2650281 Vali Loss: 0.0769582 Test Loss: 0.1747466\nValidation loss decreased (0.077229 --> 0.076958).  Saving model ...\nUpdating learning rate to 4.523252674658504e-06\n\titers: 100, epoch: 56 | loss: 0.2601056\n\tspeed: 0.0338s/iter; left time: 235.1525s\n[TRAIN MEMORY] Max memory allocated in epoch 56: 27.36 MB\nEpoch: 56 cost time: 2.307234048843384\nEpoch: 56, Steps: 157 | Train Loss: 0.2649413 Vali Loss: 0.0773348 Test Loss: 0.1749846\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 4.501247553888481e-06\n\titers: 100, epoch: 57 | loss: 0.2774276\n\tspeed: 0.0346s/iter; left time: 235.4682s\n[TRAIN MEMORY] Max memory allocated in epoch 57: 27.36 MB\nEpoch: 57 cost time: 2.4272608757019043\nEpoch: 57, Steps: 157 | Train Loss: 0.2648517 Vali Loss: 0.0773040 Test Loss: 0.1744773\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 4.478343884093786e-06\n\titers: 100, epoch: 58 | loss: 0.2594079\n\tspeed: 0.0346s/iter; left time: 229.8756s\n[TRAIN MEMORY] Max memory allocated in epoch 58: 27.36 MB\nEpoch: 58 cost time: 2.403625726699829\nEpoch: 58, Steps: 157 | Train Loss: 0.2647919 Vali Loss: 0.0773951 Test Loss: 0.1747064\nEarlyStopping counter: 3 out of 5\nUpdating learning rate to 4.454515893833178e-06\n\titers: 100, epoch: 59 | loss: 0.2594405\n\tspeed: 0.0340s/iter; left time: 220.7416s\n[TRAIN MEMORY] Max memory allocated in epoch 59: 27.36 MB\nEpoch: 59 cost time: 2.3213987350463867\nEpoch: 59, Steps: 157 | Train Loss: 0.2647207 Vali Loss: 0.0770195 Test Loss: 0.1747471\nEarlyStopping counter: 4 out of 5\nUpdating learning rate to 4.4297380934865604e-06\n\titers: 100, epoch: 60 | loss: 0.2877764\n\tspeed: 0.0367s/iter; left time: 232.8301s\n[TRAIN MEMORY] Max memory allocated in epoch 60: 27.36 MB\nEpoch: 60 cost time: 2.3687326908111572\nEpoch: 60, Steps: 157 | Train Loss: 0.2646572 Vali Loss: 0.0773230 Test Loss: 0.1748471\nEarlyStopping counter: 5 out of 5\nEarly stopping\nAverage epoch time: 2.40 seconds\n>>>>>>>testing : transformer_test_xPatch_custom_ftM_sl96_ll48_pl192_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1326\n\n--- Model Statistics ---\n[MODEL SIZE] Model file size: 1.52 MB\n[PARAMS] Total parameters: 396,280\nmse:0.17474661767482758, mae:0.29635393619537354\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"!python run.py --is_training 1 --model xPatch --model_id transformer_test --ma_type wma --data custom --learning_rate 0.00005 --data_path exchange_rate.csv --seq_len 96 --pred_len 336 --patience 5 --enc_in 8 --dropout 0 --batch_size 16 --lradj 'sigmoid' --period_len 1 --d_model 256 --use_multi_gpu --devices '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T07:45:48.075186Z","iopub.execute_input":"2025-10-22T07:45:48.075885Z","iopub.status.idle":"2025-10-22T07:46:53.227665Z","shell.execute_reply.started":"2025-10-22T07:45:48.075856Z","shell.execute_reply":"2025-10-22T07:46:53.226725Z"}},"outputs":[{"name":"stdout","text":"Args in experiment:\nNamespace(is_training=1, train_only=False, model_id='transformer_test', model='xPatch', period_len=1, d_model=256, data='custom', root_path='./dataset', data_path='exchange_rate.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', embed='timeF', seq_len=96, label_len=48, pred_len=336, enc_in=8, patch_len=48, stride=8, padding_patch='end', ma_type='wma', alpha=0.3, beta=0.3, num_workers=10, itr=1, train_epochs=100, batch_size=16, patience=5, learning_rate=5e-05, des='test', loss='mse', lradj='sigmoid', use_amp=False, revin=1, dropout=0.0, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, dvices='0,1', device_ids=[0, 1])\nUse GPU: cuda:0\n>>>>>>>start training : transformer_test_xPatch_custom_ftM_sl96_ll48_pl336_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4880\nval 425\ntest 1182\n\titers: 100, epoch: 1 | loss: 0.3416228\n\tspeed: 0.0342s/iter; left time: 1040.2387s\n\titers: 200, epoch: 1 | loss: 0.3972996\n\tspeed: 0.0137s/iter; left time: 416.0005s\n\titers: 300, epoch: 1 | loss: 0.4115866\n\tspeed: 0.0135s/iter; left time: 407.3150s\n[TRAIN MEMORY] Max memory allocated in epoch 1: 34.89 MB\nEpoch: 1 cost time: 5.189918756484985\nEpoch: 1, Steps: 305 | Train Loss: 0.3685347 Vali Loss: 0.0983117 Test Loss: 0.3293597\nValidation loss decreased (inf --> 0.098312).  Saving model ...\nUpdating learning rate to 1.9766777377990105e-07\n\titers: 100, epoch: 2 | loss: 0.3473456\n\tspeed: 0.0312s/iter; left time: 939.8772s\n\titers: 200, epoch: 2 | loss: 0.3414964\n\tspeed: 0.0139s/iter; left time: 417.3368s\n\titers: 300, epoch: 2 | loss: 0.3693007\n\tspeed: 0.0133s/iter; left time: 398.9433s\n[TRAIN MEMORY] Max memory allocated in epoch 2: 34.89 MB\nEpoch: 2 cost time: 4.459363698959351\nEpoch: 2, Steps: 305 | Train Loss: 0.3588497 Vali Loss: 0.0980993 Test Loss: 0.3292731\nValidation loss decreased (0.098312 --> 0.098099).  Saving model ...\nUpdating learning rate to 5.297334308904794e-07\n\titers: 100, epoch: 3 | loss: 0.3084077\n\tspeed: 0.0312s/iter; left time: 929.6056s\n\titers: 200, epoch: 3 | loss: 0.2990861\n\tspeed: 0.0132s/iter; left time: 391.9248s\n\titers: 300, epoch: 3 | loss: 0.3486243\n\tspeed: 0.0136s/iter; left time: 402.6455s\n[TRAIN MEMORY] Max memory allocated in epoch 3: 34.89 MB\nEpoch: 3 cost time: 4.400060415267944\nEpoch: 3, Steps: 305 | Train Loss: 0.3587802 Vali Loss: 0.0980416 Test Loss: 0.3290694\nValidation loss decreased (0.098099 --> 0.098042).  Saving model ...\nUpdating learning rate to 1.0772330336670663e-06\n\titers: 100, epoch: 4 | loss: 0.3570293\n\tspeed: 0.0315s/iter; left time: 928.2044s\n\titers: 200, epoch: 4 | loss: 0.3885521\n\tspeed: 0.0134s/iter; left time: 394.4576s\n\titers: 300, epoch: 4 | loss: 0.3580327\n\tspeed: 0.0134s/iter; left time: 390.9822s\n[TRAIN MEMORY] Max memory allocated in epoch 4: 34.89 MB\nEpoch: 4 cost time: 4.426705598831177\nEpoch: 4, Steps: 305 | Train Loss: 0.3586682 Vali Loss: 0.0977477 Test Loss: 0.3289416\nValidation loss decreased (0.098042 --> 0.097748).  Saving model ...\nUpdating learning rate to 1.9631651012203446e-06\n\titers: 100, epoch: 5 | loss: 0.3405804\n\tspeed: 0.0322s/iter; left time: 940.5373s\n\titers: 200, epoch: 5 | loss: 0.3857463\n\tspeed: 0.0135s/iter; left time: 391.4212s\n\titers: 300, epoch: 5 | loss: 0.3232710\n\tspeed: 0.0135s/iter; left time: 392.6566s\n[TRAIN MEMORY] Max memory allocated in epoch 5: 34.89 MB\nEpoch: 5 cost time: 4.424922466278076\nEpoch: 5, Steps: 305 | Train Loss: 0.3585153 Vali Loss: 0.0978405 Test Loss: 0.3286748\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 3.3640347303765784e-06\n\titers: 100, epoch: 6 | loss: 0.3729102\n\tspeed: 0.0316s/iter; left time: 911.5506s\n\titers: 200, epoch: 6 | loss: 0.4652543\n\tspeed: 0.0139s/iter; left time: 400.2400s\n\titers: 300, epoch: 6 | loss: 0.3155155\n\tspeed: 0.0133s/iter; left time: 382.1040s\n[TRAIN MEMORY] Max memory allocated in epoch 6: 34.89 MB\nEpoch: 6 cost time: 4.455005168914795\nEpoch: 6, Steps: 305 | Train Loss: 0.3583371 Vali Loss: 0.0976586 Test Loss: 0.3293633\nValidation loss decreased (0.097748 --> 0.097659).  Saving model ...\nUpdating learning rate to 5.509481168463486e-06\n\titers: 100, epoch: 7 | loss: 0.3291889\n\tspeed: 0.0308s/iter; left time: 881.1996s\n\titers: 200, epoch: 7 | loss: 0.3309073\n\tspeed: 0.0134s/iter; left time: 381.0725s\n\titers: 300, epoch: 7 | loss: 0.3817479\n\tspeed: 0.0135s/iter; left time: 381.7539s\n[TRAIN MEMORY] Max memory allocated in epoch 7: 34.89 MB\nEpoch: 7 cost time: 4.367509603500366\nEpoch: 7, Steps: 305 | Train Loss: 0.3580932 Vali Loss: 0.0981317 Test Loss: 0.3301298\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 8.647724011220513e-06\n\titers: 100, epoch: 8 | loss: 0.3794002\n\tspeed: 0.0318s/iter; left time: 898.2002s\n\titers: 200, epoch: 8 | loss: 0.3191209\n\tspeed: 0.0140s/iter; left time: 393.4494s\n\titers: 300, epoch: 8 | loss: 0.3110834\n\tspeed: 0.0133s/iter; left time: 373.2025s\n[TRAIN MEMORY] Max memory allocated in epoch 8: 34.89 MB\nEpoch: 8 cost time: 4.468735694885254\nEpoch: 8, Steps: 305 | Train Loss: 0.3577469 Vali Loss: 0.0976935 Test Loss: 0.3319104\nEarlyStopping counter: 2 out of 5\nUpdating learning rate to 1.2949480975154542e-05\n\titers: 100, epoch: 9 | loss: 0.3750009\n\tspeed: 0.0315s/iter; left time: 881.1342s\n\titers: 200, epoch: 9 | loss: 0.3802626\n\tspeed: 0.0135s/iter; left time: 374.9215s\n\titers: 300, epoch: 9 | loss: 0.3913324\n\tspeed: 0.0133s/iter; left time: 368.2800s\n[TRAIN MEMORY] Max memory allocated in epoch 9: 34.89 MB\nEpoch: 9 cost time: 4.411479711532593\nEpoch: 9, Steps: 305 | Train Loss: 0.3573537 Vali Loss: 0.0972331 Test Loss: 0.3327692\nValidation loss decreased (0.097659 --> 0.097233).  Saving model ...\nUpdating learning rate to 1.835419812831137e-05\n\titers: 100, epoch: 10 | loss: 0.3072688\n\tspeed: 0.0314s/iter; left time: 867.4026s\n\titers: 200, epoch: 10 | loss: 0.2831307\n\tspeed: 0.0132s/iter; left time: 363.5466s\n\titers: 300, epoch: 10 | loss: 0.3604666\n\tspeed: 0.0134s/iter; left time: 367.9934s\n[TRAIN MEMORY] Max memory allocated in epoch 10: 34.89 MB\nEpoch: 10 cost time: 4.344759702682495\nEpoch: 10, Steps: 305 | Train Loss: 0.3568479 Vali Loss: 0.0972582 Test Loss: 0.3329598\nEarlyStopping counter: 1 out of 5\nUpdating learning rate to 2.4450652868470343e-05\n^C\nTraceback (most recent call last):\n  File \"/kaggle/working/xPatch/run.py\", line 109, in <module>\n    exp.train(setting)\n  File \"/kaggle/working/xPatch/exp/exp_main.py\", line 171, in train\n    outputs = self.model(batch_x)\n              ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\", line 193, in forward\n    outputs = self.parallel_apply(replicas, inputs, module_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\", line 212, in parallel_apply\n    return parallel_apply(\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 116, in parallel_apply\n    thread.start()\n  File \"/usr/lib/python3.11/threading.py\", line 969, in start\n    self._started.wait()\n  File \"/usr/lib/python3.11/threading.py\", line 629, in wait\n    signaled = self._cond.wait(timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/threading.py\", line 327, in wait\n    waiter.acquire()\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import optuna\nimport subprocess\n\ndef objective(trial):\n    learning_rate = trial.suggest_float('learning_rate', 1e-3, 9e-3)\n    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n    dropout = trial.suggest_float('dropout', 0.0, 0.3)\n\n    command = [\n        'python', 'run.py',\n        '--is_training', '1',\n        '--model', 'xPatch',\n        '--model_id', 'transformer_test',\n        '--ma_type', 'wma',\n        '--data', 'Solar',\n        '--learning_rate', str(learning_rate),\n        '--data_path', 'solar.txt',\n        '--seq_len', '720',\n        '--pred_len', '336',\n        '--period_len', '6',\n        '--enc_in', '137',\n        '--batch_size', str(batch_size),\n        '--dropout', str(dropout),\n        '--d_model', '128',\n        '--lradj', 'sigmoid',\n        '--use_multi_gpu',\n        '--devices', '0,1'\n    ]\n\n    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    output = result.stdout.decode()\n\n    mse, mae = extract_test_loss(output)\n    trial.set_user_attr('mae', mae)\n\n    print(f\"Trial {trial.number} finished:\")\n    print(f\"  Params: learning_rate={learning_rate}, batch_size={batch_size}, dropout={dropout}\")\n    print(f\"  MSE: {mse}\")\n    print(f\"  MAE: {mae}\\n\")\n\n    return mse  \nimport re\n\ndef extract_test_loss(output):\n    # match = re.search(r'Test Loss: ([0-9]*\\.?[0-9]+)', output)\n    # if match:\n    #     return float(match.group(1))\n    mse = None\n    match = re.search(r'mse:([0-9]*\\.?[0-9]+)', output)\n    if match:\n        mse = float(match.group(1))\n\n    mae = None\n    match = re.search(r'mae:([0-9]*\\.?[0-9]+)', output)\n    if match:\n        mae = float(match.group(1))\n    \n    return mse, mae\n\n\nstudy = optuna.create_study(direction='minimize')  \nstudy.optimize(objective, n_trials=10)\n\nprint(f\"Best parameters: {study.best_params}\")\nprint(f\"Best Test Loss: {study.best_value}\")\nprint(f\"Corresponding MAE: {study.best_trial.user_attrs['mae']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:12:12.588745Z","iopub.execute_input":"2025-10-20T17:12:12.589042Z","iopub.status.idle":"2025-10-20T21:30:48.306107Z","shell.execute_reply.started":"2025-10-20T17:12:12.589017Z","shell.execute_reply":"2025-10-20T21:30:48.305351Z"}},"outputs":[{"name":"stderr","text":"[I 2025-10-20 17:12:12,957] A new study created in memory with name: no-name-90b8edb7-46ec-4696-b598-c7b9d20e58f7\n[I 2025-10-20 17:32:27,146] Trial 0 finished with value: 0.2092823088169098 and parameters: {'learning_rate': 0.007261784748083803, 'batch_size': 32, 'dropout': 0.22611893104072384}. Best is trial 0 with value: 0.2092823088169098.\n","output_type":"stream"},{"name":"stdout","text":"Trial 0 finished:\n  Params: learning_rate=0.007261784748083803, batch_size=32, dropout=0.22611893104072384\n  MSE: 0.2092823088169098\n  MAE: 0.22057713568210602\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 17:51:24,032] Trial 1 finished with value: 0.20720736682415009 and parameters: {'learning_rate': 0.0012993099554097513, 'batch_size': 128, 'dropout': 0.1854885340712285}. Best is trial 1 with value: 0.20720736682415009.\n","output_type":"stream"},{"name":"stdout","text":"Trial 1 finished:\n  Params: learning_rate=0.0012993099554097513, batch_size=128, dropout=0.1854885340712285\n  MSE: 0.20720736682415009\n  MAE: 0.21882843971252441\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 18:11:27,892] Trial 2 finished with value: 0.20877335965633392 and parameters: {'learning_rate': 0.00844239292935512, 'batch_size': 32, 'dropout': 0.17666494589098122}. Best is trial 1 with value: 0.20720736682415009.\n","output_type":"stream"},{"name":"stdout","text":"Trial 2 finished:\n  Params: learning_rate=0.00844239292935512, batch_size=32, dropout=0.17666494589098122\n  MSE: 0.20877335965633392\n  MAE: 0.22052359580993652\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 18:30:21,908] Trial 3 finished with value: 0.20092272758483887 and parameters: {'learning_rate': 0.002697086288273036, 'batch_size': 128, 'dropout': 0.19049472328033862}. Best is trial 3 with value: 0.20092272758483887.\n","output_type":"stream"},{"name":"stdout","text":"Trial 3 finished:\n  Params: learning_rate=0.002697086288273036, batch_size=128, dropout=0.19049472328033862\n  MSE: 0.20092272758483887\n  MAE: 0.21779343485832214\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 19:10:21,601] Trial 4 finished with value: 0.22757302224636078 and parameters: {'learning_rate': 0.008240802823253388, 'batch_size': 128, 'dropout': 0.29520638748490285}. Best is trial 3 with value: 0.20092272758483887.\n","output_type":"stream"},{"name":"stdout","text":"Trial 4 finished:\n  Params: learning_rate=0.008240802823253388, batch_size=128, dropout=0.29520638748490285\n  MSE: 0.22757302224636078\n  MAE: 0.22917728126049042\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 19:45:12,838] Trial 5 finished with value: 0.20534831285476685 and parameters: {'learning_rate': 0.006161606308735051, 'batch_size': 128, 'dropout': 0.06804926049557647}. Best is trial 3 with value: 0.20092272758483887.\n","output_type":"stream"},{"name":"stdout","text":"Trial 5 finished:\n  Params: learning_rate=0.006161606308735051, batch_size=128, dropout=0.06804926049557647\n  MSE: 0.20534831285476685\n  MAE: 0.22273769974708557\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 20:05:19,993] Trial 6 finished with value: 0.2195960283279419 and parameters: {'learning_rate': 0.006405633673769745, 'batch_size': 32, 'dropout': 0.03304492639872062}. Best is trial 3 with value: 0.20092272758483887.\n","output_type":"stream"},{"name":"stdout","text":"Trial 6 finished:\n  Params: learning_rate=0.006405633673769745, batch_size=32, dropout=0.03304492639872062\n  MSE: 0.2195960283279419\n  MAE: 0.22309952974319458\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 20:25:29,720] Trial 7 finished with value: 0.20915691554546356 and parameters: {'learning_rate': 0.008561160657065626, 'batch_size': 32, 'dropout': 0.1808791838313536}. Best is trial 3 with value: 0.20092272758483887.\n","output_type":"stream"},{"name":"stdout","text":"Trial 7 finished:\n  Params: learning_rate=0.008561160657065626, batch_size=32, dropout=0.1808791838313536\n  MSE: 0.20915691554546356\n  MAE: 0.2211609184741974\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 21:11:00,527] Trial 8 finished with value: 0.2066275030374527 and parameters: {'learning_rate': 0.005413253660526275, 'batch_size': 128, 'dropout': 0.04354500980111179}. Best is trial 3 with value: 0.20092272758483887.\n","output_type":"stream"},{"name":"stdout","text":"Trial 8 finished:\n  Params: learning_rate=0.005413253660526275, batch_size=128, dropout=0.04354500980111179\n  MSE: 0.2066275030374527\n  MAE: 0.22223573923110962\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 21:30:48,301] Trial 9 finished with value: 0.21056117117404938 and parameters: {'learning_rate': 0.008792179901702581, 'batch_size': 32, 'dropout': 0.2727959154413893}. Best is trial 3 with value: 0.20092272758483887.\n","output_type":"stream"},{"name":"stdout","text":"Trial 9 finished:\n  Params: learning_rate=0.008792179901702581, batch_size=32, dropout=0.2727959154413893\n  MSE: 0.21056117117404938\n  MAE: 0.22153320908546448\n\nBest parameters: {'learning_rate': 0.002697086288273036, 'batch_size': 128, 'dropout': 0.19049472328033862}\nBest Test Loss: 0.20092272758483887\nCorresponding MAE: 0.21779343485832214\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import optuna\nimport subprocess\n\ndef objective(trial):\n    learning_rate = trial.suggest_float('learning_rate', 1e-3, 9e-3)\n    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n    dropout = trial.suggest_float('dropout', 0.0, 0.3)\n\n    command = [\n        'python', 'run.py',\n        '--is_training', '1',\n        '--model', 'xPatch',\n        '--model_id', 'transformer_test',\n        '--ma_type', 'wma',\n        '--data', 'Solar',\n        '--learning_rate', str(learning_rate),\n        '--data_path', 'solar.txt',\n        '--seq_len', '720',\n        '--pred_len', '720',\n        '--period_len', '6',\n        '--enc_in', '137',\n        '--batch_size', str(batch_size),\n        '--dropout', str(dropout),\n        '--d_model', '128',\n        '--lradj', 'sigmoid',\n        '--use_multi_gpu',\n        '--devices', '0,1'\n    ]\n\n    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    output = result.stdout.decode()\n\n    mse, mae = extract_test_loss(output)\n    trial.set_user_attr('mae', mae)\n\n    print(f\"Trial {trial.number} finished:\")\n    print(f\"  Params: learning_rate={learning_rate}, batch_size={batch_size}, dropout={dropout}\")\n    print(f\"  MSE: {mse}\")\n    print(f\"  MAE: {mae}\\n\")\n\n    return mse  \nimport re\n\ndef extract_test_loss(output):\n    # match = re.search(r'Test Loss: ([0-9]*\\.?[0-9]+)', output)\n    # if match:\n    #     return float(match.group(1))\n    mse = None\n    match = re.search(r'mse:([0-9]*\\.?[0-9]+)', output)\n    if match:\n        mse = float(match.group(1))\n\n    mae = None\n    match = re.search(r'mae:([0-9]*\\.?[0-9]+)', output)\n    if match:\n        mae = float(match.group(1))\n    \n    return mse, mae\n\n\nstudy = optuna.create_study(direction='minimize')  \nstudy.optimize(objective, n_trials=10)\n\nprint(f\"Best parameters: {study.best_params}\")\nprint(f\"Best Test Loss: {study.best_value}\")\nprint(f\"Corresponding MAE: {study.best_trial.user_attrs['mae']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T21:30:48.307246Z","iopub.execute_input":"2025-10-20T21:30:48.307447Z","execution_failed":"2025-10-21T01:55:03.894Z"}},"outputs":[{"name":"stderr","text":"[I 2025-10-20 21:30:48,312] A new study created in memory with name: no-name-ba4b7f8b-d0ae-498e-8d84-01e32d93bf5a\n[I 2025-10-20 21:54:31,157] Trial 0 finished with value: 0.22531536221504211 and parameters: {'learning_rate': 0.005723619778331879, 'batch_size': 64, 'dropout': 0.1338720923463781}. Best is trial 0 with value: 0.22531536221504211.\n","output_type":"stream"},{"name":"stdout","text":"Trial 0 finished:\n  Params: learning_rate=0.005723619778331879, batch_size=64, dropout=0.1338720923463781\n  MSE: 0.22531536221504211\n  MAE: 0.2303074151277542\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 22:18:19,708] Trial 1 finished with value: 0.21869970858097076 and parameters: {'learning_rate': 0.004618726421311829, 'batch_size': 32, 'dropout': 0.035446284292643127}. Best is trial 1 with value: 0.21869970858097076.\n","output_type":"stream"},{"name":"stdout","text":"Trial 1 finished:\n  Params: learning_rate=0.004618726421311829, batch_size=32, dropout=0.035446284292643127\n  MSE: 0.21869970858097076\n  MAE: 0.22499991953372955\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 23:13:34,529] Trial 2 finished with value: 0.21466869115829468 and parameters: {'learning_rate': 0.006749991849723282, 'batch_size': 128, 'dropout': 0.04259279249432768}. Best is trial 2 with value: 0.21466869115829468.\n","output_type":"stream"},{"name":"stdout","text":"Trial 2 finished:\n  Params: learning_rate=0.006749991849723282, batch_size=128, dropout=0.04259279249432768\n  MSE: 0.21466869115829468\n  MAE: 0.22793444991111755\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-20 23:51:23,981] Trial 3 finished with value: 0.22832220792770386 and parameters: {'learning_rate': 0.005550210602950566, 'batch_size': 64, 'dropout': 0.2720918619033171}. Best is trial 2 with value: 0.21466869115829468.\n","output_type":"stream"},{"name":"stdout","text":"Trial 3 finished:\n  Params: learning_rate=0.005550210602950566, batch_size=64, dropout=0.2720918619033171\n  MSE: 0.22832220792770386\n  MAE: 0.23565629124641418\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-21 00:12:55,960] Trial 4 finished with value: 0.21431562304496765 and parameters: {'learning_rate': 0.0020087105421181944, 'batch_size': 64, 'dropout': 0.2300127676083211}. Best is trial 4 with value: 0.21431562304496765.\n","output_type":"stream"},{"name":"stdout","text":"Trial 4 finished:\n  Params: learning_rate=0.0020087105421181944, batch_size=64, dropout=0.2300127676083211\n  MSE: 0.21431562304496765\n  MAE: 0.22165313363075256\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-21 00:36:53,485] Trial 5 finished with value: 0.2138867974281311 and parameters: {'learning_rate': 0.0010506126156340106, 'batch_size': 64, 'dropout': 0.18671740510715432}. Best is trial 5 with value: 0.2138867974281311.\n","output_type":"stream"},{"name":"stdout","text":"Trial 5 finished:\n  Params: learning_rate=0.0010506126156340106, batch_size=64, dropout=0.18671740510715432\n  MSE: 0.2138867974281311\n  MAE: 0.21938197314739227\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-21 01:00:20,616] Trial 6 finished with value: 0.21131253242492676 and parameters: {'learning_rate': 0.0029097959659010065, 'batch_size': 32, 'dropout': 0.14669304317794712}. Best is trial 6 with value: 0.21131253242492676.\n","output_type":"stream"},{"name":"stdout","text":"Trial 6 finished:\n  Params: learning_rate=0.0029097959659010065, batch_size=32, dropout=0.14669304317794712\n  MSE: 0.21131253242492676\n  MAE: 0.2196730077266693\n\n","output_type":"stream"}],"execution_count":null}]}